[
  {
    "objectID": "syllabuslp.html",
    "href": "syllabuslp.html",
    "title": "syllabus",
    "section": "",
    "text": "INSTRUCTOR: Carson Trego, carson.trego@huslers.unl.edu\nCLASS: days:{X, X, X } time{XX:XX}\nLOCATION: XXX, X Hall\nHELP HOURS: M 12:30P-1:30P, F 12:30P-2:30P\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nIMPORTANT DATES:\nExam1: xx/xx\nExam2: xx/xx\nExam3: xx/xx\nFinal Project Presentation: xx/xx\nFinal Project paper due: xx/xx\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nGRADE PORTION:\nExam1: 20%\nExam2: 20%\nFinal Project: 30%\nHomework/participation: 30%\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nHOMEWORK:\nTo avoid homework from causing any stressful cramming and loss sleep, homework will be cut into small pieces that are assigned and completed for each section completed. This means that you may have more than 1 homework items assigned per week, but they will be very short, and will not be due until the necessary information is taught in class.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nPARTICIPATION:\nThese are usually given as 100% or 0%. If you are present in class and attempt to participate in the activities, you will get these points.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nSICK/ABSENT POLICY:\nAbsences will result in a zero in the daily participation points, however, these points can be earned back.\nThe current policy for earning points back is attending office hours/zoom and explaining the information that you were absent for, and answering a few questions. Once this is considered satisfactory, you will earn a credit for a missed class. You may only do this at a rate of one point back per week.\nIf a particularly bad illness happens or another difficult circumstances, a doctors note or other relevant form of documentation may be provided and all points will be given back without needing to accomplish a task.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nCALCULATORS\nCalculators will be allowed on tests. Graphers are allowed if you register to have your memory cleared 10 minutes before the exam. Once cleared, you will be provided with a sticker showing proof of clearing.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nEXAM STRUCTURE\n@@ideally i would like to provide tests in way that is proctored yet allows for significant time, but I am currently working out how that would be achieved. I ideal circumstances, I could be a late class that allows students ample time after the scheduled time, but things dont always work out as planned, so I will gather more information before setting this in stone\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nFINAL PROJECT\nRather than a final exam, you will be expected to do a group/solo project where you will be given a practical question to answer using the information that was covered in class. The project will be in three parts: a presentation,paper, and reflection. The presentation will be given during classtimes, and the paper will be submitted a few days after presentations. Attendance and reflections on your own and other classmates projects will be required to receive a full grade.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nACADEMIC HONESTY\nAs with all courses, academic dishonesty can potentially result in significant grade reduction and further punishment. If you are suspected of committing academic dishonesty, you will be given a chance to meet and discuss what had happen before facing any punishment.\nStudents may be randomly asked to casually discuss the answers they have provided on assignments. Being called in does NOT mean that you are being accused of academic dishonesty, and you will be given ample time to collect your thoughts as explaining answers is inherently stressful for many students.\nThe polices for academic dishonesty will be stated on a per assignment basis, so while it may be allowed to discuss problems with others on one assignment, it may not be allowed for another. In general, copying statements word for word in your answers will NOT be allowed.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nPHOTO POLICY\nStudents often take photos of the board to save time on notes. These notes will be provided on the canvas website, and any relevant board information will be shared as well. So photos should not be required.\nPlease do not take pictures in class without permission prior, as many students prefer not to be recorded. If you would like a copy of any information in class, please let me know!\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nSERVICES FOR STUDENTS WITH DISABILITIES\nThe university provides accommodations for disabilities registered with the Services for Students with Disabilities (SSD). If you are registered with the SSD, please schedule a meeting with me as soon as possible to discuss and plan accommodations.\nIf you have a disability and are not registered and are concerned about a disability, you can contact the SSD office to discuss accommodation plans.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nACE OUTCOME 3 (obtained word for word, have questions on this)\n“Use mathematical, computational, statistical, or formal reasoning (including reasoning based on princi- ples of logic) to solve problems, draw inferences, and determine reasonableness. The reinforced skills for STAT 218 are writing and critical thinking. STAT 218 will provide the student opportunities to achieve this learning outcome through readings, quizzes, and in-class activities. These assignments (together with exams) will be used by the instructor to assess achievement of the outcome. The final exam will be used by the Department of Statistics to carry out an overall assessment of STAT 218 and the course’s effectiveness at assisting students in achieving Outcome 3”\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nDIVERSITY AND INCLUSION\nThe University of Nebraska-Lincoln does not discriminate on the basis of race, ethnicity, color, national origin, sex (including pregnancy), religion, age, disability, sexual orientation, gender identity, genetic information, veteran status, marital status, and/or political affiliation.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nSUPPORT SERVICES (obtained word for word, have questions on this)\n“You can schedule free appointments for individual academic coaching with First-Year Experience and Transition Program staff through MyPLAN. You can also take advantage of study stops–which provide individual and group study with learning consultants in a variety of disciplines–and free group work- shops on topics such as time management, goal setting, test preparation, and reading strategies. See success.unl.edu for schedules and more information.”\nCOUNSELING AND PSYCHOLOGICAL SERVICES\n“UNL offers a variety of options to students to aid them in dealing with stress and adversity. Counseling and Psychological & Services (CAPS) is a multidisciplinary team of psychologists and counselors that works collaboratively with Nebraska students to help them explore their feelings and thoughts and learn helpful ways to improve their mental, psychological and emotional well-being when issues arise.”\nCAPS can be reached by calling 402-472-7450.\n“Big Red Resilience & Well-Being (BRRWB) provides one-on-one well-being coaching to any student who wants to enhance their well-being. Trained well-being coaches help students create and be grateful for positive experiences, practice resilience and self-compassion, and find support as they need it. BRRWB can be reached by calling 402-472-8770”"
  },
  {
    "objectID": "PLANDOCDAY9.html",
    "href": "PLANDOCDAY9.html",
    "title": "PLANDOCDAY9",
    "section": "",
    "text": "Sampling: 50 minutes\nObjectives:\n\nStudents will be able to identify different types of sampling\nStudents will understand the potential issues with convenient sampling, as well as the necessity of convenient sampling\nStudents will be able to explain what makes the results form a sample more generalizable.\n\nConcepts:\n\ngeneralizability\nRepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nsampling frame\n\nIntuition Tools:\n\n\n\n\n\nLesson Material:\nThis is largely a vocab day so we will start with the vocab words\nGENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population.\nActivity Assessment:\nVOCAB AND SAMPLING TYPES: As with the former vocab activities, students will put in groups and assigned a board to write answers on in response to a prompt given on the projector. The prompt will describe an observational study, and the the students will have to answer the following questions:\n\nWhat is the population?\nWhat is the parameter of interest?\nWhat is the sample?\nWhat i the sampling frame?\nWhat type of sampling is being used?\nis there potential bias?\nif so, why?\nif so , how can this bias be improved?\n\nMost of the questions will feature fairly obvious forms of bad convenience sampling, but one of the questions will be intentionally tricky. The sample will be for a medical trial which involves getting willing participants to be subjected to an invasive observation. Patients were found from areas all across the globe, with no geographical clustering. The thoroughness would lead you to think that it is not convenient sampling, but they still obtained willing participants, which has the potential for bias. While this is convenient sampling, it is hard to argue that it should be improved because the way to fix this would be to do the procedure involuntarily, which would be ethically wrong. (I am trying to focus on showing that convenient sampling is not always “bad” as I have heard some students describe it at such). DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY9.html#day9",
    "href": "PLANDOCDAY9.html#day9",
    "title": "PLANDOCDAY9",
    "section": "",
    "text": "Sampling: 50 minutes\nObjectives:\n\nStudents will be able to identify different types of sampling\nStudents will understand the potential issues with convenient sampling, as well as the necessity of convenient sampling\nStudents will be able to explain what makes the results form a sample more generalizable.\n\nConcepts:\n\ngeneralizability\nRepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nsampling frame\n\nIntuition Tools:\n\n\n\n\n\nLesson Material:\nThis is largely a vocab day so we will start with the vocab words\nGENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population.\nActivity Assessment:\nVOCAB AND SAMPLING TYPES: As with the former vocab activities, students will put in groups and assigned a board to write answers on in response to a prompt given on the projector. The prompt will describe an observational study, and the the students will have to answer the following questions:\n\nWhat is the population?\nWhat is the parameter of interest?\nWhat is the sample?\nWhat i the sampling frame?\nWhat type of sampling is being used?\nis there potential bias?\nif so, why?\nif so , how can this bias be improved?\n\nMost of the questions will feature fairly obvious forms of bad convenience sampling, but one of the questions will be intentionally tricky. The sample will be for a medical trial which involves getting willing participants to be subjected to an invasive observation. Patients were found from areas all across the globe, with no geographical clustering. The thoroughness would lead you to think that it is not convenient sampling, but they still obtained willing participants, which has the potential for bias. While this is convenient sampling, it is hard to argue that it should be improved because the way to fix this would be to do the procedure involuntarily, which would be ethically wrong. (I am trying to focus on showing that convenient sampling is not always “bad” as I have heard some students describe it at such). DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY7.html",
    "href": "PLANDOCDAY7.html",
    "title": "PLANDOCDAY7",
    "section": "",
    "text": "Sampling Distributions and Z Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to correctly predict what happens to a sampling distribution as sample size increase\nStudents can correctly differentiate between the sampling distribution and the distribution of the sample\nStudents understand why the normal distribution is present in so many different areas.\nStudents know when to use the theory based approach for significance\n\nConcepts:\n\nAdding up averages at different sample size\nSampling distribution\nCLT (bell shape approach)\nNormal Distribution/ T distribution (above contextual)\nStandard statistic (sim and theory)\ncritical value\nP Value with distance and variance (overlap)\nrecipe for low p(4 cases)\n\nIntuition Tools:\n\nstandardized stat graphed along standard deviation image\n\nLesson Material:\nAt the beginning of class the students will be asked if they have heard of the normal distribution, or how it shows up everywhere. (I had asked this question to our students and a surprising majority said yes) I will then ask why and begin the example.\nThe lesson will begin with a random number generator, which is set to roll random numbers between 0 and 4. As we generate random numbers, these will be tallied on a histogram.\nAfter enough values are recorded, the process will be repeated but with an average of two rolls instead of one,\nThen the process will be repeated for an average of 4 numbers.\nWhat we are creating is sampling distributions:\nSAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nEventually, the original uniform rectangle will start to form a mound shape, hinting at a pattern.\nA variety of programs will then be shown running larger numbers and for different distributions, and all of them will come back mound shaped.\nVisually, this is the normal distribution, and what we are seeing is central limit theorem at work.\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nUsing this system, we can then sim the distribution, and count the relative proportion to get out P value.\nBut if a simulation is impractical, we can still just draw out a normal distribution given our null average and null variance and find where our score lies. This is a continuous distributions, so counting how much area this takes up will be difficult.\nSome people have calculated the amount of area past a certain point on a normal distribution, but there is a problem: it is only for a very specific distribution at mean=0 and variance = 1.\nThankfully, we can do some fancy math (which will not be on the test and will be purely for conceptual understanding) where we shift and divide our normal distribution resulting in an equation that can also shift and divide that value we want to find the significance for (this math will be performed in front of the class)\nThis equation will give us our standardized statistic\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis also is derived from tables, but you only need to know one value for each significance standard and type of hypothesis.\nThe use of these tables to find the critical value will then be shown. All 3 hypothesis values will be recorded.\nIf we are interested if a value is not equal to the the null, than the critical value is 1.96, every single time.\nSo the process here is very simple, we just input our value into the standard statistic formula, and if the absolute value of that number is greater than 1.96, then we know that the value is significant assuming a required p value of 0.05 or less.\nNOTE: a larger difference from the mean increases the Z score, and a lower variance also increases the z score. This hints at the the fact that larger differences from the mean and lower variances tend to make lower p values.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\nAssessment:\nSTANDARDIZED SIGNIFICANCE: Students will be given 10 circumstances, and only 3 where the validity conditions are met. If the validity conditions are not met, the student may skip the problem, if the validity contentions are met they will use the theory approach to calculate the standard statistic and determine if the value is significant.\nthe sample sizes will be given, and the distribution will appear visually on the right. The information will either be proportional or mean based, and the population deviation may or may not be given. Using that information, students will have to select their answers.\nThis will test their ability not only to calculate the standardized statistic, but also when to use that approach. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nRRECIPE FOR LOW P VALUE: Students will be given 4 circumstances {variance low; x - mean = low || variance high, x - mean = low || variance low, x - mean = high, variance high, x - mean = high} and are asked to determine which circumstance would have the lowest p value.\nFrom the a standardized statistic formula, they should be able to find that [variance low, x - mean = high] has the lowest p value as it produces the highest standardized statistic. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY7.html#day7",
    "href": "PLANDOCDAY7.html#day7",
    "title": "PLANDOCDAY7",
    "section": "",
    "text": "Sampling Distributions and Z Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to correctly predict what happens to a sampling distribution as sample size increase\nStudents can correctly differentiate between the sampling distribution and the distribution of the sample\nStudents understand why the normal distribution is present in so many different areas.\nStudents know when to use the theory based approach for significance\n\nConcepts:\n\nAdding up averages at different sample size\nSampling distribution\nCLT (bell shape approach)\nNormal Distribution/ T distribution (above contextual)\nStandard statistic (sim and theory)\ncritical value\nP Value with distance and variance (overlap)\nrecipe for low p(4 cases)\n\nIntuition Tools:\n\nstandardized stat graphed along standard deviation image\n\nLesson Material:\nAt the beginning of class the students will be asked if they have heard of the normal distribution, or how it shows up everywhere. (I had asked this question to our students and a surprising majority said yes) I will then ask why and begin the example.\nThe lesson will begin with a random number generator, which is set to roll random numbers between 0 and 4. As we generate random numbers, these will be tallied on a histogram.\nAfter enough values are recorded, the process will be repeated but with an average of two rolls instead of one,\nThen the process will be repeated for an average of 4 numbers.\nWhat we are creating is sampling distributions:\nSAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nEventually, the original uniform rectangle will start to form a mound shape, hinting at a pattern.\nA variety of programs will then be shown running larger numbers and for different distributions, and all of them will come back mound shaped.\nVisually, this is the normal distribution, and what we are seeing is central limit theorem at work.\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nUsing this system, we can then sim the distribution, and count the relative proportion to get out P value.\nBut if a simulation is impractical, we can still just draw out a normal distribution given our null average and null variance and find where our score lies. This is a continuous distributions, so counting how much area this takes up will be difficult.\nSome people have calculated the amount of area past a certain point on a normal distribution, but there is a problem: it is only for a very specific distribution at mean=0 and variance = 1.\nThankfully, we can do some fancy math (which will not be on the test and will be purely for conceptual understanding) where we shift and divide our normal distribution resulting in an equation that can also shift and divide that value we want to find the significance for (this math will be performed in front of the class)\nThis equation will give us our standardized statistic\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis also is derived from tables, but you only need to know one value for each significance standard and type of hypothesis.\nThe use of these tables to find the critical value will then be shown. All 3 hypothesis values will be recorded.\nIf we are interested if a value is not equal to the the null, than the critical value is 1.96, every single time.\nSo the process here is very simple, we just input our value into the standard statistic formula, and if the absolute value of that number is greater than 1.96, then we know that the value is significant assuming a required p value of 0.05 or less.\nNOTE: a larger difference from the mean increases the Z score, and a lower variance also increases the z score. This hints at the the fact that larger differences from the mean and lower variances tend to make lower p values.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\nAssessment:\nSTANDARDIZED SIGNIFICANCE: Students will be given 10 circumstances, and only 3 where the validity conditions are met. If the validity conditions are not met, the student may skip the problem, if the validity contentions are met they will use the theory approach to calculate the standard statistic and determine if the value is significant.\nthe sample sizes will be given, and the distribution will appear visually on the right. The information will either be proportional or mean based, and the population deviation may or may not be given. Using that information, students will have to select their answers.\nThis will test their ability not only to calculate the standardized statistic, but also when to use that approach. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nRRECIPE FOR LOW P VALUE: Students will be given 4 circumstances {variance low; x - mean = low || variance high, x - mean = low || variance low, x - mean = high, variance high, x - mean = high} and are asked to determine which circumstance would have the lowest p value.\nFrom the a standardized statistic formula, they should be able to find that [variance low, x - mean = high] has the lowest p value as it produces the highest standardized statistic. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY5.html",
    "href": "PLANDOCDAY5.html",
    "title": "PLANDOCDAY5",
    "section": "",
    "text": "Random Variables: 50 minutes\nObjectives:\n\nStudents can identify what a random variable is and how they work practically to produce outputs\nStudents understand the purpose and theory behind simulation\nStudents know that simulations can produce partially off results\nStudents understand that a an infinitely run simulation will produce the underlying probability distribution\n\nConcepts:\n\nRandom Process #concern on teaching\nRandom Variable (RV)\nProbability as logic statement\nsimulation\nlong run probability distribution\n\nIntuition Tools:\n\nRVs as machine that makes random numbers\nProbability as conditional colors\n\nLesson Material:\nAs usual students will be presented with a handout alongside the vocab they will be shown\nAfter going over the definitions for each…\nRANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\n…We will note that simulations produce imperfect results compared to our known distributions, and to demonstrate that, I will run a sim with a small sample size. Then I will increase the sample size and show that it approaches the real distribution over time.\nAs an example in lecture, I will take a probability distribution with easy to count discrete points, and calculate the probability that our random variable X is between A and B. To do this, I will graph all points between A and B, and overlay them on the probability distribution. I will then reference the conditional probability equation, and begin counting the dots with mixed colors (Z union Y) divided by the total number of dots in the whole distribution (Z). Using that method, I will then have the probability that our RV X is between A and B, which could also be done by counting the numbers in between.\n(I will also reference that you can count the amount of dots less than B minus the amount of dots less than A)\nAssessment:\nRV PROBABILITY: Students will each be given a unique dot plot of values, and asked to calculate the probability that a RV is between two points. They will be asked to show their work visually. After doing so, groups will rotate and view the work and answers of another group, and write comments on how they think it went and if they concur. DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS.\nAssessment Activity:\nRV PROBABILITY:Students will be given a similar block distribution as I had used in the demonstration in class. They will be asked to use color overlays to calculate this. and then write out how the method relates to conditional probability of sets. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY5.html#day5",
    "href": "PLANDOCDAY5.html#day5",
    "title": "PLANDOCDAY5",
    "section": "",
    "text": "Random Variables: 50 minutes\nObjectives:\n\nStudents can identify what a random variable is and how they work practically to produce outputs\nStudents understand the purpose and theory behind simulation\nStudents know that simulations can produce partially off results\nStudents understand that a an infinitely run simulation will produce the underlying probability distribution\n\nConcepts:\n\nRandom Process #concern on teaching\nRandom Variable (RV)\nProbability as logic statement\nsimulation\nlong run probability distribution\n\nIntuition Tools:\n\nRVs as machine that makes random numbers\nProbability as conditional colors\n\nLesson Material:\nAs usual students will be presented with a handout alongside the vocab they will be shown\nAfter going over the definitions for each…\nRANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\n…We will note that simulations produce imperfect results compared to our known distributions, and to demonstrate that, I will run a sim with a small sample size. Then I will increase the sample size and show that it approaches the real distribution over time.\nAs an example in lecture, I will take a probability distribution with easy to count discrete points, and calculate the probability that our random variable X is between A and B. To do this, I will graph all points between A and B, and overlay them on the probability distribution. I will then reference the conditional probability equation, and begin counting the dots with mixed colors (Z union Y) divided by the total number of dots in the whole distribution (Z). Using that method, I will then have the probability that our RV X is between A and B, which could also be done by counting the numbers in between.\n(I will also reference that you can count the amount of dots less than B minus the amount of dots less than A)\nAssessment:\nRV PROBABILITY: Students will each be given a unique dot plot of values, and asked to calculate the probability that a RV is between two points. They will be asked to show their work visually. After doing so, groups will rotate and view the work and answers of another group, and write comments on how they think it went and if they concur. DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS.\nAssessment Activity:\nRV PROBABILITY:Students will be given a similar block distribution as I had used in the demonstration in class. They will be asked to use color overlays to calculate this. and then write out how the method relates to conditional probability of sets. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY3.html",
    "href": "PLANDOCDAY3.html",
    "title": "PLANDOCDAY3",
    "section": "",
    "text": "Basic Vocab Time - population and sample: 25 minutes\nObjectives:\n\nStudents can properly differentiate between a sample and population\nStudents can identify the observational units and variables in a study\nStudents understand the difference between an experiment and an observational study\nStudents know why we use samples\n\nConcepts:\n\nPopulation\nSample\nproportion\nmean\nINTRODUCE THE BLOCK\nObservational Unit\nObservational Study\nExperiment\n\nIntuition Tools:\n\nBurger stats\n\nLesson Material:\nQuite a minimal lesson, students will have a vocab handout and follow along with a slideshow with definitions and examples\nPOPULATION: The group of everything you want to know about\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: The subset you grabbed from the population to get an idea of the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nWhy we use samples:\n\nless effort/cost\nmeasurement destruction\nimpossibility\n\nAs an example of desruction, students will be told the about the burger example and the dissection example, where observing destroys the entity in question.\nIn the case of impossibility, animal studies will be mentioned.\nNOTE: you can measure entire populations if you simply define your population to be manageable: (e.g. a whole class)\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\\\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”\n(note, assessment is in the activity)\n\nAssessment Activity- population and sample: 25 minutes\nVOCAB: students will be arranged into groups an placed in a region of the class with writing materials (if it is not a class with 360 boards on the walls, I will go out and prepare some big writing devices to slap on the wall) Students will then be given an example of study, and then be asked:\n\nis this an observational study or an experiment\nwhat is the population\nwhat is the sample\nwhat are the variables being measured?\nwhich variables are response and which are explanatory\nwhat is the observational unit in the study\n\nGroups will then be given a moment to discuss, and if the groups do not concur, an open discussion will take place. DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY3.html#day-3",
    "href": "PLANDOCDAY3.html#day-3",
    "title": "PLANDOCDAY3",
    "section": "",
    "text": "Basic Vocab Time - population and sample: 25 minutes\nObjectives:\n\nStudents can properly differentiate between a sample and population\nStudents can identify the observational units and variables in a study\nStudents understand the difference between an experiment and an observational study\nStudents know why we use samples\n\nConcepts:\n\nPopulation\nSample\nproportion\nmean\nINTRODUCE THE BLOCK\nObservational Unit\nObservational Study\nExperiment\n\nIntuition Tools:\n\nBurger stats\n\nLesson Material:\nQuite a minimal lesson, students will have a vocab handout and follow along with a slideshow with definitions and examples\nPOPULATION: The group of everything you want to know about\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: The subset you grabbed from the population to get an idea of the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nWhy we use samples:\n\nless effort/cost\nmeasurement destruction\nimpossibility\n\nAs an example of desruction, students will be told the about the burger example and the dissection example, where observing destroys the entity in question.\nIn the case of impossibility, animal studies will be mentioned.\nNOTE: you can measure entire populations if you simply define your population to be manageable: (e.g. a whole class)\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\\\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”\n(note, assessment is in the activity)\n\nAssessment Activity- population and sample: 25 minutes\nVOCAB: students will be arranged into groups an placed in a region of the class with writing materials (if it is not a class with 360 boards on the walls, I will go out and prepare some big writing devices to slap on the wall) Students will then be given an example of study, and then be asked:\n\nis this an observational study or an experiment\nwhat is the population\nwhat is the sample\nwhat are the variables being measured?\nwhich variables are response and which are explanatory\nwhat is the observational unit in the study\n\nGroups will then be given a moment to discuss, and if the groups do not concur, an open discussion will take place. DELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY23.html",
    "href": "PLANDOCDAY23.html",
    "title": "PLANDOCDAY23",
    "section": "",
    "text": "Objectives:\nStudents understand the relevance of comparing means\nStudents can use the multiple means applet to make conclusions with the simulation based approach in practical examples\nStudents understand that means alone are a bad means of comparison\nSTUDENTS KNOW THE DIFFERENCE BETWEEN BOXPLOTS AND ERROR BARS\nStudents know how 5 number summaries, mean, and variance are effected by outliers\nSTUDENTS KNOW TO NOT JUST REMOVE OUTLIERS\nConcepts:\ntwo means: quant response\n5 number summary\nconfidence intervals again\noutliers\nsim standard statistic\nIntuition Tools:\nLets pick a box\nSimultaneous boxplot and error plot\nLesson Material:\nWe will start with the “Lets pick a box” example, where one die had a higher mean than the other, but turns out, they are actually the same dice, so they are the same.\nWe will use this to start on comparing means, since just looking at which mean is higher is a bad strategy.\nWe will then talk a bit about 5 number summaries {q1,q2,q3,min,max}, and uses of them.\nShow IQR, how its calculated, and box plots for five number summaries\nNOTE THAT BOX PLOTS AND ERROR BARS ARE DIFFERENT\nFinally, we will spend a fair amount of time working with the simulation\nGIVE STUDENTS AN ANNOTATED SCREENSHOT OF THE SIMULATION AND A FILE OF THE STEPS.\nWe will then go over calculating the sim standard statistic\n\\[\n\\frac{(observedstat_{group1}-observedstat_{group1})-0}{simSD}\n\\]\n\\[\n\\frac{observedStat-null Hypothesis Value}{SD/SE}\n\\]\nAssessment:\nStudents will be given the lightbulb question (submitted in class)\nStudents will be asked why comparing means alone is not good strategy\nStudents will be given two sets of data with the same median and mean, then given two sets with exaggerated outliers (evenly), then given sets with uneven exaggerated outliers and asked to see if the mean, median, or variance changed (this will be a table)\nStudents will be given text files and be asked to evaluate their characteristics with the multiple means applets, they will then be asked to generate error bar plots.\n-With the same text data, they will be asked to make boxplots.\n-They will be asked to comment on the difference\n-they will be asked what to do with the null hypothesis given the data"
  },
  {
    "objectID": "PLANDOCDAY23.html#day-23",
    "href": "PLANDOCDAY23.html#day-23",
    "title": "PLANDOCDAY23",
    "section": "",
    "text": "Objectives:\nStudents understand the relevance of comparing means\nStudents can use the multiple means applet to make conclusions with the simulation based approach in practical examples\nStudents understand that means alone are a bad means of comparison\nSTUDENTS KNOW THE DIFFERENCE BETWEEN BOXPLOTS AND ERROR BARS\nStudents know how 5 number summaries, mean, and variance are effected by outliers\nSTUDENTS KNOW TO NOT JUST REMOVE OUTLIERS\nConcepts:\ntwo means: quant response\n5 number summary\nconfidence intervals again\noutliers\nsim standard statistic\nIntuition Tools:\nLets pick a box\nSimultaneous boxplot and error plot\nLesson Material:\nWe will start with the “Lets pick a box” example, where one die had a higher mean than the other, but turns out, they are actually the same dice, so they are the same.\nWe will use this to start on comparing means, since just looking at which mean is higher is a bad strategy.\nWe will then talk a bit about 5 number summaries {q1,q2,q3,min,max}, and uses of them.\nShow IQR, how its calculated, and box plots for five number summaries\nNOTE THAT BOX PLOTS AND ERROR BARS ARE DIFFERENT\nFinally, we will spend a fair amount of time working with the simulation\nGIVE STUDENTS AN ANNOTATED SCREENSHOT OF THE SIMULATION AND A FILE OF THE STEPS.\nWe will then go over calculating the sim standard statistic\n\\[\n\\frac{(observedstat_{group1}-observedstat_{group1})-0}{simSD}\n\\]\n\\[\n\\frac{observedStat-null Hypothesis Value}{SD/SE}\n\\]\nAssessment:\nStudents will be given the lightbulb question (submitted in class)\nStudents will be asked why comparing means alone is not good strategy\nStudents will be given two sets of data with the same median and mean, then given two sets with exaggerated outliers (evenly), then given sets with uneven exaggerated outliers and asked to see if the mean, median, or variance changed (this will be a table)\nStudents will be given text files and be asked to evaluate their characteristics with the multiple means applets, they will then be asked to generate error bar plots.\n-With the same text data, they will be asked to make boxplots.\n-They will be asked to comment on the difference\n-they will be asked what to do with the null hypothesis given the data"
  },
  {
    "objectID": "PLANDOCDAY21.html",
    "href": "PLANDOCDAY21.html",
    "title": "PLANDOCDAY21",
    "section": "",
    "text": "Objectives:\nStudents will be able to identify a statistical error in certain incorrect statements.\nStudents will be able to design studies to achieve biased results in order to identify/prevent them\nStudents will be able to better identify when more information is needed\nConcepts:\nAt least once/comparisons/manytests\nconfounding variable\npurely comparing averages\npractical suvirorship bias+what you read as a form of statistical sampling (reading from one source)\nflat output tools-&gt; always fail to reject (“done wrong”)\nbiased tools -&gt; always reject [example, my shitty e coli project][camera that emits pure noise]\nwrong design for conclusion/idea not tested\nnot generalizable\nLesson Material:\nThis one should be a bit more of an easygoing/fun(?) section as we talk about the different types of errors/bad design that can come up accidentally or maliciously. Math will be sparing except for the at least once portion.\nWe will start by defining the p value again, and pointing out a flaw with the idea: if you try something enough times, you are bound to get that result at least once. We will then talk about how this applies to studies and how it can happen accidentally and how to avoid it+research ethically.\nWith the P value concept, we will then talk about survirship bias in result that come to mind, and look at reading/researching topics as a form of statistical sampling where you are exposing yourself to a limited amount of data, data which can be biased if the result is more sensational (e.g. many tabloid articles will show studies that get the most shock/have fun results, rather than being thorough)\nWe will also discuss how significant results can also be random chance alone, and that seeing studies with contradictory results is a fundamental part of science, hence why replication is important.\nWe will then discuss how tools of measurement can play a role in the results, specifically how impercise tools can turn Type II, as random noise may show no difference (here i will give an example of an experiment I did where I was using very bad tools and probably came up with nothing because of it)\nFinally we will look into how small differences can produce significance over large sample sizes, and while looking at just the mean difference can produce bad conclusions, solely looking at significance can also be a problem, so it is important to look at a variety of measurements when finding conclusions.\n-=-=\nAssessment:\n“19 studies show that there is no association between happiness and painting your computer the color blue, but a recent study showed a significant difference in happiness for people who painted their computer blue, are the previous studies wrong?” No, variation is normal\n“multiple studies show a significant difference in happiness in patient who were assigned to regularly eat shredded gold, should you budget your money to pay for regular meals with shredded gold” No, we have no idea if the effect is large enough to make a difference worth the cost\n“a chocolate company wants you to find a health benefit to eating their candy in order to increase sales, how can you find this without technically lying?” test for a large number of traits until significance is apparent\n“a leaded gasoline company wants you to show that their gasoline has an immeasurable effect on public health. In an experiment where people are exposed to leaded fumes, how do you find this result without technically lying?” Use imprecise instruments to measure the results"
  },
  {
    "objectID": "PLANDOCDAY21.html#day-21",
    "href": "PLANDOCDAY21.html#day-21",
    "title": "PLANDOCDAY21",
    "section": "",
    "text": "Objectives:\nStudents will be able to identify a statistical error in certain incorrect statements.\nStudents will be able to design studies to achieve biased results in order to identify/prevent them\nStudents will be able to better identify when more information is needed\nConcepts:\nAt least once/comparisons/manytests\nconfounding variable\npurely comparing averages\npractical suvirorship bias+what you read as a form of statistical sampling (reading from one source)\nflat output tools-&gt; always fail to reject (“done wrong”)\nbiased tools -&gt; always reject [example, my shitty e coli project][camera that emits pure noise]\nwrong design for conclusion/idea not tested\nnot generalizable\nLesson Material:\nThis one should be a bit more of an easygoing/fun(?) section as we talk about the different types of errors/bad design that can come up accidentally or maliciously. Math will be sparing except for the at least once portion.\nWe will start by defining the p value again, and pointing out a flaw with the idea: if you try something enough times, you are bound to get that result at least once. We will then talk about how this applies to studies and how it can happen accidentally and how to avoid it+research ethically.\nWith the P value concept, we will then talk about survirship bias in result that come to mind, and look at reading/researching topics as a form of statistical sampling where you are exposing yourself to a limited amount of data, data which can be biased if the result is more sensational (e.g. many tabloid articles will show studies that get the most shock/have fun results, rather than being thorough)\nWe will also discuss how significant results can also be random chance alone, and that seeing studies with contradictory results is a fundamental part of science, hence why replication is important.\nWe will then discuss how tools of measurement can play a role in the results, specifically how impercise tools can turn Type II, as random noise may show no difference (here i will give an example of an experiment I did where I was using very bad tools and probably came up with nothing because of it)\nFinally we will look into how small differences can produce significance over large sample sizes, and while looking at just the mean difference can produce bad conclusions, solely looking at significance can also be a problem, so it is important to look at a variety of measurements when finding conclusions.\n-=-=\nAssessment:\n“19 studies show that there is no association between happiness and painting your computer the color blue, but a recent study showed a significant difference in happiness for people who painted their computer blue, are the previous studies wrong?” No, variation is normal\n“multiple studies show a significant difference in happiness in patient who were assigned to regularly eat shredded gold, should you budget your money to pay for regular meals with shredded gold” No, we have no idea if the effect is large enough to make a difference worth the cost\n“a chocolate company wants you to find a health benefit to eating their candy in order to increase sales, how can you find this without technically lying?” test for a large number of traits until significance is apparent\n“a leaded gasoline company wants you to show that their gasoline has an immeasurable effect on public health. In an experiment where people are exposed to leaded fumes, how do you find this result without technically lying?” Use imprecise instruments to measure the results"
  },
  {
    "objectID": "PLANDOCDAY2.html",
    "href": "PLANDOCDAY2.html",
    "title": "PLANDOCDAY2",
    "section": "",
    "text": "Carson Custom Day: 50minutes\nThis lesson is one that i have not seen on many lesson plans, but I believe will be part of a reoccurring theme in the class of visual statistics. These strategies have helped me significantly with getting better intuition for statistics, and I think these can help students too. Also shapes and colors are nice to have in lessons. It may sound more complex at first, but I see this lesson as a long term investment and I will work to provide good visual examples to make this less difficult.\n\nObjectives:\n\nStudents can draw set symbols as venn diagrams\nStudents understand how the set symbols relate to logical statements\nStudents can understand when to apply conditional probability solutions, and use the formula\nStudents know when it is appropriate to simply multiply and simply add probabilities for intersections and unions\nStudents know how to calculate the at least once probability, and how this effects aspects of our daily lives\n\nConcepts:\n\nUnion, Intersection, Complement\nOR, AND, NOT\nSupport\nIF, TRUE , FALSE\nConditional probability\nIndependent multiplication (AND)\nDisjoint/non disjoint addition (OR)\nAt least once problem\n\nIntuition Tools:\n\na\\(\\cap\\) d, yoUr\nSet symbols as logic gates\ngeometric distribution not fully named before using set symbols\n\nLesson Material:\nStudents will be given a wordless document with set symbols , Venn diagrams, the corresponding logic tables, and an example of the operation on a numerical set and a nominal set. This first document will have absolutely no words outside of AND, OR, NOT, and it will be a bit of an artsy delivery similar to NASA’s golden record or SCP ...|…..|..|. , but the purpose it not to be artsy, but to deliver the material in the simplest way possible and to encourage them to try and figure it out. Words can shut out brains down, so I think this will be an intriguing and engaging exercise.\nFollowing some time in silence, a discussion will begin about the meaning.\nSUPPPORT: all potential outcomes\nUNION/OR: take all values present in both sets, no repeats\nINTERSECTION/AND: take only the values that are present in both sets, no repeats\nCOMPLEMENT/NOT: Take only the values NOT in the set in question\nEMPTY SET: set with nothing\nAfter given these words, they will be asked to write them down on the paper, labeling the operations by name.\nDISJOINT: The intersection is empty, both sets do not overlap\nINDEPENDENCE: The outcome of one event doesn’t effect the other\nCONDITIONAL PROBABILITY: prob A given B=(A AND B)/B\nHere we mention the operation for union of sets, and that if they are disjoint you can simply add them.\nFinally we will explain that if events A and B are independent, you can multiply them together to get the probability of event A AND B as A*B.\nThis will then be applied to a coin flip, where they will be asked to calculate the probability of rolling at least one heads out of 4 flips, which is the complement of the rolling all tails.\nASSESSMENT:\nSET OPERATIONS: students will be given a variety of numeric and nominal sets, and asked to perform the set operations. These will start as individual ones, then combine multiple.\nFollowing these operations, the students will be asked to draw the Venn diagram with the support of the operation displayed. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nGEOMETRIC: students will be given an question where they calculate the probability of either a dice roll or lottery ticket succeeding at least once after a X tries. After, they will be asked to display the method they used in set symbols. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nCONDITIONAL: students will be given a typical conditional probability question, and asked to evaluate the probability. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nOPTIONAL: one idea I thought would be fun, but maybe not fun for anyone else, would be to give them sets of 1s and 0s and a flowchart with set symbols and as them to go down the line and find the output values. Then using a binary to decimal table, write the inputs and outputs, and finally they are asked what operation the paper calculator is performing (+-*/) DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: BONUS HOMWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY2.html#day-2",
    "href": "PLANDOCDAY2.html#day-2",
    "title": "PLANDOCDAY2",
    "section": "",
    "text": "Carson Custom Day: 50minutes\nThis lesson is one that i have not seen on many lesson plans, but I believe will be part of a reoccurring theme in the class of visual statistics. These strategies have helped me significantly with getting better intuition for statistics, and I think these can help students too. Also shapes and colors are nice to have in lessons. It may sound more complex at first, but I see this lesson as a long term investment and I will work to provide good visual examples to make this less difficult.\n\nObjectives:\n\nStudents can draw set symbols as venn diagrams\nStudents understand how the set symbols relate to logical statements\nStudents can understand when to apply conditional probability solutions, and use the formula\nStudents know when it is appropriate to simply multiply and simply add probabilities for intersections and unions\nStudents know how to calculate the at least once probability, and how this effects aspects of our daily lives\n\nConcepts:\n\nUnion, Intersection, Complement\nOR, AND, NOT\nSupport\nIF, TRUE , FALSE\nConditional probability\nIndependent multiplication (AND)\nDisjoint/non disjoint addition (OR)\nAt least once problem\n\nIntuition Tools:\n\na\\(\\cap\\) d, yoUr\nSet symbols as logic gates\ngeometric distribution not fully named before using set symbols\n\nLesson Material:\nStudents will be given a wordless document with set symbols , Venn diagrams, the corresponding logic tables, and an example of the operation on a numerical set and a nominal set. This first document will have absolutely no words outside of AND, OR, NOT, and it will be a bit of an artsy delivery similar to NASA’s golden record or SCP ...|…..|..|. , but the purpose it not to be artsy, but to deliver the material in the simplest way possible and to encourage them to try and figure it out. Words can shut out brains down, so I think this will be an intriguing and engaging exercise.\nFollowing some time in silence, a discussion will begin about the meaning.\nSUPPPORT: all potential outcomes\nUNION/OR: take all values present in both sets, no repeats\nINTERSECTION/AND: take only the values that are present in both sets, no repeats\nCOMPLEMENT/NOT: Take only the values NOT in the set in question\nEMPTY SET: set with nothing\nAfter given these words, they will be asked to write them down on the paper, labeling the operations by name.\nDISJOINT: The intersection is empty, both sets do not overlap\nINDEPENDENCE: The outcome of one event doesn’t effect the other\nCONDITIONAL PROBABILITY: prob A given B=(A AND B)/B\nHere we mention the operation for union of sets, and that if they are disjoint you can simply add them.\nFinally we will explain that if events A and B are independent, you can multiply them together to get the probability of event A AND B as A*B.\nThis will then be applied to a coin flip, where they will be asked to calculate the probability of rolling at least one heads out of 4 flips, which is the complement of the rolling all tails.\nASSESSMENT:\nSET OPERATIONS: students will be given a variety of numeric and nominal sets, and asked to perform the set operations. These will start as individual ones, then combine multiple.\nFollowing these operations, the students will be asked to draw the Venn diagram with the support of the operation displayed. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nGEOMETRIC: students will be given an question where they calculate the probability of either a dice roll or lottery ticket succeeding at least once after a X tries. After, they will be asked to display the method they used in set symbols. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nCONDITIONAL: students will be given a typical conditional probability question, and asked to evaluate the probability. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nOPTIONAL: one idea I thought would be fun, but maybe not fun for anyone else, would be to give them sets of 1s and 0s and a flowchart with set symbols and as them to go down the line and find the output values. Then using a binary to decimal table, write the inputs and outputs, and finally they are asked what operation the paper calculator is performing (+-*/) DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: BONUS HOMWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY14Y15.html",
    "href": "PLANDOCDAY14Y15.html",
    "title": "PLANDOCDAY14Y15",
    "section": "",
    "text": "Objectives:\nStudents know the effect of statistic, margin of error, confidence level, standard error, variance, and sample size on the length of the confidence interval\nStudents can explain what extreme confidence intervals look like\nStudents know what happens to a confidence interval at very large n\nConcepts:\nConfidence interval Range/width/length\nCI hypothesis testing\n2SD method\nIntuition tools:\nlumps\nbars on lumps\nbars on lumps vs other bars on lumps\nLesson Material:\nI plan to first take some time to reflect on confidence intervals\nWhile doing so, we will show confidence interval hypothesis testing by showing lumps with bars and how they overlap or dont, we will also show error bars that go over zero and talk about what they mean.\nAfter words, we will talk about the effect of each variable on confidence interval range, and then show simulations demonstrating what happens when they are brought to the extremes (I have an R sim somewhere that simulates this, as I had used it when i was teaching)\nBasically, the main point that show up is that increasing sample size can shrink the CI down to a theoretical single value.\nI would like to have a shiny app that does somnething similar that students can use, but they will be given this table\n\n\n\nIncreasing ________\n________ the CI width\n\n\n\n\nthe statistic: \\(\\mu, \\bar{x},...\\)\nhas no effect on\n\n\nMargin of error\nincreases\n\n\nConfidence level\nincreases\n\n\nStandard error\nincreases\n\n\nVariance of original sample\nincreases\n\n\nSample size\ndecreases\n\n\n\nI then plan to show the trick where you can purely use visuals to get the standard deviation, and then I will introduce them to the 2SD method. (I know this works for the norm, but I will look into other ones)(this is the ratio/inflection point method)\nFrom this we should be able to do use images of lumps alone to do significance testing using confidence intervals. It will be mostly visual and with few numbers, so i think it will be a nice change of pace and very helpful for intuition.\nFrom that, I may, optimistically, be able to introduce them to the F distribution. Given that this is all just standard deviation from lumps, and those standard deviations make bars on the lumps, and those bars on the lumps allow you to significance test, then using that method to show the intuition behind the f distribution.\nIf possible, we will briefly go over the f ratio, and how it relates to anova/how its used. This may seem optimistic, but I am fairly confident that the use of visual lumps could allow them to learn this concept without it being too difficult. I will not go over anova tables, just the ratio.\nI will cover sims vs theory, but i do need to thoroughly study re sampling before I share any information about it.\nAssessment:\n{revisited}“you find that the confidence interval is 95% between -5 and +5, so are 95% of values in the sample within (-5,+5)” FALSE, its the sampling distribution\n{revisited}“Why dont we use 100% confidence intervals to give absolute certainty with out estimations” They would describe all possible values and thus be useless\n{revisited}“Given a mean value of mu and a continuous distribution, what is the 0% confidence interval?” (mu,mu) or just mu\n“We simultaneously want to be very certain of our confidence interval, and want the confidence interval to be small, how can we do that in terms of sample size and confidence level” High Confidenc level, high sample size.\nStudents will be given data, then they will be asked to draw the sampling distribution change (loosely)\nStudents will be given a lump with a null line. They will need to:\n-approximate the standard deviation\n-proximate the CI using 2SD\n-determine if they will reject or fail to reject\n-(Maybe a similar question with the f ratio, it depends, we will allocate more time to this section but there also still is a lot of material, so i will adjust my plans as I see the classes performance)\nStudents will be given a qsduestion on confidence intervals as before, but they will be expected to know which formulas to use based on the context of the question."
  },
  {
    "objectID": "PLANDOCDAY14Y15.html#day-1415",
    "href": "PLANDOCDAY14Y15.html#day-1415",
    "title": "PLANDOCDAY14Y15",
    "section": "",
    "text": "Objectives:\nStudents know the effect of statistic, margin of error, confidence level, standard error, variance, and sample size on the length of the confidence interval\nStudents can explain what extreme confidence intervals look like\nStudents know what happens to a confidence interval at very large n\nConcepts:\nConfidence interval Range/width/length\nCI hypothesis testing\n2SD method\nIntuition tools:\nlumps\nbars on lumps\nbars on lumps vs other bars on lumps\nLesson Material:\nI plan to first take some time to reflect on confidence intervals\nWhile doing so, we will show confidence interval hypothesis testing by showing lumps with bars and how they overlap or dont, we will also show error bars that go over zero and talk about what they mean.\nAfter words, we will talk about the effect of each variable on confidence interval range, and then show simulations demonstrating what happens when they are brought to the extremes (I have an R sim somewhere that simulates this, as I had used it when i was teaching)\nBasically, the main point that show up is that increasing sample size can shrink the CI down to a theoretical single value.\nI would like to have a shiny app that does somnething similar that students can use, but they will be given this table\n\n\n\nIncreasing ________\n________ the CI width\n\n\n\n\nthe statistic: \\(\\mu, \\bar{x},...\\)\nhas no effect on\n\n\nMargin of error\nincreases\n\n\nConfidence level\nincreases\n\n\nStandard error\nincreases\n\n\nVariance of original sample\nincreases\n\n\nSample size\ndecreases\n\n\n\nI then plan to show the trick where you can purely use visuals to get the standard deviation, and then I will introduce them to the 2SD method. (I know this works for the norm, but I will look into other ones)(this is the ratio/inflection point method)\nFrom this we should be able to do use images of lumps alone to do significance testing using confidence intervals. It will be mostly visual and with few numbers, so i think it will be a nice change of pace and very helpful for intuition.\nFrom that, I may, optimistically, be able to introduce them to the F distribution. Given that this is all just standard deviation from lumps, and those standard deviations make bars on the lumps, and those bars on the lumps allow you to significance test, then using that method to show the intuition behind the f distribution.\nIf possible, we will briefly go over the f ratio, and how it relates to anova/how its used. This may seem optimistic, but I am fairly confident that the use of visual lumps could allow them to learn this concept without it being too difficult. I will not go over anova tables, just the ratio.\nI will cover sims vs theory, but i do need to thoroughly study re sampling before I share any information about it.\nAssessment:\n{revisited}“you find that the confidence interval is 95% between -5 and +5, so are 95% of values in the sample within (-5,+5)” FALSE, its the sampling distribution\n{revisited}“Why dont we use 100% confidence intervals to give absolute certainty with out estimations” They would describe all possible values and thus be useless\n{revisited}“Given a mean value of mu and a continuous distribution, what is the 0% confidence interval?” (mu,mu) or just mu\n“We simultaneously want to be very certain of our confidence interval, and want the confidence interval to be small, how can we do that in terms of sample size and confidence level” High Confidenc level, high sample size.\nStudents will be given data, then they will be asked to draw the sampling distribution change (loosely)\nStudents will be given a lump with a null line. They will need to:\n-approximate the standard deviation\n-proximate the CI using 2SD\n-determine if they will reject or fail to reject\n-(Maybe a similar question with the f ratio, it depends, we will allocate more time to this section but there also still is a lot of material, so i will adjust my plans as I see the classes performance)\nStudents will be given a qsduestion on confidence intervals as before, but they will be expected to know which formulas to use based on the context of the question."
  },
  {
    "objectID": "PLANDOCDAY12.html",
    "href": "PLANDOCDAY12.html",
    "title": "PLANDOCDAY12",
    "section": "",
    "text": "Objectives:\nStudents can decompose and rewire the CI formula\nStudents can calculate proportion confidence intervals with the theory based method\nConcepts\nProportion intervals\nZ score\nMoE proportion\nProportion SE\nProportion CI Validity condiitons\nIntuition Tools:\nLesson Material:\nStudents will be given the following, they will be instructed to decompose the formula {i did this one in class when I taught, it seemed successfull}\n\n\n\n\nSample Statistic\nPopulation Parameter\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nProportion\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\n\nCI=Observered_Proportion \\(\\pm\\) Margin_Of_Error\nObserved_Proportion = \\(\\hat{p}\\) {if working with a proportion}{\nMargin_Of_Error = Multiplier*Standard_Error\nStandard_Error = \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p}) }{n}}\\) {If working with proportion}\nMultiplier=\\(Z_{\\alpha/2}\\) {if working with a proportion}\n\\(Z_{\\alpha/2}\\) = 1.96 {IF confidence level is 95%}\nVALDIDTIY CONDTIOINS\nAt least 10 observational units in both categories (10 successes and 10 failures)\nAssessment:\nYou sampled 100 UNL students, and 25 of them said that they like baseball. Estimate the population parameter for the proportion of UNL students who like baseball and write your results as a 95% confidence interval.\nAre the validity conditions met?\nYou sampled 250 Nebraska residents and 210 of them said they like baseball. Estimate the population parameter for the proportion of Nebraska residents who like baseball and write your results as a 95% confidence interval.\nAre the validity conditions met?\n\nvalidity conditions\ngeneral formula decomposition\nmargin of error proportion\nz score revisited\nvalidity conditions (said again)"
  },
  {
    "objectID": "PLANDOCDAY12.html#day-12",
    "href": "PLANDOCDAY12.html#day-12",
    "title": "PLANDOCDAY12",
    "section": "",
    "text": "Objectives:\nStudents can decompose and rewire the CI formula\nStudents can calculate proportion confidence intervals with the theory based method\nConcepts\nProportion intervals\nZ score\nMoE proportion\nProportion SE\nProportion CI Validity condiitons\nIntuition Tools:\nLesson Material:\nStudents will be given the following, they will be instructed to decompose the formula {i did this one in class when I taught, it seemed successfull}\n\n\n\n\nSample Statistic\nPopulation Parameter\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nProportion\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\n\nCI=Observered_Proportion \\(\\pm\\) Margin_Of_Error\nObserved_Proportion = \\(\\hat{p}\\) {if working with a proportion}{\nMargin_Of_Error = Multiplier*Standard_Error\nStandard_Error = \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p}) }{n}}\\) {If working with proportion}\nMultiplier=\\(Z_{\\alpha/2}\\) {if working with a proportion}\n\\(Z_{\\alpha/2}\\) = 1.96 {IF confidence level is 95%}\nVALDIDTIY CONDTIOINS\nAt least 10 observational units in both categories (10 successes and 10 failures)\nAssessment:\nYou sampled 100 UNL students, and 25 of them said that they like baseball. Estimate the population parameter for the proportion of UNL students who like baseball and write your results as a 95% confidence interval.\nAre the validity conditions met?\nYou sampled 250 Nebraska residents and 210 of them said they like baseball. Estimate the population parameter for the proportion of Nebraska residents who like baseball and write your results as a 95% confidence interval.\nAre the validity conditions met?\n\nvalidity conditions\ngeneral formula decomposition\nmargin of error proportion\nz score revisited\nvalidity conditions (said again)"
  },
  {
    "objectID": "PLANDOCDAY10.html",
    "href": "PLANDOCDAY10.html",
    "title": "PLANDOCDAY10",
    "section": "",
    "text": "Errors: 50 minutes\nObjectives:\n\nStudents can correctly identify type 1 and 2 errors.\nStudents understand the meaning of alpha and beta in the context of statistical errors.\nStudents know what factors increase the likelihood of each error, and what factors(non statistical use of this word) increase power and p value.\nStudents can identify relative power levels and p values from visual data alone\n\nConcepts:\n\nType 1 Error\nType 2 Error\nSignificance level alpha\nbeta\nPower 1-beta\n\nIntuition Tools:\n\nT1ER T2EFTR\n\nLesson Material:\nThe lesson will start with the terminology being defined, students will be provided with the box chart of errors and a graph of two distributions showing where alpha and beta sit.\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nWhile wrote memorization of the error terminology may seem pointless when it can just be checked, these terms come up surprising amount, so I will let instruct them to look carefully at the vocab and variables.\nTo make memorization easier, I will show the T1ER T2EFTR method, where T1ER stands for Type 1-&gt;Reject, as you erroneously rejected the null. This looks like like “TIER”. The next one is T2EFTR, which stands for Type 2-&gt;Fail To Reject, as you are erroneously failing to reject the null. This, if written weirdly, looks like “TLEFTR” (teh-lef-ter), which inst a word, but its works well to memorize regardless for some reason.\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )\n\n\n\n\nAssessment:\nERRORS: Students will be given an example of an experiment, a conclusion, and the truth, and they will be asked to put the example in one of 4 boxes on the type of error/correct conclusion chart. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nP AND POWER: Students will be given 4 variables {meanx-nullmean common variance, significance level, sample size}. The experiment will suggest one set up with numerical values where all values are held constant except one (so one experiment will have a diff=10, var=1,a=0.05, and n=10, while the other has diff=10, var=1,a=0.05, and n=20, and students will have to chose the one with the higher power level.\nAs an important review, students will also be given a situation with the same variables stated, and asked to predict which one has a lower p value. (NOTE: the alpha will not effect the p value)\nNext, I will give them two distributions on the same chart alongside another two distributions on the same chart. The distributions will minimal labels. From this, the students will have to select which charts have lower p values, and which charts have higher power levels\nThis will hopefully give them a conceptual understanding of the p value and power so that they can predict power levels and p value without needing all of the data.\nAn example of some charts that will be given to them for the minimal information portion can be found below (circumstance b has a higher power and lower p value) DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm1,norm2),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance A\", x=\"X\", y=\"Density\")\n\n\n\n\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm3,norm4),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance B\", x=\"X\", y=\"Density\")"
  },
  {
    "objectID": "PLANDOCDAY10.html#day10",
    "href": "PLANDOCDAY10.html#day10",
    "title": "PLANDOCDAY10",
    "section": "",
    "text": "Errors: 50 minutes\nObjectives:\n\nStudents can correctly identify type 1 and 2 errors.\nStudents understand the meaning of alpha and beta in the context of statistical errors.\nStudents know what factors increase the likelihood of each error, and what factors(non statistical use of this word) increase power and p value.\nStudents can identify relative power levels and p values from visual data alone\n\nConcepts:\n\nType 1 Error\nType 2 Error\nSignificance level alpha\nbeta\nPower 1-beta\n\nIntuition Tools:\n\nT1ER T2EFTR\n\nLesson Material:\nThe lesson will start with the terminology being defined, students will be provided with the box chart of errors and a graph of two distributions showing where alpha and beta sit.\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nWhile wrote memorization of the error terminology may seem pointless when it can just be checked, these terms come up surprising amount, so I will let instruct them to look carefully at the vocab and variables.\nTo make memorization easier, I will show the T1ER T2EFTR method, where T1ER stands for Type 1-&gt;Reject, as you erroneously rejected the null. This looks like like “TIER”. The next one is T2EFTR, which stands for Type 2-&gt;Fail To Reject, as you are erroneously failing to reject the null. This, if written weirdly, looks like “TLEFTR” (teh-lef-ter), which inst a word, but its works well to memorize regardless for some reason.\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )\n\n\n\n\nAssessment:\nERRORS: Students will be given an example of an experiment, a conclusion, and the truth, and they will be asked to put the example in one of 4 boxes on the type of error/correct conclusion chart. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nP AND POWER: Students will be given 4 variables {meanx-nullmean common variance, significance level, sample size}. The experiment will suggest one set up with numerical values where all values are held constant except one (so one experiment will have a diff=10, var=1,a=0.05, and n=10, while the other has diff=10, var=1,a=0.05, and n=20, and students will have to chose the one with the higher power level.\nAs an important review, students will also be given a situation with the same variables stated, and asked to predict which one has a lower p value. (NOTE: the alpha will not effect the p value)\nNext, I will give them two distributions on the same chart alongside another two distributions on the same chart. The distributions will minimal labels. From this, the students will have to select which charts have lower p values, and which charts have higher power levels\nThis will hopefully give them a conceptual understanding of the p value and power so that they can predict power levels and p value without needing all of the data.\nAn example of some charts that will be given to them for the minimal information portion can be found below (circumstance b has a higher power and lower p value) DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm1,norm2),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance A\", x=\"X\", y=\"Density\")\n\n\n\n\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm3,norm4),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance B\", x=\"X\", y=\"Density\")"
  },
  {
    "objectID": "MethodsAndGoals.html",
    "href": "MethodsAndGoals.html",
    "title": "Reflection",
    "section": "",
    "text": "Self Reflection:\nAfter typing the below text, I have decided to put the self reflection at the top.\nI feel like I have added a fair amount of information on this site, and I am happy that I was able to get some of it working (except the github hosting aspect). I like a lot of whats on here, but I still see the project as far from finished, even when not including the upcoming weeks.\nI would like to add more detail to the assessments, to the point where they are all independent files like the handouts. I would also like to improve the aesthetics of it all, as a lot of the formatting fell flat (for example, the code chunks are NOT supposed to be visible as that is just distracting.\nOverall, while I am proud of the information on here and the fact that this is my first quarto site, I still see many avenues for improvement, and I plan to attend office hours to improve some of the more technical issues I have faced.\n\nThe following entry is a combination of my reflection and methods for the class. I think about class design a lot so the methods I would like to employ are deeply related to the ideas on how the class should be structured.\nEvery class has core ideas, so I have decided to plan the class around a few central themes. These ideas will be introduced early on, and then referenced regularly throughout the class:\n\nVisual statistics\n\nStatistics can be much easier to have an intuition for when visuals are provided, so the class will make frequent use of graphs and colors along side formulas and concepts. (This will require a bit of set theory to begin)\n\nlack of words\n\nMath is harder with words, and sometimes words can prevent a deeper understanding of a topic. I would like to frequently give students information and questions that feature very little words or hard data, and instead require them to visually asses the problem\nThis also applies to labeled formulas. Eventually, I would like to prepare annotated formulas that use as few words as practical. Mindlessly using formulas is not good, but sometimes a labeled formula can help build a conceptual understanding if the elements are considered closely by the reader.\n\ndice and spinners\n\nprobability distributions are abstract, but everyone knows how dice and spinners work, I would like to regularly reference these when discussing probability distributions.\n\n“numbers do not speak for themselves”\n\nWithout assumptions in place, statistics can be very subjective (as is math in general). I want to regularly encourage students to interpret the data themselves when no assumptions are provided, and stress how many aspects of statistics are subjective.\n\nvocab is vocab\n\nSometimes theirs no replacement for just reading and understanding vocab. This will be given to them directly rather than hoping the correct definitions are gathered from discussion alone.\n\nSlides not the focus\n\nThere will be some use of slides in class used to paste some visual ideas in front of the class, but they will not be consistently up. Often times words on slides can detract from the topic, so the slides will not have much text information on them. To maintain pace and engagement, I would prefer to use an overhead or board to perform methods in class, and physical methods performed on the spot will be preferred to slides.\nIn my own lectures, I have notices that I tend to disconnect the slideshow frequently to display simulations and write on the board/overhead. So demonstrations will be prioritized to slides.\n\nclear expectation\n\nWhile I would love students to be so interested in the course that they start watching 3blue1brown videos and learn things that I haven’t even mentioned, this does not feel realistic. My goal is for students to know what concepts to understand and how to train them. I will ask some challenge questions, but they will be graded as more of a bonus and gauge of engagement and ability rather than a requirement.\n\n\nWith exams, there are a few methods I am interested to investigate\n\nexam independent of time\n\nWhile I believe that everything in the course will be planned so that, with sufficient hard work, every student will be able to understand and answer correctly, I’ve always felt that time doesn’t effect everyone the same. Some students approach exams with a ton panic, others check their work more than others, some students accidentally get hung up on a question, these kinda traits can make some students do worse on time tests than others even if they don’t have a contrition recognized by the SSD. To ease this aspect and hopefully make the learning less stressful I would like to figure out a way to make tests have such ample time that students don’t think about it (within reason). I don’t want the test to be less challenging, just challenging in the understanding of materials of materials way and not challenging in a resistance to panic way.\nIn a perfect world, I would have a room that I could rent for a few hours and have closed book proctored tests\n\nCompetency and Challenge\n\nTests serve two functions: to measure general competency and to gauge how well the class is absorbing the materials. Both aspects are useful: If I were to only gauge competency, I would not know when to speed up or slow down, and if I were to only gauge ability on a relative scale, this would miss the main point of the class: to produce competent readers of data. An idea I would like to investigate on exams, is to have high weighted competency section and a lesser weighted/bonus challenge section (this will likely not be formally stated in the test). This allows me to grade primarily on competency, and that students who display competency get a good grade, but also lets me see how the class is doing and if students have taken a special interest in statistics. I think of this like a skewed distribution where the competency (hopefully the median) is centered around the higher percentages, but the the distribution quickly drops down and the density at 100% is near 0.\n\n\nI personally believe that final projects are generally more effective than final exams, as many undergrad final projects have given me information and experience that I still find important years later, so the final will be project based:\n\nThe final project will be in three parts: presentation, paper, assessment and reflection\\\nThe presentation will happen first during the week before finals. Students will be given reactions on how to improve anything wrong with the information so far.\nThe paper will be submitted during finals week. This will have more in depth information and be graded to a more exact standard\nThe reflection will be a combination of students attending and reacting to other presentations, along with reflecting on their experience working as a group.\n\nThe class will feature many activities based on participation, with the following main goals. Activities in class will be on the spot and not require to pre reading to understand. Ideally, the handouts and introduction lecture should cover it.\n\nActivities allow students to confront ideas with each other to learn them\nActivities give students an opportunity to question ideas layed out in class (it is near impossible not to miss something when teaching, so if I get successfully (and politely) called out on something during an activity, I will consider it to be a teaching win.\nActivities are not just for completing work, Ideally, they will be structured so that students feel more comfortable expressing ideas openly and to each other. if student are too rigid, they may find engaging in class to be too nerve racking as they feel judged by strangers. In practice, I will not get at them for discussing topics that aren’t immediately math related (within reason).\n\nThere will also be demonstrations, but these demonstrations will be used as a means to deliver information on methods that are particularly confusing, or display activities that would take a large amount of classroom\n\nFor example, there is a demonstration I do where I calculate the P value using colored squares and clear paper. This could be performed in class but it could be demonstrated as a demonstration in much less time (I have noticed that time can be a huge challenge so I intend to have backups and compensating actions for if things do not go as fast as planned)\n\nHomework will have the following ideas in its design:\n\nHomework will be very short and frequent. This is to avoid all the problems with stressful cramming and encourage learning little by little. Homework will be assigned after the material is began and they will have until the lecture after the lecture after the section is completed.\nHomework should straight forward and avoid situations where these student has no idea how to even look for an answer. All homework questions will related directly to on canvas materials. If the student gives themselves the proper time. the learning process should be calm.\nMost of the homework questions will be nigh-objective in correct answers, but to encourage free thinking and engagement there will always be a creative question they are to answer, which is graded more on submission. This is to encourage them to take a risk and attempt at reorganizing what they learned, without being penalized for not copying definitions exactly. If they make flawed assumptions, they will be notified about it, but it will not harm their grade.\n\nThis plan is already feeling very optimistic, but I have a few even more optimistic stretch goals:\n\nHomework and projects will not just have context, but a consistent narrative. A prime example of narrative in homework is the Murder in SQL City assignment. Aside from being fun, I am willing to be that narrative aspects can increase engagement and understanding.\nStudents will be given intentionally subjective questions in class, which they will discuss with others and hopefully (politely lightheartedly) debate each other.\nSUPER STRETCH GOAL: if by some wild circumstances I end up being am inhumanly efficient teacher and the class goes exceptionally, I would love to briefly mention a little bit of R. Programming basics are extremely useful and I have benefit tremendously from the classes that included it. Realistically, this would be very basic stuff, more realistically, we will not have time, but I like having optimistic stretch goals regardless."
  },
  {
    "objectID": "handoutPREIIIA.html",
    "href": "handoutPREIIIA.html",
    "title": "HANDOUT PRE A",
    "section": "",
    "text": "DATA TYPES\n\nlibrary(DiagrammeR)\n\nWarning: package 'DiagrammeR' was built under R version 4.2.3\n\nDiagrammeR::grViz(\"digraph {\n\ngraph [layout = dot, rankdir = LR]\n\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = rectangle, style = filled, fillcolor = Linen]\n\nvar  [label = 'variable']\ncat [label = 'categorical']\nquant [label =  'quantitative']\nord [label = 'ordinal']\nnomi [label= 'nominal']\ndisc [label= 'discrete']\ncont [label= 'continuous']\n\n# edge definitions with the node IDs\nvar -&gt;{cat, quant} \ncat -&gt; {nomi, ord}\nquant -&gt; {disc, cont}\n}\")\n\n\n\n\n\nCLASS: groups of values\n\n\n\n\n\n\n\nCount\n(these are the classes)\nFrequency\n(number of times a value within these classes showed up in the data)\n\n\n\n\n1-10\n2\n\n\n11-20\n5\n\n\n21-30\n3\n\n\n\nHISTOGRAM: A chart displaying the amount of elements in each class\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar()+\n  labs(title=\"Histogram\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Frequency\")\n\n\n\n\nRELATIVE FREQUENCY HISTOGRAM: like a normal histogram, but it shows the amount of said class relative to the total, rather than just the amount of said class. NOTE: the scale ranges from 0-100%, AKA, 0-1\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Relative Frequency Histogram\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Frequency/Total\")\n\n\n\n\nPROBABILITY DISTRIBUTION: Very similar to the relative frequency histogram, except the percentages on the y-axis can be thought of as probabilities. You can add these, so in the below example, if you were to put all of the numbers in a bag and select one at random, there is a 20% chance of getting a number between 1 and 10, and a 70% chance of getting a number between 1 and 20.\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Probability distribution\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Probability\")\n\n\n\n\nDISCRETE DISTRIBUTION: A probability distribution with a finite number of outcomes between two different points. In other words, you can only get certain exact numbers.\nThe distribution of a six sided die can be seen below:\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2,3,4,5,6)\n)\n\ncutoff &lt;-c(0,1,2,3,4,5,6)\nclassttl &lt;-c(\"1\",\"2\",\"3\",\"4\", \"5\",\"6\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Distribution of 6 sided die\")+\n  xlab(\"Number Rolled\")+\n  ylab(\"Probability\")\n\n\n\n\nIMPORTANT CONCEPT: the probability of rolling a value is the area that that the bars take up, so for the chance of rolling a 1 or 2, the width of the bars is 1+1=2 and the height of the bars is \\(\\approx\\) 0.16666, so the probability of rolling one or two is 0.16666*2 \\(\\approx\\) 0.3333 or 33.3%.\nCONTINUOUS DISTRIBUTION: A probability distribution with an infinite number of outcomes between two different points. (NOTE: there are an infinite number of real numbers between 0 and 1.)\n\nlibrary(ggplot2)\n\nlistval &lt;- data.frame(vals= seq(0,10,length.out=1000000))\n\n\nggplot(listval, aes(x=vals))+\n  geom_density(fill=\"grey\", color=\"black\")+\n  labs(title = \"continuous probability distribution\")+\n  xlab(\"x\")+\n  ylab(\"density\")\n\n\n\n\nSHAPE: literally the shape of the distribution, visually.\nNOTE: the distributions are below are a bit blocky, but continuous distributions can also have the same shapes."
  },
  {
    "objectID": "HandoutH.html",
    "href": "HandoutH.html",
    "title": "HandoutH",
    "section": "",
    "text": "GENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population."
  },
  {
    "objectID": "HandoutF.html",
    "href": "HandoutF.html",
    "title": "HandoutF",
    "section": "",
    "text": "SAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\\[\nZ=\\frac{x-\\mu}{\\sigma}\n\\]\nRecipe for low p Values\n\nAs variation decreases, p value decreases\nAs the difference between the null mean and the alternative mean increase, the p value decreases ."
  },
  {
    "objectID": "HandoutCAR.html",
    "href": "HandoutCAR.html",
    "title": "HandoutCAR",
    "section": "",
    "text": "I will later edit this so that I have a custom image made exactly how I intend to present it, but as a placeholder, I have prepared some useful resources."
  },
  {
    "objectID": "handoutB.html",
    "href": "handoutB.html",
    "title": "handoutB",
    "section": "",
    "text": "Variance: measure of variability present in a set of data.\nThis is calculated by taking average difference between each value and the mean of the data set\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}=\\sigma^{2}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}=s^{2}\\)\nStandard deviation: literally just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nDistribution with lower variance\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\nDistribution with higher variance\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances."
  },
  {
    "objectID": "courseschedule.html",
    "href": "courseschedule.html",
    "title": "COURSE SCHEDULE",
    "section": "",
    "text": "NOTE: “DAY”s are not literal days, just 50 minute sections which can be adjusted based on student responses. The pacing seems appropriate, but I am welcome to be wrong on that, and flexible on how much review will be done for each section.\n&gt;&gt;&gt;“DAY”1\nSection C-S, 25 min ~ LECTURE\n\nsyllabus\n\nSection P-1 25min Handout PRE A\n\nVariables\nCategorical vs Quantitative Variables\nClasses\nHistograms/Relative Frequency Histograms\nProbability Distributions\nShape\n\n&gt;&gt;&gt;“DAY” 2\nSection C-1 50min HANDOUT CAR\n\nvenn diagrams + set symbols\nSupport\nset symbols as logic+“if”\nindependent probability\nconditional probability\n“At least once” problem\n\n&gt;&gt;&gt;“DAY”3\nSection P-1 25min HANDOUT A\n\npopulation\nsample\nmean\n“the block” stat/param types\nobservational unit\nvariable\nobservational study\nexperiment\n\nsection P-1 25 min ~ACTIVITY\n\nvocab match (group or lecture)\n\n&gt;&gt;&gt;“DAY”4\nSection P-2 25min HANDOUT B\n\nRelative frequency histograms\nprobability distributions\nshape\nmeasures of central tendency\nvariability: variance/stdev\noutliers\n\nSection P-1 ~ ACTIVITY\n\nvocab match (group or lecture)\n\n&gt;&gt;&gt;“DAY”5\nSection P-2 25minHANDOUT C\n\nRandom Process #concern on teaching\nRandom Variable\nProbability as logic statement\nSimulation\nlong run probability distribution\n\nSection P-2 ~ EXAMPLE LECTURE\n\nLecture example, probability distributions as colors (no p value)\n\nActivity: Students calculate probability by overlay method.\n&gt;&gt;&gt;“DAY”6\nSection 1-1/2 25min HANDOUT D+E\n\nreiterate population and sample\nreiterate data types\nthe block {[proportion parameter,sample statistic]*[Quant mean,Cat proportion]}\nNull distribution\nalternative space\n“significance” (how weird/percent match)\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nSection 1-1/2 25min ~ EXAMPLE LECTURE\n\nP value as two distinct sets being overlayed (conditional probability with colors)\nP values as overlap percent match\nDolphin-like example ##check the name on this\n\n&gt;&gt;&gt;“DAY”7\nSection 1-1/2 25min ~ EXAMPLE LECTURE\n\nAdding up averages of dice with different sample size\nsampling distribution\nCLT (the graph goes to bell shape)(brief)\nTheory based and sim\nP value with distance and variance (overlap with colors)\n\nSection 1-3/4 25min HANDOUT F\n\nAdding up averages at different sample size\nsampling distribution revisisted\nCLT (bell shape approach)\nNormal Distribution\nStandard Statistic (sim and theory)\ncritical value\np value with distance and variance\nrecipe for low p values\n\n&gt;&gt;&gt;“DAY” 8\nsection 1-5 50min ~ ACTIVITY HANDOUT G\n\nT statistic\nStandard Error\nDegrees of Freedom\nCircumstantial flow chart\n\n&gt;&gt;&gt;“DAY” 9\nSection 2-1 HANDOUT H\n\ngeneralizability\nrepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nother sampling forms (brief)\nsampling frame\n\nSection 2-1 ~ ACTIVITY\n\ndescribe the sampling\nstate the problem\nstate a solution\n\n&gt;&gt;&gt;“DAY” 10\nSection 2-2/3 25min HANDOUT I\n\nType 1 error\nType 2 error\nsignificance level alpha\nbeta\neffect size\npower\n\nSection 2-2/3 25min ~ ACTIVITY\n\nPredict the relative power and P from statistics\nPredict the relative power and P from visual charts alone\n\n&gt;&gt;&gt;“DAY” 11\nSection 3.1: confidence interval introduction\n\nconfidence interval purpose\nconfidence intervals meaning\nsampling distribution revisited again\nstandard error revisited\nmargin of error\nconfidence level\nmisconceptions about confidence intervals\nintroduce flow chart (evaluate step by step)\n\n&gt;&gt;&gt;“DAY” 12\nSection 3.2: proportion interval\n\nvalidity conditions\ngeneral formula decomposition\nmargin of error proportion\nz score revisited\nvalidity conditions (said again)\n\n&gt;&gt;&gt;“DAY”13\nSection 3.3: mean interval\n\nvalidity conditions\ngeneral formula decomposition\nmargin of error proportion\nst score revisited\nconnections of both formulas\nvalidity conditions said again\nprovide final flowchart\n\n&gt;&gt;&gt;“DAY” 14 + 15\nSection 3.4: compiling confidence intervals, statistics in lumps\n\nreflect on normal distribution\n2SD\neffects by MOE, confidence level, variance, sample size on width\nConfidence intervals to compare to null/hypothesis test (2 lumps)\nUltimate test(F), variance on variance 2+ lump (this makes sense to me)(this may wait for later or never)\nsimulated non-parametric (i will not call it that)(i will check make sure my sim logic is correct)\ntraining wheels off, given multiple situations and respond to them freely\npurely visual p value test (sd inflection pt*2)\nremark that all tests (i think) have been cat in quant out, and we are only in quadrant II (this quadrant system is a WIP, ignore)\n\n&gt;&gt;&gt; “DAY” 16\nSection 5.1 [will return to ch4] categorical response\n\nfinally begin quadrant 1\npreviously response was quantitative and explanatory was categorical, now both are categorical\nconditional probability recap\ntwo way conditional probability table\nconditional proportions\nrelative risk\n\n&gt;&gt;&gt;“DAY” 17\nSection 5.2 simulated comparison\n\ntwo proportions applet\napplet p value\nsim standard statistic\nconfidence interval\nconfidence interval hypothesis testing revisted\n\n&gt;&gt;&gt;“DAY” 18\n5.3 Theory based comparison\n\nValidity conditions\nNew standard error\nSS=(ob-null)/sd/se &gt;&gt;&gt; brush up on why SD is divided by SE instead of n.\nTheory confidence interval\nSay validity conditions again\n\n&gt;&gt;&gt;“DAY” 19 [OPTIONAL-BUT WOULD REALLY LIKE TO COVER CHISQR]\n5.? other measures of comparison CHECK BOOK FOIR THIS\n\nchi square test\nfishers exact test\nflowchart: when to use each in chapter 5\nupdate quadrant chart\n\n\n&gt;&gt;&gt;“DAY” 20 taking a break from the quadrants\nSection 4.1/2\n\nultra reiterate explanatory and response (+why we use them instead of ind+dep)\nexperiment vs observational\nconfounding variables\nstudy diagram\nrandomization and confounding effects\ncause and effect possible by randomization and experimental studies\ncorrelation isnt yeah\nblocking\n\n&gt;&gt;&gt;“DAY” 21 [OPTIONAL]\nSection ?.? Statistical errors\n(which ones the split plot one)\n\nInteraction-&gt; interaction not accounted for\nmissing variable\nthe ol many separate tests\npure random chance (one study)\nsurvivorship bias+what you read as a form of statistical sampling (reading from one soucre)\nwrong design [elaborate]\nidea not tested (try once vs long term safety)\nnot generalizable\nflat output tools-&gt; always fail to reject (“done wrong”)\nbiased tools -&gt; always reject [example, my e coli project][camera that reports pure noise]\nblocking?\npurely comparing averages, not even accounting for significance or just having a really low standard\n\n&gt;&gt;&gt;“DAY” 22 [OPTIONAL]\nsection ?.? Statistical error: Interaction\n\nInteraction as a concept\nInteraction plots\nInteraction in errors/conclusiions\nblocking?\n\n\n&gt;&gt;&gt;“DAY” 23 [VERY IMPORTANT SECTION, MAKE SURE THIS HAPPENS]\nchapter6.1/2 Comparing two means: quant response [MAKE SINGULAR HANDOUT WITH PROPORTIONS]\n\nTable of comparing means [parameter, statistic][proportion, mean]\nsim based apporach\nstandardized statistric\nReject/FTR condtions\nMultiple means handout: how to\n\n&gt;&gt;&gt;“DAY” 24\nchapter 6.3 Theory based approach\n\nTheory based validity conditions\nTheory based SE\nT stat formula\nTheory based CI\nReiterate Reject/FTR conditions\nRestate validity conitions\n\n&gt;&gt;&gt;PAIRED DATA\n&gt;&gt;&gt;LINEAR REGRESSION\n&gt;&gt; [OPTIONAL IDEAS]\n\nproposed Alternatives to p\nIntroduction to data fabrication (show RNG article)\n\n&gt;&gt;&gt;“FINAL PROJECT”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site.\nThis is my first site attempt, there are many things wrong, but it is still much better than I had anticipated."
  },
  {
    "objectID": "handoutA.html",
    "href": "handoutA.html",
    "title": "HandoutA",
    "section": "",
    "text": "DATA: The measured values or categories recorded on an individual of interest.\nPOPULATION: The complete collection of ALL items of interest for a given problem\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: Sub collection of items from the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\n\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”"
  },
  {
    "objectID": "HandoutC.html",
    "href": "HandoutC.html",
    "title": "HandoutC",
    "section": "",
    "text": "RANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\nShort Day!"
  },
  {
    "objectID": "HandoutDE.html",
    "href": "HandoutDE.html",
    "title": "HandoutDE",
    "section": "",
    "text": "Col1\nSample\nStatistics\n(modified latin letters)\nPopulation Parameters\n(greek letters)\n\n\n\n\nProportion/\nCategorical\n(Start with “p”)\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\nMean/\nQuantitative\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nVariance\n\\(s^{2}\\)\n\\(\\sigma^{2}\\)\n\n\n\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis.\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nNOTE: the P Value and P Hat Value are completely different variables\nHypothesis symbols:\n\\(null= H_{0}\\)\n\\(alternative= H_{a}\\)\nLet “k” be the null value (the mean of the null distribution)\nTesting for difference of means\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu=k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu \\neq k\n\\]\nTesting if mean specificially greater than\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu \\leq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu &gt; k\n\\]\nTesting if mean specifically less than\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu \\geq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu &lt; k\n\\]\nLet “k” be the null value (the proportion of the null distribution)\nTesting for difference of proportion\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} =k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} \\neq k\n\\]\nTesting if specificially greater than\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} \\leq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} &gt; k\n\\]\nTesting if specifically less than\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} \\geq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} &lt; k\n\\]\n#the other lesson used an equal sign, but I think this makes sense and the i have seen it on the internet. I am unsure which is correct\nP value as conditional probability\n\\[\nP.Value=\\frac{AlternativeSpace \\cap H_{0}}{H_{0}}\n\\]"
  },
  {
    "objectID": "HandoutG.html",
    "href": "HandoutG.html",
    "title": "HandoutG",
    "section": "",
    "text": "\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nIf we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work.\n###(to explain this decision making process, a flowchart will be attached in the handout)"
  },
  {
    "objectID": "HandoutI.html",
    "href": "HandoutI.html",
    "title": "HandoutI",
    "section": "",
    "text": "The Null Hypothesis is ________\nTrue\nFalse\n\n\n\n\nRejected\nType 1 Error\n“False Positive”\nCorrect\n\n\nNot rejected\nCorrect\nType 2 error\n“False Negative”\n\n\n\nT1ER: Type 1 Error (is when you) Reject\nT2EFTR: Type 2 Error (is when you) Fail To Reject\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LESSONPLAN",
    "section": "",
    "text": "PreSemesterToDo\nReflection\nSyllabus\nCourseSchedule"
  },
  {
    "objectID": "index.html#main-directory",
    "href": "index.html#main-directory",
    "title": "LESSONPLAN",
    "section": "",
    "text": "PreSemesterToDo\nReflection\nSyllabus\nCourseSchedule"
  },
  {
    "objectID": "PLANDOCDAY1.html",
    "href": "PLANDOCDAY1.html",
    "title": "PLANDOCDAY1",
    "section": "",
    "text": "Syllabus Day: 25 minutes\nObjectives:\n\nStudents understand course expectations\nStudents understand the policies in place for the class\nStudents are aware of what resources and accommodations are available to them\nCore rules are stressed enough to be easily remembered\n\nConcepts:\n\nThe syllabus\n\nJust for safety, I will check here to make sure everyone understands mean, median, and mode. You never know where someone is coming from, and I wouldn’t mind going over it.\nActivity: Students will be assigned a starter activity (at home)so I know some basic info about their perception of the class, concerns, and how to address them.\nAssessment:\nn/a\n\n----&gt;First Lecture: 25 minutes\nCourse objectives\n\nStudents know the main datatypes and when to use them\nStudents can draw a connection chart (flowchart labeled how they are related to each other) between classes, histograms, relative frequency histograms, and probability distributions\nStudents can label the shape of distributions fitting the main distribution shapes\n\nConcepts:\n\nvariable\ncategorical vs quantitative\nclasses\nhistograms/relative frequency histograms\nprobability distributions\nshape\n\nIntuition Tools:\n\nProbability distributions as machines that pump out random numbers\nProbability as area, in literal square meters\nProbability distributions as historgrams\nDiscrete as dice\nContinuous as spinner\nRay gun skew\n\nLesson material:\nVARIABLE: this is any measurable quality of an entity\nVariables have two subgroups:\nCATEGORICAL: relating to groups which can be represented by numbers, but do not operate like numbers. Their spacing is either inconsistent or illogical.\nCategorical variables have two groups:\nnominal: groups which have no implied order (“carbs”, “protein”, “fats”)\nordinal: groups which have an implied order, but not necessarily a number (letter grades, outcome of a race)\nQUANTITATIVE: measurements that have numerical values that operate like regular integers/real numbers do. Their spacing is even and predictable.\nQuantitative variable shave two groups:\ndiscrete: If there are finite points between two points in the set, the variable is discrete (people in a room, dice roll points, number of trials succeeded)\n##I need to find out if rationals are discrete or continuous; update, apparently its continuous\n##personal note: some classes discuss the ratio and interval data types, but I had found this setup with continuous and discrete more relevant to this kind of statistics work.\ncontinuous: if there is an infinite number of points between two points, the variable is continuous (time, distance(kinda), mass(big kinda))\nCLASSES: groups of values chunked together, usually to groupn a bunch of values that would be disorderly otherwise\nHISTOGRAM: chart that essentially tallies the number of items that fit in each class in a set, except instead of using tallies, you fill in bars.\nRELATIVE FREQUENCY HISTOGRAM: chart thats exactly like a histogram, but instead of being charted by the number of values in a class, its the number of values in a class divided by the total, forming a fraction between 0 and 1 (equivalently forming a percent between 0% and 100%)\nPROBABILITY DISTRIBUTION: Much like the relative frequency histogram, but instead of thinking of the chart as the outcome of a measurement, it is treated as a machine that pumps out results. The probability of each result is the percentage seen on the side. NOTE: the idea of RVs as the outputs of a machine will be one of the intuition tools used in this class\nImportant note: the probabilities can be added up, as the bars take up a certain amount of area, and area = probability (or at least, this is a way of thinking about it visually, which i intend to use to make statistics more intuitive and visual)\nDISCRETE DISTRIBUTION: Probability distribution, but with discrete values ONLY. Think dice.\nCONTINUOUS DISTRIBUTION: Probability distribution, with an infinite number of values between two points. Think wheel spinner, but with no pegs - just a rainbow of possible hues.\nSHAPE: the literal shape of a distribution, but rather than using traditional shapes, we use words like symmetric, non symmetric ,skewed, unimodal, bimodal, and uniform. (this is where a picture will be provided with these shapes)\nskewed right: mean&gt;median&gt;mode(peak)\nskewed left: mean&lt;median&lt;mode(peak)\nSymmetric unimodal: mean=median=mode\nHere I plan to use the ray gun example on how to remember which skew it is. (the ray gun example is a strategy I came up with to remember skew, if the ray gun is pointing right, its skewed right, if the ray gun is pointing left, its skewed left)\nAssessment:\nDATATYPES: To assess the students knowledge of datatypes, they will be given a list of variables and told to arrange them into certain boxes by their datatype. For example, if the variable “time required to finish a test” is given, it will be put in the variable&gt;quantitative&gt; continuous. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nDISTRIBUTIONS AND CHARTS: Students will be given a list of numbers, and a desired class size, and from that they will be asked to count the classes, make a histogram, then a relative frequency histogram, then a discrete probability distribution by hand. Finally, they will be asked to use the chart determine the probability of getting a number in one of two of the classes, which can be added. #verify accuracy of this last part# DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nSHAPE: Students will be given two exercises to asses the knowledge of shape. The first will be to observe a few distributions and declare their shape, and the next part they will be given a mean, median, and mode (then later just two) and asked to determine the skew of a distribution. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY1.html#day-1",
    "href": "PLANDOCDAY1.html#day-1",
    "title": "PLANDOCDAY1",
    "section": "",
    "text": "Syllabus Day: 25 minutes\nObjectives:\n\nStudents understand course expectations\nStudents understand the policies in place for the class\nStudents are aware of what resources and accommodations are available to them\nCore rules are stressed enough to be easily remembered\n\nConcepts:\n\nThe syllabus\n\nJust for safety, I will check here to make sure everyone understands mean, median, and mode. You never know where someone is coming from, and I wouldn’t mind going over it.\nActivity: Students will be assigned a starter activity (at home)so I know some basic info about their perception of the class, concerns, and how to address them.\nAssessment:\nn/a\n\n----&gt;First Lecture: 25 minutes\nCourse objectives\n\nStudents know the main datatypes and when to use them\nStudents can draw a connection chart (flowchart labeled how they are related to each other) between classes, histograms, relative frequency histograms, and probability distributions\nStudents can label the shape of distributions fitting the main distribution shapes\n\nConcepts:\n\nvariable\ncategorical vs quantitative\nclasses\nhistograms/relative frequency histograms\nprobability distributions\nshape\n\nIntuition Tools:\n\nProbability distributions as machines that pump out random numbers\nProbability as area, in literal square meters\nProbability distributions as historgrams\nDiscrete as dice\nContinuous as spinner\nRay gun skew\n\nLesson material:\nVARIABLE: this is any measurable quality of an entity\nVariables have two subgroups:\nCATEGORICAL: relating to groups which can be represented by numbers, but do not operate like numbers. Their spacing is either inconsistent or illogical.\nCategorical variables have two groups:\nnominal: groups which have no implied order (“carbs”, “protein”, “fats”)\nordinal: groups which have an implied order, but not necessarily a number (letter grades, outcome of a race)\nQUANTITATIVE: measurements that have numerical values that operate like regular integers/real numbers do. Their spacing is even and predictable.\nQuantitative variable shave two groups:\ndiscrete: If there are finite points between two points in the set, the variable is discrete (people in a room, dice roll points, number of trials succeeded)\n##I need to find out if rationals are discrete or continuous; update, apparently its continuous\n##personal note: some classes discuss the ratio and interval data types, but I had found this setup with continuous and discrete more relevant to this kind of statistics work.\ncontinuous: if there is an infinite number of points between two points, the variable is continuous (time, distance(kinda), mass(big kinda))\nCLASSES: groups of values chunked together, usually to groupn a bunch of values that would be disorderly otherwise\nHISTOGRAM: chart that essentially tallies the number of items that fit in each class in a set, except instead of using tallies, you fill in bars.\nRELATIVE FREQUENCY HISTOGRAM: chart thats exactly like a histogram, but instead of being charted by the number of values in a class, its the number of values in a class divided by the total, forming a fraction between 0 and 1 (equivalently forming a percent between 0% and 100%)\nPROBABILITY DISTRIBUTION: Much like the relative frequency histogram, but instead of thinking of the chart as the outcome of a measurement, it is treated as a machine that pumps out results. The probability of each result is the percentage seen on the side. NOTE: the idea of RVs as the outputs of a machine will be one of the intuition tools used in this class\nImportant note: the probabilities can be added up, as the bars take up a certain amount of area, and area = probability (or at least, this is a way of thinking about it visually, which i intend to use to make statistics more intuitive and visual)\nDISCRETE DISTRIBUTION: Probability distribution, but with discrete values ONLY. Think dice.\nCONTINUOUS DISTRIBUTION: Probability distribution, with an infinite number of values between two points. Think wheel spinner, but with no pegs - just a rainbow of possible hues.\nSHAPE: the literal shape of a distribution, but rather than using traditional shapes, we use words like symmetric, non symmetric ,skewed, unimodal, bimodal, and uniform. (this is where a picture will be provided with these shapes)\nskewed right: mean&gt;median&gt;mode(peak)\nskewed left: mean&lt;median&lt;mode(peak)\nSymmetric unimodal: mean=median=mode\nHere I plan to use the ray gun example on how to remember which skew it is. (the ray gun example is a strategy I came up with to remember skew, if the ray gun is pointing right, its skewed right, if the ray gun is pointing left, its skewed left)\nAssessment:\nDATATYPES: To assess the students knowledge of datatypes, they will be given a list of variables and told to arrange them into certain boxes by their datatype. For example, if the variable “time required to finish a test” is given, it will be put in the variable&gt;quantitative&gt; continuous. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nDISTRIBUTIONS AND CHARTS: Students will be given a list of numbers, and a desired class size, and from that they will be asked to count the classes, make a histogram, then a relative frequency histogram, then a discrete probability distribution by hand. Finally, they will be asked to use the chart determine the probability of getting a number in one of two of the classes, which can be added. #verify accuracy of this last part# DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nSHAPE: Students will be given two exercises to asses the knowledge of shape. The first will be to observe a few distributions and declare their shape, and the next part they will be given a mean, median, and mode (then later just two) and asked to determine the skew of a distribution. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY11.html",
    "href": "PLANDOCDAY11.html",
    "title": "PLANDOCDAY11",
    "section": "",
    "text": "Objectives:\n\nStudents can describe how confidence level effects the CI\nStudents understand the main purpose and meaning of a confidence interval\nStudents understand the CI describes the sampling distribution, not the original data as a whole\nStudents know the meaning of the margin of error\nStudents understand confidence level as something you set prior, not as a result of the data\nStudents can describe how confidence level effects the CI\n\nConcepts:\n\nConfidence Interval\nSampling distribution\nMargin of Error\nConfidence Level\nStandard Error\n\nIntuition Tools:\n\nExtreme Confidence Levels\n\nLesson Material:\nCONFIDENCE INTERVAL: A range of values where there is a probability (The confidence level) that the parameter is within. For example, if we have a Confidence interval of (-1,1) and a confidence level of 95%, and this confidence interval is estimating the mean, then we are 95% confident that the population mean is between -1 and 1.\nCONFIDENCE LEVEL: A value of how much certainty is desired when estimating the parameter. You can be more confident in larger ranges of numbers, but larger intervals are less specific. For example, I may be 80% confident I will end the semester with a grade between 85% and 95%, but I’m nearly 100% confident I will get a grade between -100,000% and +100,000%.\nSAMPLING DISTRIBUTION: Probability distribution describing where you predict the true estimate to be. Usually we are use to predictions being single values, but the sampling distribution is a whole distribution of probabilities. To be statistically honest, you arent saying a single value, but instead claiming that there are some values that are more likely than others. {this needs some work}.\nSTANDARD ERROR: Estimate of the variance of the sampling distribution. More varied data will be more difficult to have small confidence intervals, and parameters will be harder to estimate. This is estimated by dividing the variance by the sample size\nMARGIN OF ERROR: Value that shows how large the confidence interval will be, it is formed by multiplying the confidence level by the standard error.\nCENTRAL LIMIT THEOREM: Can be complex, but essentially drawing multiple samples from the same distribution will likely arrive at the normal distribution.\nEFFECT OF MORE SAMPLES: Sampling distribution becomes more normal, Standard error goes down, confidence interval shrinks.\nAssessment: {some of these may be saved for later}\n“you find that the confidence interval is 95% between -5 and +5, so are 95% of values in the sample within (-5,+5)” FALSE, its the sampling distribution\n“Why dont we use 100% confidence intervals to give absolute certainty with out estimations” They would describe all possible values and thus be useless\n“Given a mean value of mu and a continuous distribution, what is the 0% confidence interval?” (mu,mu) or just mu"
  },
  {
    "objectID": "PLANDOCDAY11.html#day-11",
    "href": "PLANDOCDAY11.html#day-11",
    "title": "PLANDOCDAY11",
    "section": "",
    "text": "Objectives:\n\nStudents can describe how confidence level effects the CI\nStudents understand the main purpose and meaning of a confidence interval\nStudents understand the CI describes the sampling distribution, not the original data as a whole\nStudents know the meaning of the margin of error\nStudents understand confidence level as something you set prior, not as a result of the data\nStudents can describe how confidence level effects the CI\n\nConcepts:\n\nConfidence Interval\nSampling distribution\nMargin of Error\nConfidence Level\nStandard Error\n\nIntuition Tools:\n\nExtreme Confidence Levels\n\nLesson Material:\nCONFIDENCE INTERVAL: A range of values where there is a probability (The confidence level) that the parameter is within. For example, if we have a Confidence interval of (-1,1) and a confidence level of 95%, and this confidence interval is estimating the mean, then we are 95% confident that the population mean is between -1 and 1.\nCONFIDENCE LEVEL: A value of how much certainty is desired when estimating the parameter. You can be more confident in larger ranges of numbers, but larger intervals are less specific. For example, I may be 80% confident I will end the semester with a grade between 85% and 95%, but I’m nearly 100% confident I will get a grade between -100,000% and +100,000%.\nSAMPLING DISTRIBUTION: Probability distribution describing where you predict the true estimate to be. Usually we are use to predictions being single values, but the sampling distribution is a whole distribution of probabilities. To be statistically honest, you arent saying a single value, but instead claiming that there are some values that are more likely than others. {this needs some work}.\nSTANDARD ERROR: Estimate of the variance of the sampling distribution. More varied data will be more difficult to have small confidence intervals, and parameters will be harder to estimate. This is estimated by dividing the variance by the sample size\nMARGIN OF ERROR: Value that shows how large the confidence interval will be, it is formed by multiplying the confidence level by the standard error.\nCENTRAL LIMIT THEOREM: Can be complex, but essentially drawing multiple samples from the same distribution will likely arrive at the normal distribution.\nEFFECT OF MORE SAMPLES: Sampling distribution becomes more normal, Standard error goes down, confidence interval shrinks.\nAssessment: {some of these may be saved for later}\n“you find that the confidence interval is 95% between -5 and +5, so are 95% of values in the sample within (-5,+5)” FALSE, its the sampling distribution\n“Why dont we use 100% confidence intervals to give absolute certainty with out estimations” They would describe all possible values and thus be useless\n“Given a mean value of mu and a continuous distribution, what is the 0% confidence interval?” (mu,mu) or just mu"
  },
  {
    "objectID": "PLANDOCDAY13.html",
    "href": "PLANDOCDAY13.html",
    "title": "PLANDOCDAY13",
    "section": "",
    "text": "Objectives:\nStudents can decompose and rewire the CI formula\nStudents can calculate mean confidence intervals with the theory based method\nConcepts\nmean intervals\nt score\nMoE mean\nSE mean\nmean CI Validity condiitons\nIntuition Tools:\nLesson Material:\nStudents will be given the following, they will be instructed to decompose the formula {i did this one in class when I taught, it seemed successfull}\n\n\n\n\nSample Statistic\nPopulation Parameter\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nProportion\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\nVariance\n\\(s^{2}\\)\n\\(\\sigma^{2}\\)\n\n\n\nProportion CI validity condtions:\n-At least 10 observational units in both categories (10 successes and 10 failures)\nMean CI validity conditions:\n-Quantitative Variable\n-Symmetric Distribution\n-At least 20 Observations\nCI=Observered_Statistic \\(\\pm\\) Margin_Of_Error\nObserved_Statistic = \\(\\hat{p}\\) {if working with a proportion}\nObserved_Statistic = \\(\\bar{x}\\) {if working with a mean}\nMargin_Of_Error = Multiplier*Standard_Error\nStandard_Error = \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p}) }{n}}\\) {If working with proportion}\nStandard_Error = \\(\\sqrt{\\frac{s^{2} }{ n} }\\) {if working with a mean}\nMultiplier=\\(Z_{\\alpha/2}\\) {if working with a proportion}\nMultiplier=\\(t_{\\alpha/2, df}\\) {if working with a mean}\n\\(Z_{\\alpha/2}\\) = 1.96 {IF confidence level is 95%}\n\\(t_{\\alpha/2, df}\\) = {table required}\ndf=n-1\nVALDIDTIY CONDTIOINS\nAt least 10 observational units in both categories (10 successes and 10 failures)\nAssessment:\nYou sampled 100 people at a movie theater to rate some movie on a scale from 1 to 10. The average rating in the sample is 7.5 out of 10, with a sample variance of 2, what is the 95% confidence interval for the watcher rating of the movie. Are the validity conditions met?\nIn a sample of 90 people, the average number of cantaloupes purchased per person on their last grocery run was 3.1 with a sample standard deviation of 5, what is the 95% confidence interval for the average amount of cantaloupes purchased for all customers? Are the validity conditions met?\nINCLUDE t TABLE IN HANDOUT"
  },
  {
    "objectID": "PLANDOCDAY13.html#day-13",
    "href": "PLANDOCDAY13.html#day-13",
    "title": "PLANDOCDAY13",
    "section": "",
    "text": "Objectives:\nStudents can decompose and rewire the CI formula\nStudents can calculate mean confidence intervals with the theory based method\nConcepts\nmean intervals\nt score\nMoE mean\nSE mean\nmean CI Validity condiitons\nIntuition Tools:\nLesson Material:\nStudents will be given the following, they will be instructed to decompose the formula {i did this one in class when I taught, it seemed successfull}\n\n\n\n\nSample Statistic\nPopulation Parameter\n\n\nMean\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nProportion\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\nVariance\n\\(s^{2}\\)\n\\(\\sigma^{2}\\)\n\n\n\nProportion CI validity condtions:\n-At least 10 observational units in both categories (10 successes and 10 failures)\nMean CI validity conditions:\n-Quantitative Variable\n-Symmetric Distribution\n-At least 20 Observations\nCI=Observered_Statistic \\(\\pm\\) Margin_Of_Error\nObserved_Statistic = \\(\\hat{p}\\) {if working with a proportion}\nObserved_Statistic = \\(\\bar{x}\\) {if working with a mean}\nMargin_Of_Error = Multiplier*Standard_Error\nStandard_Error = \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p}) }{n}}\\) {If working with proportion}\nStandard_Error = \\(\\sqrt{\\frac{s^{2} }{ n} }\\) {if working with a mean}\nMultiplier=\\(Z_{\\alpha/2}\\) {if working with a proportion}\nMultiplier=\\(t_{\\alpha/2, df}\\) {if working with a mean}\n\\(Z_{\\alpha/2}\\) = 1.96 {IF confidence level is 95%}\n\\(t_{\\alpha/2, df}\\) = {table required}\ndf=n-1\nVALDIDTIY CONDTIOINS\nAt least 10 observational units in both categories (10 successes and 10 failures)\nAssessment:\nYou sampled 100 people at a movie theater to rate some movie on a scale from 1 to 10. The average rating in the sample is 7.5 out of 10, with a sample variance of 2, what is the 95% confidence interval for the watcher rating of the movie. Are the validity conditions met?\nIn a sample of 90 people, the average number of cantaloupes purchased per person on their last grocery run was 3.1 with a sample standard deviation of 5, what is the 95% confidence interval for the average amount of cantaloupes purchased for all customers? Are the validity conditions met?\nINCLUDE t TABLE IN HANDOUT"
  },
  {
    "objectID": "PLANDOCDAY16.html",
    "href": "PLANDOCDAY16.html",
    "title": "PLANDOCDAY16",
    "section": "",
    "text": "NOTE: Alongside confidence intervals, I would really like to spend more time on categorical response as it is SO COMMON in life. Students may not be making regression models in the mid-distant future, but it is very likely that they will be confronted with confidence intervals and even more so likely that they will have a “difference of two groups” question that involves a categorical response. There inst as much as I would like here for now, so i plan to develop it more in the future.\nObjectives:\nStudents know how to compare groups of categorical response and explanatory\nStudents know how to use conditional proporiton tables\nStudents understand relative risk\nStudents can us the two proportions simulate differences applet\nConcepts:\nCategorical Response\nConditional Probability\nConditional Proportions\nRelative Risk\nSimulated differences\nConfidence intervals\nChi squared\nIntuition tools:\nLesson Material:\nThe lesson will start with a review of the earlier material on conditional probability\nWe will then go on to use a conditional proportion chart, simply examining the meaning without doing any inference.\nAfter this, we will ask if the groups are different, and then after some surface inspection, we will then begin calculating the conditional porportions.\nFrom this, we will then define and calculate relative risk\nRELATIVE RISK: Ratio between two conditional proporitions. or, “how many times large the risk of an outcome is for one group compared to another”\n=-=-=\nUsing the proportion tables, we will then use the sim to calculated simulated differences of two groups to do significance testing.\nFrom that, we will then calculate the sim standardized statistic\n\\[\n\\frac{Observed Stat-Hypothesized Null}{StDev Of Null}=\\frac{(\\hat{p_1}-\\hat{p_2})-0}{SimulatedFromApplet}\n\\]\nLater we will also work with the theory based approach\nValidity contentions (at least 10 observations in each box)\n\\[\nSE=\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\n\\[\n\\hat{p}=\\frac{s1+s2}{n1+n2}\n\\]\nFrom that applying the ci\n\\[\n(p^*_1-p^*_2)\\pm 2*\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\nYa know, because 2 is usually good enough.\nFrom this, I was considering touching on fishers exact test and chi square, but really chi square should be enough since it is used frequently in the biology course they will be taking.\nCHI SQUARE:::NEED MORE INVESTIGATION\n\\[\n\\chi^2_{df}=\\sum\\frac{(O-E)^2}{E}\n\\]\nWhere E is the expected under the null, and O is the observed value of the cell\nUseful just about everywhere.\nAssessment:\nStudents will be given counts of fly genotypes and expected ratios for those genotypes. Using a chi square test, they will be asked to determine if the flies are mating randomly.\nStudents will be given a box of proportions, they will be asked to find the conditional proportions of two conditions and the relative risk between the two.\nStudents will then use the aforementioned box to calculate the confidence interval of the difference of the two groups\nStudents will be asked to do a similar task but using the simulated differences applet.\nStudents may be given a chi square problem with a 0 expected and asked what to do (this happened to me in a bio course)"
  },
  {
    "objectID": "PLANDOCDAY16.html#day-16171819",
    "href": "PLANDOCDAY16.html#day-16171819",
    "title": "PLANDOCDAY16",
    "section": "",
    "text": "NOTE: Alongside confidence intervals, I would really like to spend more time on categorical response as it is SO COMMON in life. Students may not be making regression models in the mid-distant future, but it is very likely that they will be confronted with confidence intervals and even more so likely that they will have a “difference of two groups” question that involves a categorical response. There inst as much as I would like here for now, so i plan to develop it more in the future.\nObjectives:\nStudents know how to compare groups of categorical response and explanatory\nStudents know how to use conditional proporiton tables\nStudents understand relative risk\nStudents can us the two proportions simulate differences applet\nConcepts:\nCategorical Response\nConditional Probability\nConditional Proportions\nRelative Risk\nSimulated differences\nConfidence intervals\nChi squared\nIntuition tools:\nLesson Material:\nThe lesson will start with a review of the earlier material on conditional probability\nWe will then go on to use a conditional proportion chart, simply examining the meaning without doing any inference.\nAfter this, we will ask if the groups are different, and then after some surface inspection, we will then begin calculating the conditional porportions.\nFrom this, we will then define and calculate relative risk\nRELATIVE RISK: Ratio between two conditional proporitions. or, “how many times large the risk of an outcome is for one group compared to another”\n=-=-=\nUsing the proportion tables, we will then use the sim to calculated simulated differences of two groups to do significance testing.\nFrom that, we will then calculate the sim standardized statistic\n\\[\n\\frac{Observed Stat-Hypothesized Null}{StDev Of Null}=\\frac{(\\hat{p_1}-\\hat{p_2})-0}{SimulatedFromApplet}\n\\]\nLater we will also work with the theory based approach\nValidity contentions (at least 10 observations in each box)\n\\[\nSE=\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\n\\[\n\\hat{p}=\\frac{s1+s2}{n1+n2}\n\\]\nFrom that applying the ci\n\\[\n(p^*_1-p^*_2)\\pm 2*\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}\n\\]\nYa know, because 2 is usually good enough.\nFrom this, I was considering touching on fishers exact test and chi square, but really chi square should be enough since it is used frequently in the biology course they will be taking.\nCHI SQUARE:::NEED MORE INVESTIGATION\n\\[\n\\chi^2_{df}=\\sum\\frac{(O-E)^2}{E}\n\\]\nWhere E is the expected under the null, and O is the observed value of the cell\nUseful just about everywhere.\nAssessment:\nStudents will be given counts of fly genotypes and expected ratios for those genotypes. Using a chi square test, they will be asked to determine if the flies are mating randomly.\nStudents will be given a box of proportions, they will be asked to find the conditional proportions of two conditions and the relative risk between the two.\nStudents will then use the aforementioned box to calculate the confidence interval of the difference of the two groups\nStudents will be asked to do a similar task but using the simulated differences applet.\nStudents may be given a chi square problem with a 0 expected and asked what to do (this happened to me in a bio course)"
  },
  {
    "objectID": "PLANDOCDAY20.html",
    "href": "PLANDOCDAY20.html",
    "title": "PLANDOCDAY20",
    "section": "",
    "text": "Section 4.1/2\nObjectives:\nStudents can explain what explanatory and response variables are and why we use those terms instead of independent and dependent in statistics\nstudents can describe and identify experiment and observational studies\nstudents understand the effect confounding/lurking variables have on studies\nstudents can make study diagrams\nstudents know correlations isnt _____\nStudent know the role of random assignment in establishing cuasation\nStudents have some understanding of blocking\nConcepts:\nexplanatory\nresponse\nconfounding/lurking variable\nexperiment\nobservational study\nStudy diagramming\nIntuition tools:\nLesson Material:\nWe will begin by reviewing the explanatory and response variables, we have gone over these a lot, but this time it is very important that there arent any mistake made in defining these and sometimes repeating can help when its later in the semester.\nWe will then go over a weird correlation, and then add a lurking variable that makes everything make sense. In this case, the ice cream shark attack example may be used.\nFrom that we will point out that the heat wave is a lurking. confounding variable, and modify the study diagram.\nWe will then talk about the two different studies, and how experiments differ from observation. we will review the strengths of each including:\n-establishment of causation\n-controlling for factors that coincide with the other\n-ethical/economic limits\nSpecifically, we will need to stress the importance fo random assignment in the experiments, otherwise a similar effect could still take place.\nWe will also talk about some side limits that can still infer with experiments, such as measuring the wrong characteristic or not being able to randomize certain aspects.\nWe will then briefly discuss blocking as a way to generate conlcusions.\nAssessment:\nStudents will be given a simple study and be asked to diagram it.\nStudents will then be told about a confound and asked to include it in the diagram\nStudents will be given a question with certain goals in mind, and they will be asked to determine what kind of study they should use (observation of experiment)\nStudents will be asked to identify the randomization (and what is not) in an experiment\nStudents will be given a situation with some wildly differing groups and be asked what to block."
  },
  {
    "objectID": "PLANDOCDAY20.html#day-20",
    "href": "PLANDOCDAY20.html#day-20",
    "title": "PLANDOCDAY20",
    "section": "",
    "text": "Section 4.1/2\nObjectives:\nStudents can explain what explanatory and response variables are and why we use those terms instead of independent and dependent in statistics\nstudents can describe and identify experiment and observational studies\nstudents understand the effect confounding/lurking variables have on studies\nstudents can make study diagrams\nstudents know correlations isnt _____\nStudent know the role of random assignment in establishing cuasation\nStudents have some understanding of blocking\nConcepts:\nexplanatory\nresponse\nconfounding/lurking variable\nexperiment\nobservational study\nStudy diagramming\nIntuition tools:\nLesson Material:\nWe will begin by reviewing the explanatory and response variables, we have gone over these a lot, but this time it is very important that there arent any mistake made in defining these and sometimes repeating can help when its later in the semester.\nWe will then go over a weird correlation, and then add a lurking variable that makes everything make sense. In this case, the ice cream shark attack example may be used.\nFrom that we will point out that the heat wave is a lurking. confounding variable, and modify the study diagram.\nWe will then talk about the two different studies, and how experiments differ from observation. we will review the strengths of each including:\n-establishment of causation\n-controlling for factors that coincide with the other\n-ethical/economic limits\nSpecifically, we will need to stress the importance fo random assignment in the experiments, otherwise a similar effect could still take place.\nWe will also talk about some side limits that can still infer with experiments, such as measuring the wrong characteristic or not being able to randomize certain aspects.\nWe will then briefly discuss blocking as a way to generate conlcusions.\nAssessment:\nStudents will be given a simple study and be asked to diagram it.\nStudents will then be told about a confound and asked to include it in the diagram\nStudents will be given a question with certain goals in mind, and they will be asked to determine what kind of study they should use (observation of experiment)\nStudents will be asked to identify the randomization (and what is not) in an experiment\nStudents will be given a situation with some wildly differing groups and be asked what to block."
  },
  {
    "objectID": "PLANDOCDAY22.html",
    "href": "PLANDOCDAY22.html",
    "title": "PLANDOCDAY22",
    "section": "",
    "text": "Objectives:\nStudents understand how interaction works in daily life\nStudents can read an interaction graph\nStudents know the slope method for detecting interacction\nStudents know the dot on dot method for detecting interaction (I made this up)\nStudents know how interaction can play a role in deceptive results\nConcepts:\nInteraction\nGeneralizability (again)\nIntuition Tools:\nDot on dot: Same initial conditions\nLesson Material:\nStudents will be first introduced to a concept they are likely familiar with but maybe didnt know the scientific/statistical relevance of: Some things help in some contexts but hurt in others, and sometimes things are more than the sum of their parts.\nWe will return to the 2x2 table with two groups of two categorical (group:(environment a; environment b) and treatment:(on; off))\nNow for the example, there is the example of caffeine and extroversion (need to find source), but something this one sounds more fun\n\nlibrary(ggplot2)\n\ngroup&lt;-rep(c( 'with spouse','believes they are  alone'),each=2)\ntrt&lt;-rep(c('control', 'shoulder massage'),times=2)\n#Y&lt;-c(4,10,6,0)\nY&lt;-factor(c('calm','eepy','some concerns','AAAAAA'), \nlevels=c('AAAAAA', 'STRESSED', 'stressed','uncomfortable', 'some concerns',\n         'mid', 'calm', 'at peace', 'relaxed', 'no concerns', 'eepy'),\nordered=TRUE)\n\ndf&lt;-data.frame(group,trt,Y)\n\ncuscolor&lt;-c('believes they are  alone'='green','with spouse'='green')\n\nggplot(df, aes(x=trt, y=Y, group=group, color=group))+\n  geom_line(size=2, aes(linetype=group))+\n  geom_point(size=2)+\n  labs(title='effect of unannounced shoulder massage on emotional state',\n       x='treatment given',\n       y='emotioanal state') +\n  theme_bw()+\n  scale_linetype()+\n  scale_color_manual(values=cuscolor)+\n  theme(\n    panel.background = element_rect(fill = \"black\"),\n    plot.background = element_rect(fill = \"black\"),\n    text = element_text(color = \"green\"),\n    legend.text = element_text(color = \"green\"),\n    legend.title = element_text(color = \"green\"),\n    legend.key = element_rect(fill=\"black\"),\n    legend.background = element_rect(fill=\"black\"),\n    axis.text = element_text(color = \"green\"),\n    axis.title = element_text(color = \"green\"),\n    panel.grid = element_line(color = \"green\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe will then go over the different types of interaction {widening, closening, crossing, flat and not flat} and non interaction {both up, both down, both flat} and starting conditions {on top, different} (I plan to make an image table with each instance, but I will have tho think of a way to organize it)\nWe will first cover that interaction is when the slope is not approx equivalent, and then we will go over the method where you line up the starter points, then see if they end up in the same location, which is a method i made up in the event that anyone had missed slope for some reason.\nWe will then talk about how context can make or break certain treatments, and how categorizing things as effective, noneffective,helpful, or harmful is potentially deceptive. Interaction and generalizability can go hand in hand\nAssessment:\nThe first assessment is very simple, students will be given a list of pictures (no words or numbers) and be asked to determine if interaction is present\n“a study where patients were locked in a fallout shelter for 30 days with dwindling food supply found that handing them a stick of dynamite significantly decreased their stress levels, your colleague reads this and suggests handing out sticks of dynamite as a way to decrease stress levels. Why is his idea flawed?” The study is not generalizable because there is likely an interaction effect between being handed dynamite and being in a dying fallout shelter.\nStudents will be asked to come up with a few examples of situations where an interaction effect is likely present."
  },
  {
    "objectID": "PLANDOCDAY22.html#day-22",
    "href": "PLANDOCDAY22.html#day-22",
    "title": "PLANDOCDAY22",
    "section": "",
    "text": "Objectives:\nStudents understand how interaction works in daily life\nStudents can read an interaction graph\nStudents know the slope method for detecting interacction\nStudents know the dot on dot method for detecting interaction (I made this up)\nStudents know how interaction can play a role in deceptive results\nConcepts:\nInteraction\nGeneralizability (again)\nIntuition Tools:\nDot on dot: Same initial conditions\nLesson Material:\nStudents will be first introduced to a concept they are likely familiar with but maybe didnt know the scientific/statistical relevance of: Some things help in some contexts but hurt in others, and sometimes things are more than the sum of their parts.\nWe will return to the 2x2 table with two groups of two categorical (group:(environment a; environment b) and treatment:(on; off))\nNow for the example, there is the example of caffeine and extroversion (need to find source), but something this one sounds more fun\n\nlibrary(ggplot2)\n\ngroup&lt;-rep(c( 'with spouse','believes they are  alone'),each=2)\ntrt&lt;-rep(c('control', 'shoulder massage'),times=2)\n#Y&lt;-c(4,10,6,0)\nY&lt;-factor(c('calm','eepy','some concerns','AAAAAA'), \nlevels=c('AAAAAA', 'STRESSED', 'stressed','uncomfortable', 'some concerns',\n         'mid', 'calm', 'at peace', 'relaxed', 'no concerns', 'eepy'),\nordered=TRUE)\n\ndf&lt;-data.frame(group,trt,Y)\n\ncuscolor&lt;-c('believes they are  alone'='green','with spouse'='green')\n\nggplot(df, aes(x=trt, y=Y, group=group, color=group))+\n  geom_line(size=2, aes(linetype=group))+\n  geom_point(size=2)+\n  labs(title='effect of unannounced shoulder massage on emotional state',\n       x='treatment given',\n       y='emotioanal state') +\n  theme_bw()+\n  scale_linetype()+\n  scale_color_manual(values=cuscolor)+\n  theme(\n    panel.background = element_rect(fill = \"black\"),\n    plot.background = element_rect(fill = \"black\"),\n    text = element_text(color = \"green\"),\n    legend.text = element_text(color = \"green\"),\n    legend.title = element_text(color = \"green\"),\n    legend.key = element_rect(fill=\"black\"),\n    legend.background = element_rect(fill=\"black\"),\n    axis.text = element_text(color = \"green\"),\n    axis.title = element_text(color = \"green\"),\n    panel.grid = element_line(color = \"green\")\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nWe will then go over the different types of interaction {widening, closening, crossing, flat and not flat} and non interaction {both up, both down, both flat} and starting conditions {on top, different} (I plan to make an image table with each instance, but I will have tho think of a way to organize it)\nWe will first cover that interaction is when the slope is not approx equivalent, and then we will go over the method where you line up the starter points, then see if they end up in the same location, which is a method i made up in the event that anyone had missed slope for some reason.\nWe will then talk about how context can make or break certain treatments, and how categorizing things as effective, noneffective,helpful, or harmful is potentially deceptive. Interaction and generalizability can go hand in hand\nAssessment:\nThe first assessment is very simple, students will be given a list of pictures (no words or numbers) and be asked to determine if interaction is present\n“a study where patients were locked in a fallout shelter for 30 days with dwindling food supply found that handing them a stick of dynamite significantly decreased their stress levels, your colleague reads this and suggests handing out sticks of dynamite as a way to decrease stress levels. Why is his idea flawed?” The study is not generalizable because there is likely an interaction effect between being handed dynamite and being in a dying fallout shelter.\nStudents will be asked to come up with a few examples of situations where an interaction effect is likely present."
  },
  {
    "objectID": "PLANDOCDAY24.html",
    "href": "PLANDOCDAY24.html",
    "title": "PLANDOCDAY24",
    "section": "",
    "text": "Objectives:\nStudents can use the multiple means applet to do theory based comparison of two means.\nStudents can purely-visually compare means.\nConcepts:\nConfidence intervals (again)\nStandard error ~ stdev ~ 95%ci/2\nT test\nF ratio (BRIEF)\nStudents know how to use error bars to reject or FTR the null (non difference)\nIntuition tools:\nTwo lumps, visual comparison of groups (need large df 60+ and confidence level of .95)\nLumps with lines (stdev)\nlumps with lines(confidence intervals=variance)\nLesson Material:\nWe will start with the validity conditions of theory based mm comparison.\nThis lesson will show some equations for the t test value and the confidence interval formula\n\\[\nt=\\frac{(x_1-x_2)-0}{SE}=\\frac{x_1-x_2}{\\sqrt{s^2_1/n_1+s^2_2/n_2}}\n\\]\n\\[\nCI=\\bar{x_1}-\\bar{x_1} \\pm multiplier*\\sqrt{s^2_1/n_1+s^2_2/n_2}\n\\]\nAfter some computation, students will then be (re?) introduced to the normal distribution visual SD method (going about .6 down the mode or using the inflection point) and the reintroduced to the 2SD method. Students will also be reminded that the SE is the (est) StDev of the sampling distribution, and that the confidence level can be assumed to be 2 if 95% confidence is desired.\n\n# edited code, not mine completely, just wanted to quickly demonstrate something visually\nmean_value &lt;- 0\nsd_value &lt;- 1\n\nx &lt;- seq(-4, 4, length=1000)\n\ny &lt;- dnorm(x, mean = mean_value, sd = sd_value)\n\nplot(x, y, type = \"l\", col = \"blue\", lwd = 2, xlab = \"x\", ylab = \"Density\",\n     main = \"Normal distribution 2SD CI\")\n\nabline(v = mean_value, col = \"red\", lwd = 2)\n\nabline(h = max(y)*.6, col = \"green\", lwd = 2)\n\nsegments(x0 = -sd_value, x1 = mean_value, y0 = dnorm(-1, mean = mean_value, sd = sd_value),\n         y1 = dnorm(-1, mean = mean_value, sd = sd_value), col = \"purple\", lwd = 2)\nsegments(x0 = -sd_value*2, x1 = mean_value, y0 = dnorm(2, mean = mean_value, sd = sd_value),\n         y1 = dnorm(2, mean = mean_value, sd = sd_value), col = \"purple\", lwd = 2)\n\nsegments(x0 = -sd_value*2, x1 = sd_value*2, y0 = max(y),\n         y1 = max(y), col = \"grey\", lwd = 2)\n\n\n\n\nAssessment:\nStudents will start off with two groups of images each with two shaded lumps and be asked to find which one is more significantly different.\n-Students will then be asked to write aproximate the stdeviation and confidence intervals using the visual method\n-Studetns will then accept or reject using those CIs\n-using combined stdev, they will be asked to calcuklate the sqrt(f) ratio (MAYBE NOT HAVE THIS)\n(currently looking into the visual connection of the F ratio, so this is a work in progress. The within variance is easy to find visually, the between variance is the issue right now. Im basically looking for a way to calculate the between variance using only two normal distributions with means and variance known. I will look into this at a later time)\nThere willl also be some formula using questions just to make sure they can use the provided formula. These will be fairly standard and uninteresting but still usefull"
  },
  {
    "objectID": "PLANDOCDAY24.html#day-24",
    "href": "PLANDOCDAY24.html#day-24",
    "title": "PLANDOCDAY24",
    "section": "",
    "text": "Objectives:\nStudents can use the multiple means applet to do theory based comparison of two means.\nStudents can purely-visually compare means.\nConcepts:\nConfidence intervals (again)\nStandard error ~ stdev ~ 95%ci/2\nT test\nF ratio (BRIEF)\nStudents know how to use error bars to reject or FTR the null (non difference)\nIntuition tools:\nTwo lumps, visual comparison of groups (need large df 60+ and confidence level of .95)\nLumps with lines (stdev)\nlumps with lines(confidence intervals=variance)\nLesson Material:\nWe will start with the validity conditions of theory based mm comparison.\nThis lesson will show some equations for the t test value and the confidence interval formula\n\\[\nt=\\frac{(x_1-x_2)-0}{SE}=\\frac{x_1-x_2}{\\sqrt{s^2_1/n_1+s^2_2/n_2}}\n\\]\n\\[\nCI=\\bar{x_1}-\\bar{x_1} \\pm multiplier*\\sqrt{s^2_1/n_1+s^2_2/n_2}\n\\]\nAfter some computation, students will then be (re?) introduced to the normal distribution visual SD method (going about .6 down the mode or using the inflection point) and the reintroduced to the 2SD method. Students will also be reminded that the SE is the (est) StDev of the sampling distribution, and that the confidence level can be assumed to be 2 if 95% confidence is desired.\n\n# edited code, not mine completely, just wanted to quickly demonstrate something visually\nmean_value &lt;- 0\nsd_value &lt;- 1\n\nx &lt;- seq(-4, 4, length=1000)\n\ny &lt;- dnorm(x, mean = mean_value, sd = sd_value)\n\nplot(x, y, type = \"l\", col = \"blue\", lwd = 2, xlab = \"x\", ylab = \"Density\",\n     main = \"Normal distribution 2SD CI\")\n\nabline(v = mean_value, col = \"red\", lwd = 2)\n\nabline(h = max(y)*.6, col = \"green\", lwd = 2)\n\nsegments(x0 = -sd_value, x1 = mean_value, y0 = dnorm(-1, mean = mean_value, sd = sd_value),\n         y1 = dnorm(-1, mean = mean_value, sd = sd_value), col = \"purple\", lwd = 2)\nsegments(x0 = -sd_value*2, x1 = mean_value, y0 = dnorm(2, mean = mean_value, sd = sd_value),\n         y1 = dnorm(2, mean = mean_value, sd = sd_value), col = \"purple\", lwd = 2)\n\nsegments(x0 = -sd_value*2, x1 = sd_value*2, y0 = max(y),\n         y1 = max(y), col = \"grey\", lwd = 2)\n\n\n\n\nAssessment:\nStudents will start off with two groups of images each with two shaded lumps and be asked to find which one is more significantly different.\n-Students will then be asked to write aproximate the stdeviation and confidence intervals using the visual method\n-Studetns will then accept or reject using those CIs\n-using combined stdev, they will be asked to calcuklate the sqrt(f) ratio (MAYBE NOT HAVE THIS)\n(currently looking into the visual connection of the F ratio, so this is a work in progress. The within variance is easy to find visually, the between variance is the issue right now. Im basically looking for a way to calculate the between variance using only two normal distributions with means and variance known. I will look into this at a later time)\nThere willl also be some formula using questions just to make sure they can use the provided formula. These will be fairly standard and uninteresting but still usefull"
  },
  {
    "objectID": "PLANDOCDAY4.html",
    "href": "PLANDOCDAY4.html",
    "title": "PLANDOCDAY4",
    "section": "",
    "text": "Short concept Time - Variability: 50 minutes\nObjectives:\n\nStudents know how to identify the measure of central tendency of a distribution, visually\nStudents can calculate standard deviation from variance, and variance from standard deviation\nStudents can look at two graphs and estimate which one has a higher variance\n\nConcepts:\n\nReview: relative frequency histograms\nReview: probability distributions\nReview: shape\nReview: Central Tendencies (visual review)\nVariability: variance and stdev\noutliers\n\nIntuition Tools:\n\nDot counting\nVariability as big boot\n\nLesson Material:\nMost of these concepts will be review, so I will quickly elaborate:\nMEASURES OF CENTRAL TENDENCY: mean, median, mode\nThen we will show a graph with a visible outlier, except we will note that outliers are somewhat subjective and not a mathematical truth, and that studies rarely just remove outliers as outliers can be expected\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances.\nWe will then go over some visual examples of higher variability in distributions, and that variacne and standard deviation are a means to measure this variability.\nWe will then state that the stdeviation is just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nIt will be noted that variance is calculated by taking the average difference between all of the values and the mean, so variance is another form of average, but not a measure of central tendency\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}\\)\nThis formula will be of conceptual importance, but will likely not be actually used.\nI will then show some examples of charts with low variance and charts with high variance (these are placeholders that were used in the handout, for lecture, I would prefer more chaotic sim examples)\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOne way to think about this is to pretend that the distribution is being stomped on by a big boot, and the the higher the variance, the heavier the boot, the flatter and more spread it ends up\nASSESSMENT ACTIVITY:\nCT, OUTLIERS, AND VARIANCE: A single activity will be given to asses the concepts above. Students will be paired into groups with large writing devices. They will then be given a two large dot histogram plots on the screen and asked to identify the following\n\nmeasures of central tendency (all 3)\nidentify any outliers\nstate which distribution has the higher variance\nwhy, in words, you chose the distribution you chose.\nDELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY4.html#day4",
    "href": "PLANDOCDAY4.html#day4",
    "title": "PLANDOCDAY4",
    "section": "",
    "text": "Short concept Time - Variability: 50 minutes\nObjectives:\n\nStudents know how to identify the measure of central tendency of a distribution, visually\nStudents can calculate standard deviation from variance, and variance from standard deviation\nStudents can look at two graphs and estimate which one has a higher variance\n\nConcepts:\n\nReview: relative frequency histograms\nReview: probability distributions\nReview: shape\nReview: Central Tendencies (visual review)\nVariability: variance and stdev\noutliers\n\nIntuition Tools:\n\nDot counting\nVariability as big boot\n\nLesson Material:\nMost of these concepts will be review, so I will quickly elaborate:\nMEASURES OF CENTRAL TENDENCY: mean, median, mode\nThen we will show a graph with a visible outlier, except we will note that outliers are somewhat subjective and not a mathematical truth, and that studies rarely just remove outliers as outliers can be expected\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances.\nWe will then go over some visual examples of higher variability in distributions, and that variacne and standard deviation are a means to measure this variability.\nWe will then state that the stdeviation is just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nIt will be noted that variance is calculated by taking the average difference between all of the values and the mean, so variance is another form of average, but not a measure of central tendency\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}\\)\nThis formula will be of conceptual importance, but will likely not be actually used.\nI will then show some examples of charts with low variance and charts with high variance (these are placeholders that were used in the handout, for lecture, I would prefer more chaotic sim examples)\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOne way to think about this is to pretend that the distribution is being stomped on by a big boot, and the the higher the variance, the heavier the boot, the flatter and more spread it ends up\nASSESSMENT ACTIVITY:\nCT, OUTLIERS, AND VARIANCE: A single activity will be given to asses the concepts above. Students will be paired into groups with large writing devices. They will then be given a two large dot histogram plots on the screen and asked to identify the following\n\nmeasures of central tendency (all 3)\nidentify any outliers\nstate which distribution has the higher variance\nwhy, in words, you chose the distribution you chose.\nDELIVERY: IN CLASS PARTICIPATION. STAKES: MEDIUM PARTICIPATION POINTS."
  },
  {
    "objectID": "PLANDOCDAY6.html",
    "href": "PLANDOCDAY6.html",
    "title": "PLANDOCDAY6",
    "section": "",
    "text": "Significance: 50 minutes\nObjectives:\n\nStudents understand that the null distribution is separate from the data collected\nStudents know how to use simulations to find p value\nStudents know what symbols relate to the statistic and parameter\nStudents can write a hypothesis statement with symbols\nStudents can correctly differentiate between a p value and a p hat value ( a very common issue I noticed in exams )\nStudents understand that significance standards can be changed\n\nConcepts:\n\nReview: Population and Sample\nReview: data types\nThe block (symbols for population parameters and sample stats)\nDISTINCT Null distribution\nDISTINCT alternative space\n“significance” How weird/percent match\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nIntuition Tools:\n\nP value as colors/area\nP value as conditional probability\nP value as percent match\n\nLesson Material:\nTo begin the lesson, we will go over a problem: how weird is the dolphin result (except we will use a 4 sample test instead of a 16 sample test so it is easier to count)\nGiven that the dolphins succeeded 4/4 times, how abnormal is that?\nTo answer the question, we will first simulate a distribution if the dolphins were acting randomly, then print it out on clear paper.\nThis is our null distribution\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nThen we will then graph an area on clear paper that encompasses all xvalues over 4.\nThis is out alternative space\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis\nAfter constructing these two graphs, we will then overlay them to see how much they line up, and apply the same conditional probability logic as before. After testing counting the green squares, 1, vs the amount of total blue squares 16, we can see that this event would have a 1/16 =0.0625 chance of happenng.\nThis is the p value\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nSo if the null is true, such an outcome only has a 6.25% chance of happening in the experiment.\nIs this a weird result?\nLets think about it.\nI will then ask the class to answer, in general, if 5% is a lot. They will probably give nuanced answers about how it depends on the circumstances, but I will request a yes or no answer.\nThis exercise is to point out that significance is subjective and can change.\nWhile this is up to some interpretation, I will then tell them that the significance requirement will be provided on exams unless it is a concept question, and any answer about significance is valid if the calculations are correct and the only difference is the interpretation.\nWe will then jump to the 16 count dolphin example to gather more data. A simulation will be ran, and the counts will be performed automatically.\nOnce performed, the P value will be about 0.003 or 0.03%, I will then ask the class to raise their hand to indicate if this is significant. Regardless of the class answer, I will make a point to note that the experimenters still could have gotten this result by random chance alone, so it is technically valid to claim that there isn’t enough data to reject the null.\nMostly jokes here, but the point is to shift them away from seeing 0.05 as a magic fixed number, and more of a general idea that tends to be used often.\nAssessment:\nP VALUE: Following this session, students will be given a similar prompt as the dolphin question, where they are to asked to respond to an identical problem but with a different sample size and proportion. Students will then have to calculate the P value given the information at hand.\nTo avoid different answers that could be falsely marked wrong, students will be introduced to the sim’s “infinite sample mode” (the binomial setting) so that everyone has ideal answers.\nDELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY6.html#day6",
    "href": "PLANDOCDAY6.html#day6",
    "title": "PLANDOCDAY6",
    "section": "",
    "text": "Significance: 50 minutes\nObjectives:\n\nStudents understand that the null distribution is separate from the data collected\nStudents know how to use simulations to find p value\nStudents know what symbols relate to the statistic and parameter\nStudents can write a hypothesis statement with symbols\nStudents can correctly differentiate between a p value and a p hat value ( a very common issue I noticed in exams )\nStudents understand that significance standards can be changed\n\nConcepts:\n\nReview: Population and Sample\nReview: data types\nThe block (symbols for population parameters and sample stats)\nDISTINCT Null distribution\nDISTINCT alternative space\n“significance” How weird/percent match\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nIntuition Tools:\n\nP value as colors/area\nP value as conditional probability\nP value as percent match\n\nLesson Material:\nTo begin the lesson, we will go over a problem: how weird is the dolphin result (except we will use a 4 sample test instead of a 16 sample test so it is easier to count)\nGiven that the dolphins succeeded 4/4 times, how abnormal is that?\nTo answer the question, we will first simulate a distribution if the dolphins were acting randomly, then print it out on clear paper.\nThis is our null distribution\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nThen we will then graph an area on clear paper that encompasses all xvalues over 4.\nThis is out alternative space\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis\nAfter constructing these two graphs, we will then overlay them to see how much they line up, and apply the same conditional probability logic as before. After testing counting the green squares, 1, vs the amount of total blue squares 16, we can see that this event would have a 1/16 =0.0625 chance of happenng.\nThis is the p value\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nSo if the null is true, such an outcome only has a 6.25% chance of happening in the experiment.\nIs this a weird result?\nLets think about it.\nI will then ask the class to answer, in general, if 5% is a lot. They will probably give nuanced answers about how it depends on the circumstances, but I will request a yes or no answer.\nThis exercise is to point out that significance is subjective and can change.\nWhile this is up to some interpretation, I will then tell them that the significance requirement will be provided on exams unless it is a concept question, and any answer about significance is valid if the calculations are correct and the only difference is the interpretation.\nWe will then jump to the 16 count dolphin example to gather more data. A simulation will be ran, and the counts will be performed automatically.\nOnce performed, the P value will be about 0.003 or 0.03%, I will then ask the class to raise their hand to indicate if this is significant. Regardless of the class answer, I will make a point to note that the experimenters still could have gotten this result by random chance alone, so it is technically valid to claim that there isn’t enough data to reject the null.\nMostly jokes here, but the point is to shift them away from seeing 0.05 as a magic fixed number, and more of a general idea that tends to be used often.\nAssessment:\nP VALUE: Following this session, students will be given a similar prompt as the dolphin question, where they are to asked to respond to an identical problem but with a different sample size and proportion. Students will then have to calculate the P value given the information at hand.\nTo avoid different answers that could be falsely marked wrong, students will be introduced to the sim’s “infinite sample mode” (the binomial setting) so that everyone has ideal answers.\nDELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY8.html",
    "href": "PLANDOCDAY8.html",
    "title": "PLANDOCDAY8",
    "section": "",
    "text": "T Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to calculate the standardized statistic for a mean with unknown population variance\nStudents know how to obtain t scores and t critical values\nStudents know when to use a T score vs a Z score\nStudents should know the meaning and purpose of a standard error.\n\nConcepts:\n\nT Statistic\nStandard Error\nDegrees of Freedom\nCircumstantial Flow Chart\n\nIntuition Tools:\n\nsimulation limit\ndf as measure of normality\n\nLesson Material:\nIn the last session, we had plotted averages calculated from samples to make a sampling distribution. While we noticed that the distribution became more bell shaped as the sample size increased, it can also be noticed that the variance decreased as the sample size increased.\nThis makes sense, as larger samples are more resistant to random chance, and an outline is less likely to effect an average as the amount of samples gets larger.\nAt this point, a prepared code (Already prepared, will be included) will show that the what happens to the variance (visually) as the sample size goes up, until eventually the sampling distribution only contains about one small class of values.\nIts evident that the variance is decreasing, but we actually have a formula to approximate it: the standard error.\n\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nThis is a fun fact that will become more relevant later, but in regards to todays lesson, if we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nSo the degrees of freedom is directly connected to the sample size.\nIf you look at a T table, you may notice that as the degrees of freedom increases, the T score approaches values that are identical to the Z score table.\nThis effect is related to central limit theorem, and also makes sense because as a sample increases, it becomes more steadily reflective of the population #review this passage, more specifically on central limit theorem.\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work. (to explain this decision making process, a flowchart will be attached in the handout)\nAssessment\nZ VS T: Students will be given a variety of circumstances which would prompt the use of a standardized statistic, and will be asked if they are to use the theory based Z, theory based T, or neither. there will be no evaluation in this portion. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nT SCORE: Students will be given a circumstance prompting the use of a T statistic, and given the sample size, sample variance, null mean, and sample average, they will be asked to find if their value is significant.\nOne example will require them to calculate the exact p value with a table, and the other will ask if there is significant evidence to reject the null given a specific critical value for a specific standard of significance. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nSAMPLING DISTRIBUTION: Students will be given a sample with a sample variance and sample size, and they will be asked to estimate the standard deviation of the sampling distribution with that information.\nThis will test if they are aware that the standard error is an approximation for the sampling distribution.\nthe next sampling distribution will provide the population mean and population variance, ask them to estimate the mean of the sampling distribution. With this question, all they need is the mean as the population mean is approximately equal to the mean of the sampling distribution. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PLANDOCDAY8.html#day8",
    "href": "PLANDOCDAY8.html#day8",
    "title": "PLANDOCDAY8",
    "section": "",
    "text": "T Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to calculate the standardized statistic for a mean with unknown population variance\nStudents know how to obtain t scores and t critical values\nStudents know when to use a T score vs a Z score\nStudents should know the meaning and purpose of a standard error.\n\nConcepts:\n\nT Statistic\nStandard Error\nDegrees of Freedom\nCircumstantial Flow Chart\n\nIntuition Tools:\n\nsimulation limit\ndf as measure of normality\n\nLesson Material:\nIn the last session, we had plotted averages calculated from samples to make a sampling distribution. While we noticed that the distribution became more bell shaped as the sample size increased, it can also be noticed that the variance decreased as the sample size increased.\nThis makes sense, as larger samples are more resistant to random chance, and an outline is less likely to effect an average as the amount of samples gets larger.\nAt this point, a prepared code (Already prepared, will be included) will show that the what happens to the variance (visually) as the sample size goes up, until eventually the sampling distribution only contains about one small class of values.\nIts evident that the variance is decreasing, but we actually have a formula to approximate it: the standard error.\n\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nThis is a fun fact that will become more relevant later, but in regards to todays lesson, if we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nSo the degrees of freedom is directly connected to the sample size.\nIf you look at a T table, you may notice that as the degrees of freedom increases, the T score approaches values that are identical to the Z score table.\nThis effect is related to central limit theorem, and also makes sense because as a sample increases, it becomes more steadily reflective of the population #review this passage, more specifically on central limit theorem.\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work. (to explain this decision making process, a flowchart will be attached in the handout)\nAssessment\nZ VS T: Students will be given a variety of circumstances which would prompt the use of a standardized statistic, and will be asked if they are to use the theory based Z, theory based T, or neither. there will be no evaluation in this portion. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nT SCORE: Students will be given a circumstance prompting the use of a T statistic, and given the sample size, sample variance, null mean, and sample average, they will be asked to find if their value is significant.\nOne example will require them to calculate the exact p value with a table, and the other will ask if there is significant evidence to reject the null given a specific critical value for a specific standard of significance. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS\nSAMPLING DISTRIBUTION: Students will be given a sample with a sample variance and sample size, and they will be asked to estimate the standard deviation of the sampling distribution with that information.\nThis will test if they are aware that the standard error is an approximation for the sampling distribution.\nthe next sampling distribution will provide the population mean and population variance, ask them to estimate the mean of the sampling distribution. With this question, all they need is the mean as the population mean is approximately equal to the mean of the sampling distribution. DELIVERY: IN CLASS PAPER WHICH CAN BE TAKEN HOME AND RETURNED THE FOLLOWING LECTURE. STAKES: MEDIUM HOMEWORK POINTS"
  },
  {
    "objectID": "PreSemesterToDo.html",
    "href": "PreSemesterToDo.html",
    "title": "PreSemesterToDo",
    "section": "",
    "text": "Prior to the semester, I will expect myself to complete the following tasks:\n\nREFINE upcoming materials + handouts\n\nWhile I feel like I have put effort into these. I will likely find many flaws and possible improvements prior to class. As such, I will likely need to make some changes, so i will go through the first 2+ weeks to prepare.\n\nInitialize canvas and upload introduction activity early\n\nThis intro activity will tell me how students perceive the class and how to address them. This will be a short and easy assignment, but will help me prepare some class expectations and be help with communication later on.\n\nGrab the physical materials needed for demonstrations and activities and place them in a file at least one week ahead.\nFind out if my class is MWF or TR, and make minor adjustments to the time plan.\nPrepare a few outfits for class\nWrite up exact homework questions for the first assessments on canvas, and record some expectations for in class activities."
  }
]