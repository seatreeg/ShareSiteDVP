[
  {
    "objectID": "courseschedule.html",
    "href": "courseschedule.html",
    "title": "COURSE SCHEDULE",
    "section": "",
    "text": "NOTE: Assessment are designed so that they can be performed in class or at home with minor adjustments. The homework materials will be similar to the assessments and activities given, but with different values.\n&gt;&gt;&gt;“DAY”1\nSection C-S, 25 min ~ LECTURE\n\nsyllabus\n\nSection P-1 25min Handout PRE A\n\nVariables\nCategorical vs Quantitative Variables\nClasses\nHistograms/Relative Frequency Histograms\nProbability Distributions\nShape\n\n&gt;&gt;&gt;“DAY” 2\nSection C-1 50min HANDOUT CAR\n\nvenn diagrams + set symbols\nSupport\nset symbols as logic+“if”\nindependent probability\nconditional probability\n“At least once” problem\n\n&gt;&gt;&gt;“DAY”3\nSection P-1 25min HANDOUT A\n\npopulation\nsample\nmean\n“the block” stat/param types\nobservational unit\nvariable\nobservational study\nexperiment\n\nsection P-1 25 min ~ACTIVITY\n\nvocab match (group or lecture)\n\n&gt;&gt;&gt;“DAY”4\nSection P-2 25min HANDOUT B\n\nRelative frequency histograms\nprobability distributions\nshape\nmeasures of central tendency\nvariability: variance/stdev\noutliers\n\nSection P-1 ~ ACTIVITY\n\nvocab match (group or lecture)\n\n&gt;&gt;&gt;“DAY”5\nSection P-2 25minHANDOUT C\n\nRandom Process #concern on teaching\nRandom Variable\nProbability as logic statement\nSimulation\nlong run probability distribution\n\nSection P-2 ~ EXAMPLE LECTURE\n\nLecture example, probability distributions as colors (no p value)\n\nActivity: Students calculate probability by overlay method.\n&gt;&gt;&gt;“DAY”6\nSection 1-1/2 25min HANDOUT D+E\n\nreiterate populaiton and sample\nreiterate data types\nthe block {[proportion parameter,sample statistic]*[Quant mean,Cat proportion]}\nNull distribution\nalternative space\n“significance” (how weird/percent match)\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nSection 1-1/2 25min ~ EXAMPLE LECTURE\n\nP value as two distinct sets being overlayed (conditional probability with colors)\nP values as overlap percent match\nDolphin-like example ##check the name on this\n\n&gt;&gt;&gt;“DAY”7\nSection 1-1/2 25min ~ EXAMPLE LECTURE\n\nAdding up averages of dice with different sample size\nsampling distribution\nCLT (the graph goes to bell shape)(brief)\nTheory based and sim\nP value with distance and variance (overlap with colors)\n\nSection 1-3/4 25min HANDOUT F\n\nAdding up averages at different sample size\nsampling distribution\nCLT (bell shape approach)\nNormal Distribution\nStandard Statistic (sim and theory)\ncritical value\np value with distance and variance\nrecipe for low p values\n\n&gt;&gt;&gt;“DAY” 8\nsection 1-5 50min ~ ACTIVITY HANDOUT G\n\nT statistic\nStandard Error\nDegrees of Freedom\nCircumstantial flow chart\n\n&gt;&gt;&gt;“DAY” 9\nSection 2-1 HANDOUT H\n\ngeneralizability\nrepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nother sampling forms (brief)\nsampling frame\n\nSection 2-1 ~ ACTIVITY\n\ndescribe the sampling\nstate the problem\nstate a solution\n\n&gt;&gt;&gt;“DAY” 10\nSection 2-2/3 25min HANDOUT I\n\nType 1 error\nType 2 error\nsignificance level alpha\nbeta\neffect size\npower\n\nSection 2-2/3 25min ~ ACTIVITY\n\nPredict the relative power and P from statistics\nPrecict the relative power and P from visual charts alone"
  },
  {
    "objectID": "PLANDOCDAY10.html",
    "href": "PLANDOCDAY10.html",
    "title": "PLANDOCDAY10",
    "section": "",
    "text": "Errors: 50 minutes\nObjectives:\n\nStudents can correctly identify type 1 and 2 errors.\nStudents understand the meaning of alpha and beta in the context of statistical errors.\nStudents know what factors increase the likelihood of each error, and what factors(non statistical use of this word) increase power and p value.\nStudents can identify relative power levels and p values from visual data alone\n\nConcepts:\n\nType 1 Error\nType 2 Error\nSignificance level alpha\nbeta\nPower 1-beta\n\nIntuition Tools:\n\nT1ER T2EFTR\n\nLesson Material:\nThe lesson will start with the terminology being defined, students will be provided with the box chart of errors and a graph of two distributions showing where alpha and beta sit.\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nWhile wrote memorization of the error terminology may seem pointless when it can just be checked, these terms come up surprising amount, so I will let instruct them to look carefully at the vocab and variables.\nTo make memorization easier, I will show the T1ER T2EFTR method, where T1ER stands for Type 1-&gt;Reject, as you erroneously rejected the null. This looks like like “TIER”. The next one is T2EFTR, which stands for Type 2-&gt;Fail To Reject, as you are erroneously failing to reject the null. This, if written weirdly, looks like “TLEFTR” (teh-lef-ter), which inst a word, but its works well to memorize regardless for some reason.\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )\n\n\n\n\nAssessment\nERRORS: Students will be given an example of an experiment, a conclusion, and the truth, and they will be asked to put the example in one of 4 boxes on the type of error/correct conclusion chart.\nP AND POWER: Students will be given 4 variables {meanx-nullmean common variance, significance level, sample size}. The experiment will suggest one set up with numerical values where all values are held constant except one (so one experiment will have a diff=10, var=1,a=0.05, and n=10, while the other has diff=10, var=1,a=0.05, and n=20, and students will have to chose the one with the higher power level.\nAs an important review, students will also be given a situation with the same variables stated, and asked to predict which one has a lower p value. (NOTE: the alpha will not effect the p value)\nNext, I will give them two distributions on the same chart alongside another two distributions on the same chart. The distributions will minimal labels. From this, the students will have to select which charts have lower p values, and which charts have higher power levels\nThis will hopefully give them a conceptual understanding of the p value and power so that they can predict power levels and p value without needing all of the data.\nAn example of some charts that will be given to them for the minimal information portion can be found below (circumstance b has a higher power and lower p value)\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm1,norm2),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance A\", x=\"X\", y=\"Density\")\n\n\n\n\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm3,norm4),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance B\", x=\"X\", y=\"Density\")"
  },
  {
    "objectID": "PLANDOCDAY10.html#day10",
    "href": "PLANDOCDAY10.html#day10",
    "title": "PLANDOCDAY10",
    "section": "",
    "text": "Errors: 50 minutes\nObjectives:\n\nStudents can correctly identify type 1 and 2 errors.\nStudents understand the meaning of alpha and beta in the context of statistical errors.\nStudents know what factors increase the likelihood of each error, and what factors(non statistical use of this word) increase power and p value.\nStudents can identify relative power levels and p values from visual data alone\n\nConcepts:\n\nType 1 Error\nType 2 Error\nSignificance level alpha\nbeta\nPower 1-beta\n\nIntuition Tools:\n\nT1ER T2EFTR\n\nLesson Material:\nThe lesson will start with the terminology being defined, students will be provided with the box chart of errors and a graph of two distributions showing where alpha and beta sit.\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nWhile wrote memorization of the error terminology may seem pointless when it can just be checked, these terms come up surprising amount, so I will let instruct them to look carefully at the vocab and variables.\nTo make memorization easier, I will show the T1ER T2EFTR method, where T1ER stands for Type 1-&gt;Reject, as you erroneously rejected the null. This looks like like “TIER”. The next one is T2EFTR, which stands for Type 2-&gt;Fail To Reject, as you are erroneously failing to reject the null. This, if written weirdly, looks like “TLEFTR” (teh-lef-ter), which inst a word, but its works well to memorize regardless for some reason.\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )\n\n\n\n\nAssessment\nERRORS: Students will be given an example of an experiment, a conclusion, and the truth, and they will be asked to put the example in one of 4 boxes on the type of error/correct conclusion chart.\nP AND POWER: Students will be given 4 variables {meanx-nullmean common variance, significance level, sample size}. The experiment will suggest one set up with numerical values where all values are held constant except one (so one experiment will have a diff=10, var=1,a=0.05, and n=10, while the other has diff=10, var=1,a=0.05, and n=20, and students will have to chose the one with the higher power level.\nAs an important review, students will also be given a situation with the same variables stated, and asked to predict which one has a lower p value. (NOTE: the alpha will not effect the p value)\nNext, I will give them two distributions on the same chart alongside another two distributions on the same chart. The distributions will minimal labels. From this, the students will have to select which charts have lower p values, and which charts have higher power levels\nThis will hopefully give them a conceptual understanding of the p value and power so that they can predict power levels and p value without needing all of the data.\nAn example of some charts that will be given to them for the minimal information portion can be found below (circumstance b has a higher power and lower p value)\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm1,norm2),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance A\", x=\"X\", y=\"Density\")\n\n\n\n\n\nlibrary(ggplot2)\nset.seed(20)\nnorm1 &lt;- rnorm(1000, mean=0, sd=1)\nnorm2 &lt;- rnorm(1000, mean=0.5, sd=1)\nnorm3 &lt;- rnorm(1000, mean=0, sd=1)\nnorm4 &lt;- rnorm(1000, mean=10, sd=1)\n\ndf&lt;-data.frame(\n  vals=c(norm3,norm4),\n  grps=factor(rep(1:2, each=1000))\n)\n\nggplot(df, aes(x=vals, fill=grps))+\n  geom_density(alpha=0.5)+\n  labs(title=\"Circumstance B\", x=\"X\", y=\"Density\")"
  },
  {
    "objectID": "syllabuslp.html",
    "href": "syllabuslp.html",
    "title": "syllabuslp",
    "section": "",
    "text": "INSTRUCTOR: Carson Trego, carson.trego@huslers.unl.edu\nCLASS: days:{X, X, X } time{XX:XX}\nLOCATION: XXX, X Hall\nHELP HOURS: M 12:30P-1:30P, F 12:30P-2:30P\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nIMPORTANT DATES:\nExam1: xx/xx\nExam2: xx/xx\nExam3: xx/xx\nFinal Project Presentation: xx/xx\nFinal Project paper due: xx/xx\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nGRADE PORTION:\nExam1: 20%\nExam2: 20%\nFinal Project: 30%\nHomework/participation: 30%\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nHOMEWORK:\nTo avoid homework from causing any stressful cramming and loss sleep, homeworks will be cut into small pieces that are assigned and completed for each section completed. This means that you may have 1 or more homeworks assigned per week, but they will be very short, and wil not be due until the necessary information is taught in class.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nPARTICIPATION:\nThese are usually given as 100% or 0%. If you are present in class and attempt to participate in the activities, you will get these points.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nSICK/ABSENT POLICY:\nAbsences will result in a zero in the daily participation points, however, these points can be earned back.\nThe current policy for earning points back is attending office hours/zoom and explaining the information that you were absent for, and answering a few questions. Once this is considered satisfactory, you will earn a credit for a missed class. You may only do this at a rate of one point back per week.\nIf a particularly bad illness happens or another difficult circumstances, a doctors note or other relevant form of documentation may be provided and all points will be given back without needing to accomplish a task.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nCALCULATORS\nCalculators will be allowed on tests. Graphers are allowed if you register to have your memory cleared 10 minutes before the exam. Once cleared, you will be provided with a sticker showing proof of clearing.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nEXAM STRUCTURE\n@@ideally i would like to provide tests in way that is proctored yet allows for significant time, but I am currently working out how that would be achieved. I ideal circumstances, I could be a late class that allows students ample time after the scheduled time, but things dont always work out as planned, so I will gather more information before setting this in stone\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nFINAL PROJECT\nRather than a final exam, you will be expected to do a group/solo project where you will be given a practical question to answer using the information that was covered in class. The project will be in three parts: a presentation,paper, and reflection. The presentation will be given during classtimes, and the paper will be submitted a few days after presentations. Attendance and reflections on your own and other classmates projects will be required to receive a full grade.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nACADEMIC HONESTY\nAs with all courses, academic dishonesty can potentially result in significant grade reduction and further punishment. If you are suspected of committing academic dishonesty, you will be given a chance to meet and discuss what had happen before facing any punishment.\nStudents may be randomly asked to casually discuss the answers they have provided on assignments. Being called in does NOT mean that you are being accused of academic dishonesty, and you will be given ample time to collect your thoughts as explaining answers is inherently stressful for many students.\nThe polices for academic dishonesty will be stated on a per assignment basis, so while it may be allowed to discuss problems with others on one assignment, it may not be allowed for another. In general, copying statements word for word in your answers will NOT be allowed.\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nPHOTO POLICY\nStudents often take photos of the board to save time on notes. These notes will be provided on the canvas website, and any relevant board information will be shared as well. So photos should not be required.\nPlease do not take pictures in class without permission prior, as many students prefer not to be recorded. If you would like a copy of any information in class, please let me know!"
  },
  {
    "objectID": "PLANDOCDAY9.html",
    "href": "PLANDOCDAY9.html",
    "title": "PLANDOCDAY9",
    "section": "",
    "text": "Sampling: 50 minutes\nObjectives:\n\nStudents will be able to identify different types of sampling\nStudents will understand the potential issues with convenient sampling, as well as the necessity of convenient sampling\nStudents will be able to explain what makes the results form a sample more generalizable.\n\nConcepts:\n\ngeneralizability\nRepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nsampling frame\n\nIntuition Tools:\n\nn/a\n\nLesson Material:\nThis is largely a vocab day so we will start with the vocab words\nGENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population.\nAssessment:\nAs with the former vocab activities, students will put in groups and assigned a board to write answers on in response to a prompt given on the projector. The prompt will describe an observational study, and the the students will have to answer the following questions:\n\nWhat is the population?\nWhat is the parameter of interest?\nWhat is the sample?\nWhat i the sampling frame?\nWhat type of sampling is being used?\nis there potential bias?\nif so, why?\nif so , how can this bias be improved?\n\nMost of the questions will feature fairly obvious forms of bad convenience sampling, but one of the questions will be intentionally tricky. The sample will be for a medical trial which involves getting willing participants to be subjected to an invasive observation. Patients were found from areas all across the globe, with no geographical clustering. The thoroughness would lead you to think that it is not convenient sampling, but they still obtained willing participants, which has the potential for bias. While this is convenient sampling, it is hard to argue that it should be improved because the way to fix this would be to do the procedure involuntarily, which would be ethically wrong. (I am trying to focus on showing that convenient sampling is not always “bad” as I have heard some students describe it at such)"
  },
  {
    "objectID": "PLANDOCDAY9.html#day9",
    "href": "PLANDOCDAY9.html#day9",
    "title": "PLANDOCDAY9",
    "section": "",
    "text": "Sampling: 50 minutes\nObjectives:\n\nStudents will be able to identify different types of sampling\nStudents will understand the potential issues with convenient sampling, as well as the necessity of convenient sampling\nStudents will be able to explain what makes the results form a sample more generalizable.\n\nConcepts:\n\ngeneralizability\nRepresentative sample\ngeneralizations\nbias\nconvenient sampling\nsimple random sampling\nsampling frame\n\nIntuition Tools:\n\nn/a\n\nLesson Material:\nThis is largely a vocab day so we will start with the vocab words\nGENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population.\nAssessment:\nAs with the former vocab activities, students will put in groups and assigned a board to write answers on in response to a prompt given on the projector. The prompt will describe an observational study, and the the students will have to answer the following questions:\n\nWhat is the population?\nWhat is the parameter of interest?\nWhat is the sample?\nWhat i the sampling frame?\nWhat type of sampling is being used?\nis there potential bias?\nif so, why?\nif so , how can this bias be improved?\n\nMost of the questions will feature fairly obvious forms of bad convenience sampling, but one of the questions will be intentionally tricky. The sample will be for a medical trial which involves getting willing participants to be subjected to an invasive observation. Patients were found from areas all across the globe, with no geographical clustering. The thoroughness would lead you to think that it is not convenient sampling, but they still obtained willing participants, which has the potential for bias. While this is convenient sampling, it is hard to argue that it should be improved because the way to fix this would be to do the procedure involuntarily, which would be ethically wrong. (I am trying to focus on showing that convenient sampling is not always “bad” as I have heard some students describe it at such)"
  },
  {
    "objectID": "PLANDOCDAY7.html",
    "href": "PLANDOCDAY7.html",
    "title": "PLANDOCDAY7",
    "section": "",
    "text": "Sampling Distributions and Z Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to correctly predict what happens to a sampling distribution as sample size increase\nStudents can correctly differentiate between the sampling distribution and the distribution of the sample\nStudents understand why the normal distribution is present in so many different areas.\nStudents know when to use the theory based approach for significance\n\nConcepts:\n\nAdding up averages at different sample size\nSampling distribution\nCLT (bell shape approach)\nNormal Distribution/ T distribution (above contextual)\nStandard statistic (sim and theory)\ncritical value\nP Value with distance and variance (overlap)\nrecipe for low p(4 cases)\n\nIntuition Tools:\n\nstandardized stat graphed along standard deviation image\n\nLesson Material:\nAt the beginning of class the students will be asked if they have heard of the normal distribution, or how it shows up everywhere. (I had asked this question to our students and a surprising majority said yes) I will then ask why and begin the example.\nThe lesson will begin with a random number generator, which is set to roll random numbers between 0 and 4. As we generate random numbers, these will be tallied on a histogram.\nAfter enough values are recorded, the process will be repeated but with an average of two rolls instead of one,\nThen the process will be repeated for an average of 4 numbers.\nWhat we are creating is sampling distributions:\nSAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nEventually, the original uniform rectangle will start to form a mound shape, hinting at a pattern.\nA variety of programs will then be shown running larger numbers and for different distributions, and all of them will come back mound shaped.\nVisually, this is the normal distribution, and what we are seeing is central limit theorem at work.\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nUsing this system, we can then sim the distribution, and count the relative proportion to get out P value.\nBut if a simulation is impractical, we can still just draw out a normal distribution given our null average and null variance and find where our score lies. This is a continuous distributions, so counting how much area this takes up will be difficult.\nSome people have calculated the amount of area past a certain point on a normal distribution, but there is a problem: it is only for a very specific distribution at mean=0 and variance = 1.\nThankfully, we can do some fancy math (which will not be on the test and will be purely for conceptual understanding) where we shift and divide our normal distribution resulting in an equation that can also shift and divide that value we want to find the significance for (this math will be performed in front of the class)\nThis equation will give us our standardized statistic\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis also is derived from tables, but you only need to know one value for each significance standard and type of hypothesis.\nThe use of these tables to find the critical value will then be shown. All 3 hypothesis values will be recorded.\nIf we are interested if a value is not equal to the the null, than the critical value is 1.96, every single time.\nSo the process here is very simple, we just input our value into the standard statistic formula, and if the absolute value of that number is greater than 1.96, then we know that the value is significant assuming a required p value of 0.05 or less.\nNOTE: a larger difference from the mean increases the Z score, and a lower variance also increases the z score. This hints at the the fact that larger differences from the mean and lower variances tend to make lower p values.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\nAssessment:\nSTANDARDIZED SIGNIFICANCE: Students will be given 10 circumstances, and only 3 where the validity conditions are met. If the validity conditions are not met, the student may skip the problem, if the validity contentions are met they will use the theory approach to calculate the standard statistic and determine if the value is significant.\nthe sample sizes will be given, and the distribution will appear visually on the right. The information will either be proportional or mean based, and the population deviation may or may not be given. Using that information, students will have to select their answers.\nThis will test their ability not only to calculate the standardized statistic, but also when to use that approach.\nRRECIPE FOR LOW P VALUE: Students will be given 4 circumstances {variance low; x - mean = low || variance high, x - mean = low || variance low, x - mean = high, variance high, x - mean = high} and are asked to determine which circumstance would have the lowest p value.\nFrom the a standardized statistic formula, they should be able to find that [variance low, x - mean = high] has the lowest p value as it produces the highest standardized statistic."
  },
  {
    "objectID": "PLANDOCDAY7.html#day7",
    "href": "PLANDOCDAY7.html#day7",
    "title": "PLANDOCDAY7",
    "section": "",
    "text": "Sampling Distributions and Z Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to correctly predict what happens to a sampling distribution as sample size increase\nStudents can correctly differentiate between the sampling distribution and the distribution of the sample\nStudents understand why the normal distribution is present in so many different areas.\nStudents know when to use the theory based approach for significance\n\nConcepts:\n\nAdding up averages at different sample size\nSampling distribution\nCLT (bell shape approach)\nNormal Distribution/ T distribution (above contextual)\nStandard statistic (sim and theory)\ncritical value\nP Value with distance and variance (overlap)\nrecipe for low p(4 cases)\n\nIntuition Tools:\n\nstandardized stat graphed along standard deviation image\n\nLesson Material:\nAt the beginning of class the students will be asked if they have heard of the normal distribution, or how it shows up everywhere. (I had asked this question to our students and a surprising majority said yes) I will then ask why and begin the example.\nThe lesson will begin with a random number generator, which is set to roll random numbers between 0 and 4. As we generate random numbers, these will be tallied on a histogram.\nAfter enough values are recorded, the process will be repeated but with an average of two rolls instead of one,\nThen the process will be repeated for an average of 4 numbers.\nWhat we are creating is sampling distributions:\nSAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nEventually, the original uniform rectangle will start to form a mound shape, hinting at a pattern.\nA variety of programs will then be shown running larger numbers and for different distributions, and all of them will come back mound shaped.\nVisually, this is the normal distribution, and what we are seeing is central limit theorem at work.\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nUsing this system, we can then sim the distribution, and count the relative proportion to get out P value.\nBut if a simulation is impractical, we can still just draw out a normal distribution given our null average and null variance and find where our score lies. This is a continuous distributions, so counting how much area this takes up will be difficult.\nSome people have calculated the amount of area past a certain point on a normal distribution, but there is a problem: it is only for a very specific distribution at mean=0 and variance = 1.\nThankfully, we can do some fancy math (which will not be on the test and will be purely for conceptual understanding) where we shift and divide our normal distribution resulting in an equation that can also shift and divide that value we want to find the significance for (this math will be performed in front of the class)\nThis equation will give us our standardized statistic\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis also is derived from tables, but you only need to know one value for each significance standard and type of hypothesis.\nThe use of these tables to find the critical value will then be shown. All 3 hypothesis values will be recorded.\nIf we are interested if a value is not equal to the the null, than the critical value is 1.96, every single time.\nSo the process here is very simple, we just input our value into the standard statistic formula, and if the absolute value of that number is greater than 1.96, then we know that the value is significant assuming a required p value of 0.05 or less.\nNOTE: a larger difference from the mean increases the Z score, and a lower variance also increases the z score. This hints at the the fact that larger differences from the mean and lower variances tend to make lower p values.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\nAssessment:\nSTANDARDIZED SIGNIFICANCE: Students will be given 10 circumstances, and only 3 where the validity conditions are met. If the validity conditions are not met, the student may skip the problem, if the validity contentions are met they will use the theory approach to calculate the standard statistic and determine if the value is significant.\nthe sample sizes will be given, and the distribution will appear visually on the right. The information will either be proportional or mean based, and the population deviation may or may not be given. Using that information, students will have to select their answers.\nThis will test their ability not only to calculate the standardized statistic, but also when to use that approach.\nRRECIPE FOR LOW P VALUE: Students will be given 4 circumstances {variance low; x - mean = low || variance high, x - mean = low || variance low, x - mean = high, variance high, x - mean = high} and are asked to determine which circumstance would have the lowest p value.\nFrom the a standardized statistic formula, they should be able to find that [variance low, x - mean = high] has the lowest p value as it produces the highest standardized statistic."
  },
  {
    "objectID": "PLANDOCDAY5.html",
    "href": "PLANDOCDAY5.html",
    "title": "PLANDOCDAY5",
    "section": "",
    "text": "Random Variables: 50 minutes\nObjectives:\n\nStudents can identify what a random variable is and how they work practically to produce outputs\nStudents understand the purpose and theory behind simulation\nStudents know that simulations can produce partially off results\nStudents understand that a an infinitely run simulation will produce the underlying probability distribution\n\nConcepts:\n\nRandom Process #concern on teaching\nRandom Variable (RV)\nProbability as logic statement\nsimulation\nlong run probability distribution\n\nIntuition Tools:\n\nRVs as machine that makes random numbers\nProbability as conditional colors\n\nLesson Material:\nAs usual students will be presented with a handout alongside the vocab they will be shown\nAfter going over the definitions for each…\nRANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\n…We will note that simulations produce imperfect results compared to our known distributions, and to demonstrate that, I will run a sim with a small sample size. Then I will increase the sample size and show that it approaches the real distribution over time.\nAs an example in lecture, I will take a probability distribution with easy to count discrete points, and calculate the probability that our random variable X is between A and B. To do this, I will graph all points between A and B, and overlay them on the probability distribution. I will then reference the conditional probability equation, and begin counting the dots with mixed colors (Z union Y) divided by the total number of dots in the whole distribution (Z). Using that method, I will then have the probability that our RV X is between A and B, which could also be done by counting the numbers in between.\n(I will also reference that you can count the amount of dots less than B minus the amount of dots less than A)\nAssessment:\nStudents will each be given a unique dot plot of values, and asked to calculate the probability that a RV is between two points. They will be asked to show their work visually. After doing so, groups will rotate and view the work and answers of another group, and write comments on how they think it went and if they concur.\nAssessment Activity:\nStudents will be given a similar block distribution as I had used in the demonstration in class. They will be asked to use color overlays to calculate this. and then write out how the method relates to conditional probability of sets."
  },
  {
    "objectID": "PLANDOCDAY5.html#day5",
    "href": "PLANDOCDAY5.html#day5",
    "title": "PLANDOCDAY5",
    "section": "",
    "text": "Random Variables: 50 minutes\nObjectives:\n\nStudents can identify what a random variable is and how they work practically to produce outputs\nStudents understand the purpose and theory behind simulation\nStudents know that simulations can produce partially off results\nStudents understand that a an infinitely run simulation will produce the underlying probability distribution\n\nConcepts:\n\nRandom Process #concern on teaching\nRandom Variable (RV)\nProbability as logic statement\nsimulation\nlong run probability distribution\n\nIntuition Tools:\n\nRVs as machine that makes random numbers\nProbability as conditional colors\n\nLesson Material:\nAs usual students will be presented with a handout alongside the vocab they will be shown\nAfter going over the definitions for each…\nRANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\n…We will note that simulations produce imperfect results compared to our known distributions, and to demonstrate that, I will run a sim with a small sample size. Then I will increase the sample size and show that it approaches the real distribution over time.\nAs an example in lecture, I will take a probability distribution with easy to count discrete points, and calculate the probability that our random variable X is between A and B. To do this, I will graph all points between A and B, and overlay them on the probability distribution. I will then reference the conditional probability equation, and begin counting the dots with mixed colors (Z union Y) divided by the total number of dots in the whole distribution (Z). Using that method, I will then have the probability that our RV X is between A and B, which could also be done by counting the numbers in between.\n(I will also reference that you can count the amount of dots less than B minus the amount of dots less than A)\nAssessment:\nStudents will each be given a unique dot plot of values, and asked to calculate the probability that a RV is between two points. They will be asked to show their work visually. After doing so, groups will rotate and view the work and answers of another group, and write comments on how they think it went and if they concur.\nAssessment Activity:\nStudents will be given a similar block distribution as I had used in the demonstration in class. They will be asked to use color overlays to calculate this. and then write out how the method relates to conditional probability of sets."
  },
  {
    "objectID": "PLANDOCDAY3.html",
    "href": "PLANDOCDAY3.html",
    "title": "PLANDOCDAY3",
    "section": "",
    "text": "Basic Vocab Time - population and sample: 25 minutes\nObjectives:\n\nStudents can properly differentiate between a sample and population\nStudents can identify the observational units and variables in a study\nStudents understand the difference between an experiment and an observational study\nStudents know why we use samples\n\nConcepts:\n\nPopulation\nSample\nproportion\nmean\nINTRODUCE THE BLOCK\nObservational Unit\nObservational Study\nExperiment\n\nIntuition Tools:\n\nBurger stats\n\nLesson Material:\nQuite a minimal lesson, students will have a vocab handout and follow along with a slideshow with definitions and examples\nPOPULATION: The group of everything you want to know about\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: The subset you grabbed from the population to get an idea of the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nWhy we use samples:\n\nless effort/cost\nmeasurement destruction\nimpossibility\n\nAs an example of desruction, students will be told the about the burger example and the dissection example, where observing destroys the entity in question.\nIn the case of impossibility, animal studies will be mentioned.\nNOTE: you can measure entire populations if you simply define your population to be manageable: (e.g. a whole class)\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\\\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”\n(note, assessment is in the activity)\n\nActivity- population and sample: 25 minutes\nVOCAB: students will be arranged into groups an placed in a region of the class with writing materials (if it is not a class with 360 boards on the walls, I will go out and prepare some big writing devices to slap on the wall) Students will then be given an example of study, and then be asked:\n\nis this an observational study or an experiment\nwhat is the population\nwhat is the sample\nwhat are the variables being measured?\nwhich variables are response and which are explanatory\nwhat is the observational unit in the study\n\nGroups will then be given a moment to discuss, and if the groups do not concur, an open discussion will take place."
  },
  {
    "objectID": "PLANDOCDAY3.html#day-3",
    "href": "PLANDOCDAY3.html#day-3",
    "title": "PLANDOCDAY3",
    "section": "",
    "text": "Basic Vocab Time - population and sample: 25 minutes\nObjectives:\n\nStudents can properly differentiate between a sample and population\nStudents can identify the observational units and variables in a study\nStudents understand the difference between an experiment and an observational study\nStudents know why we use samples\n\nConcepts:\n\nPopulation\nSample\nproportion\nmean\nINTRODUCE THE BLOCK\nObservational Unit\nObservational Study\nExperiment\n\nIntuition Tools:\n\nBurger stats\n\nLesson Material:\nQuite a minimal lesson, students will have a vocab handout and follow along with a slideshow with definitions and examples\nPOPULATION: The group of everything you want to know about\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: The subset you grabbed from the population to get an idea of the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nWhy we use samples:\n\nless effort/cost\nmeasurement destruction\nimpossibility\n\nAs an example of desruction, students will be told the about the burger example and the dissection example, where observing destroys the entity in question.\nIn the case of impossibility, animal studies will be mentioned.\nNOTE: you can measure entire populations if you simply define your population to be manageable: (e.g. a whole class)\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\\\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”\n(note, assessment is in the activity)\n\nActivity- population and sample: 25 minutes\nVOCAB: students will be arranged into groups an placed in a region of the class with writing materials (if it is not a class with 360 boards on the walls, I will go out and prepare some big writing devices to slap on the wall) Students will then be given an example of study, and then be asked:\n\nis this an observational study or an experiment\nwhat is the population\nwhat is the sample\nwhat are the variables being measured?\nwhich variables are response and which are explanatory\nwhat is the observational unit in the study\n\nGroups will then be given a moment to discuss, and if the groups do not concur, an open discussion will take place."
  },
  {
    "objectID": "PLANDOCDAY2.html",
    "href": "PLANDOCDAY2.html",
    "title": "PLANDOCDAY2",
    "section": "",
    "text": "Carson Custom Day: 50minutes\nThis lesson is one that i have not seen on many lesson plans, but I believe will be part of a reoccurring theme in the class of visual statistics. These strategies have helped me significantly with getting better intuition for statistics, and I think these can help students too. Also shapes and colors are nice to have in lessons. It may sound more complex at first, but I see this lesson as a long term investment and I will work to provide good visual examples to make this less difficult.\n\nObjectives:\n\nStudents can draw set symbols as venn diagrams\nStudents understand how the set symbols relate to logical statements\nStudents can understand when to apply conditional probability solutions, and use the formula\nStudents know when it is appropriate to simply multiply and simply add probabilities for intersections and unions\nStudents know how to calculate the at least once probability, and how this effects aspects of our daily lives\n\nConcepts:\n\nUnion, Intersection, Complement\nOR, AND, NOT\nSupport\nIF, TRUE , FALSE\nConditional probability\nIndependent multiplication (AND)\nDisjoint/non disjoint addition (OR)\nAt least once problem\n\nIntuition Tools:\n\na\\(\\cap\\) d, yoUr\nSet symbols as logic gates\ngeometric distribution not fully named before using set symbols\n\nLesson Material:\nStudents will be given a wordless document with set symbols ,venn diagrams, the corresponding logic tables, and an example of the operation on a numerical set and a nominal set. This first document will have absolutely no words outside of AND, OR, NOT, and it will be a bit of an artsy delivery similar to NASAs golden record or SCP ...|…..|..|. , but the purpose it not to be artsy, but to deliver the material in the simplest way possible and to encourage them to try and figure it out. Words can shut out brains down, so I think this will be an intreguining and engaging exercise.\nFollowing some time in silence, a discussion will begin about the meaning.\nSUPPPORT: all potential outcomes\nUNION/OR: take all values present in both sets, no repeats\nINTERSECTION/AND: take only the values that are present in both sets, no repeats\nCOMPLEMENT/NOT: Take only the values NOT in the set in question\nEMPTY SET: set with nothing\nAfter given these words, they will be asked to write them down on the paper, labeling the operations by name.\nDISJOINT: The intersection is empty, both sets do not overlap\nINDEPENDENCE: The outcome of one event doesn’t effect the other\nCONDITIONAL PROBABILITY: prob A given B=(A AND B)/B\nHere we mention the operation for union of sets, and that if they are disjoint you can simply add them.\nFinally we will explain that if events A and B are independent, you can multiply them together to get the probability of event A AND B as A*B.\nThis will then be applied to a coin flip, where they will be asked to calculate the probability of rolling at least one heads out of 4 flips, which is the complement of the rolling all tails.\nASSESSMENT:\nSET OPERATIONS: students will be given a variety of numeric and nominal sets, and asked to perform the set operations. These will start as individual ones, then combine multiple\nFollowing these operations, the students will be asked to draw the Venn diagram with the support of the operation displayed\nGEOMETRIC: students will be given an question where they calculate the probability of either a dice roll or lottery ticket succeeding at least once after a X tries. After, they will be asked to display the method they used in set symbols.\nCONDITIONAL: students will be given a typical conditional probability question, and asked to evaluate the probability.\nOPTIONAL: one idea I thought would be fun, but maybe not fun for anyone else, would be to give them sets of 1s and 0s and a flowchart with set symbols and as them to go down the line and find the output values. Then using a binary to decimal table, write the inputs and outputs, and finally they are asked what operation the paper calculator is performing (+-*/)"
  },
  {
    "objectID": "PLANDOCDAY2.html#day-2",
    "href": "PLANDOCDAY2.html#day-2",
    "title": "PLANDOCDAY2",
    "section": "",
    "text": "Carson Custom Day: 50minutes\nThis lesson is one that i have not seen on many lesson plans, but I believe will be part of a reoccurring theme in the class of visual statistics. These strategies have helped me significantly with getting better intuition for statistics, and I think these can help students too. Also shapes and colors are nice to have in lessons. It may sound more complex at first, but I see this lesson as a long term investment and I will work to provide good visual examples to make this less difficult.\n\nObjectives:\n\nStudents can draw set symbols as venn diagrams\nStudents understand how the set symbols relate to logical statements\nStudents can understand when to apply conditional probability solutions, and use the formula\nStudents know when it is appropriate to simply multiply and simply add probabilities for intersections and unions\nStudents know how to calculate the at least once probability, and how this effects aspects of our daily lives\n\nConcepts:\n\nUnion, Intersection, Complement\nOR, AND, NOT\nSupport\nIF, TRUE , FALSE\nConditional probability\nIndependent multiplication (AND)\nDisjoint/non disjoint addition (OR)\nAt least once problem\n\nIntuition Tools:\n\na\\(\\cap\\) d, yoUr\nSet symbols as logic gates\ngeometric distribution not fully named before using set symbols\n\nLesson Material:\nStudents will be given a wordless document with set symbols ,venn diagrams, the corresponding logic tables, and an example of the operation on a numerical set and a nominal set. This first document will have absolutely no words outside of AND, OR, NOT, and it will be a bit of an artsy delivery similar to NASAs golden record or SCP ...|…..|..|. , but the purpose it not to be artsy, but to deliver the material in the simplest way possible and to encourage them to try and figure it out. Words can shut out brains down, so I think this will be an intreguining and engaging exercise.\nFollowing some time in silence, a discussion will begin about the meaning.\nSUPPPORT: all potential outcomes\nUNION/OR: take all values present in both sets, no repeats\nINTERSECTION/AND: take only the values that are present in both sets, no repeats\nCOMPLEMENT/NOT: Take only the values NOT in the set in question\nEMPTY SET: set with nothing\nAfter given these words, they will be asked to write them down on the paper, labeling the operations by name.\nDISJOINT: The intersection is empty, both sets do not overlap\nINDEPENDENCE: The outcome of one event doesn’t effect the other\nCONDITIONAL PROBABILITY: prob A given B=(A AND B)/B\nHere we mention the operation for union of sets, and that if they are disjoint you can simply add them.\nFinally we will explain that if events A and B are independent, you can multiply them together to get the probability of event A AND B as A*B.\nThis will then be applied to a coin flip, where they will be asked to calculate the probability of rolling at least one heads out of 4 flips, which is the complement of the rolling all tails.\nASSESSMENT:\nSET OPERATIONS: students will be given a variety of numeric and nominal sets, and asked to perform the set operations. These will start as individual ones, then combine multiple\nFollowing these operations, the students will be asked to draw the Venn diagram with the support of the operation displayed\nGEOMETRIC: students will be given an question where they calculate the probability of either a dice roll or lottery ticket succeeding at least once after a X tries. After, they will be asked to display the method they used in set symbols.\nCONDITIONAL: students will be given a typical conditional probability question, and asked to evaluate the probability.\nOPTIONAL: one idea I thought would be fun, but maybe not fun for anyone else, would be to give them sets of 1s and 0s and a flowchart with set symbols and as them to go down the line and find the output values. Then using a binary to decimal table, write the inputs and outputs, and finally they are asked what operation the paper calculator is performing (+-*/)"
  },
  {
    "objectID": "PLANDOCDAY4.html",
    "href": "PLANDOCDAY4.html",
    "title": "PLANDOCDAY4",
    "section": "",
    "text": "Short concept Time - Variability: 50 minutes\nObjectives:\n\nStudents know how to identify the measure of central tendency of a distribution, visually\nStudents can calculate standard deviation from variance, and variance from standard deviation\nStudents can look at two graphs and estimate which one has a higher variance\n\nConcepts:\n\nReview: relative frequency histograms\nReview: probability distributions\nReview: shape\nReview: Central Tendencies (visual review)\nVariability: variance and stdev\noutliers\n\nIntuition Tools:\n\nDot counting\nVariability as big boot\n\nLesson Material:\nMost of these concepts will be review, so I will quickly elaborate:\nMEASURES OF CENTRAL TENDENCY: mean, median, mode\nThen we will show a graph with a visible outlier, except we will note that outliers are somewhat subjective and not a mathematical truth, and that studies rarely just remove outliers as outliers can be expected\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances.\nWe will then go over some visual examples of higher variability in distributions, and that variacne and standard deviation are a means to measure this variability.\nWe will then state that the stdeviation is just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nIt will be noted that variance is calculated by taking the average difference between all of the values and the mean, so variance is another form of average, but not a measure of central tendency\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}\\)\nThis formula will be of conceptual importance, but will likely not be actually used.\nI will then show some examples of charts with low variance and charts with high variance (these are placeholders that were used in the handout, for lecture, I would prefer more chaotic sim examples)\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOne way to think about this is to pretend that the distribution is being stomped on by a big boot, and the the higher the variance, the heavier the boot, the flatter and more spread it ends up\nASSESSMENT:\nA single activity will be given to asses the concepts above. Students will be paired into groups with large writing devices. They will then be given a two large dot histogram plots on the screen and asked to identify the following\n\nmeasures of central tendency (all 3)\nidentify any outliers\nstate which distribution has the higher variance\nwhy, in words, you chose the distribution you chose."
  },
  {
    "objectID": "PLANDOCDAY4.html#day4",
    "href": "PLANDOCDAY4.html#day4",
    "title": "PLANDOCDAY4",
    "section": "",
    "text": "Short concept Time - Variability: 50 minutes\nObjectives:\n\nStudents know how to identify the measure of central tendency of a distribution, visually\nStudents can calculate standard deviation from variance, and variance from standard deviation\nStudents can look at two graphs and estimate which one has a higher variance\n\nConcepts:\n\nReview: relative frequency histograms\nReview: probability distributions\nReview: shape\nReview: Central Tendencies (visual review)\nVariability: variance and stdev\noutliers\n\nIntuition Tools:\n\nDot counting\nVariability as big boot\n\nLesson Material:\nMost of these concepts will be review, so I will quickly elaborate:\nMEASURES OF CENTRAL TENDENCY: mean, median, mode\nThen we will show a graph with a visible outlier, except we will note that outliers are somewhat subjective and not a mathematical truth, and that studies rarely just remove outliers as outliers can be expected\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances.\nWe will then go over some visual examples of higher variability in distributions, and that variacne and standard deviation are a means to measure this variability.\nWe will then state that the stdeviation is just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nIt will be noted that variance is calculated by taking the average difference between all of the values and the mean, so variance is another form of average, but not a measure of central tendency\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}\\)\nThis formula will be of conceptual importance, but will likely not be actually used.\nI will then show some examples of charts with low variance and charts with high variance (these are placeholders that were used in the handout, for lecture, I would prefer more chaotic sim examples)\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOne way to think about this is to pretend that the distribution is being stomped on by a big boot, and the the higher the variance, the heavier the boot, the flatter and more spread it ends up\nASSESSMENT:\nA single activity will be given to asses the concepts above. Students will be paired into groups with large writing devices. They will then be given a two large dot histogram plots on the screen and asked to identify the following\n\nmeasures of central tendency (all 3)\nidentify any outliers\nstate which distribution has the higher variance\nwhy, in words, you chose the distribution you chose."
  },
  {
    "objectID": "PLANDOCDAY6.html",
    "href": "PLANDOCDAY6.html",
    "title": "PLANDOCDAY6",
    "section": "",
    "text": "Significance: 50 minutes\nObjectives:\n\nStudents understand that the null distribution is separate from the data collected\nStudents know how to use simulations to find p value\nStudents know what symbols relate to the statistic and parameter\nStudents can write a hypothesis statement with symbols\nStudents can correctly differentiate between a p value and a p hat value ( a very common issue I noticed in exams )\nStudents understand that significance standards can be changed\n\nConcepts:\n\nReview: Population and Sample\nReview: data types\nThe block (symbols for population parameters and sample stats)\nDISTINCT Null distribution\nDISTINCT alternative space\n“significance” How weird/percent match\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nIntuition Tools:\n\nP value as colors/area\nP value as conditional probability\nP value as percent match\n\nLesson Material:\nTo begin the lesson, we will go over a problem: how weird is the dolphin result (except we will use a 4 sample test instead of a 16 sample test so it is easier to count)\nGiven that the dolphins succeeded 4/4 times, how abnormal is that?\nTo answer the question, we will first simulate a distribution if the dolphins were acting randomly, then print it out on clear paper.\nThis is our null distribution\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nThen we will then graph an area on clear paper that encompasses all xvalues over 4.\nThis is out alternative space\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis\nAfter constructing these two graphs, we will then overlay them to see how much they line up, and apply the same conditional probability logic as before. After testing counting the green squares, 1, vs the amount of total blue squares 16, we can see that this event would have a 1/16 =0.0625 chance of happenng.\nThis is the p value\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nSo if the null is true, such an outcome only has a 6.25% chance of happening in the experiment.\nIs this a weird result?\nLets think about it.\nI will then ask the class to answer, in general, if 5% is a lot. They will probably give nuanced answers about how it depends on the circumstances, but I will request a yes or no answer.\nThis exercise is to point out that significance is subjective and can change.\nWhile this is up to some interpretation, I will then tell them that the significance requirement will be provided on exams unless it is a concept question, and any answer about significance is valid if the calculations are correct and the only difference is the interpretation.\nWe will then jump to the 16 count dolphin example to gather more data. A simulation will be ran, and the counts will be performed automatically.\nOnce performed, the P value will be about 0.003 or 0.03%, I will then ask the class to raise their hand to indicate if this is significant. Regardless of the class answer, I will make a point to note that the experimenters still could have gotten this result by random chance alone, so it is technically valid to claim that there isn’t enough data to reject the null.\nMostly jokes here, but the point is to shift them away from seeing 0.05 as a magic fixed number, and more of a general idea that tends to be used often.\nAssessment:\nFollowing this session, students will be given a similar prompt as the dolphin question, where they are to asked to respond to an identical problem but with a different sample size and proportion. Students will then have to calculate the P value given the information at hand.\nTo avoid different answers that could be falsely marked wrong, students will be introduced to the sim’s “infinite sample mode” (the binomial setting) so that everyone has ideal answers."
  },
  {
    "objectID": "PLANDOCDAY6.html#day6",
    "href": "PLANDOCDAY6.html#day6",
    "title": "PLANDOCDAY6",
    "section": "",
    "text": "Significance: 50 minutes\nObjectives:\n\nStudents understand that the null distribution is separate from the data collected\nStudents know how to use simulations to find p value\nStudents know what symbols relate to the statistic and parameter\nStudents can write a hypothesis statement with symbols\nStudents can correctly differentiate between a p value and a p hat value ( a very common issue I noticed in exams )\nStudents understand that significance standards can be changed\n\nConcepts:\n\nReview: Population and Sample\nReview: data types\nThe block (symbols for population parameters and sample stats)\nDISTINCT Null distribution\nDISTINCT alternative space\n“significance” How weird/percent match\nhypothesis symbols format\nP value vs P hat\n“infinite sample mode”\n\nIntuition Tools:\n\nP value as colors/area\nP value as conditional probability\nP value as percent match\n\nLesson Material:\nTo begin the lesson, we will go over a problem: how weird is the dolphin result (except we will use a 4 sample test instead of a 16 sample test so it is easier to count)\nGiven that the dolphins succeeded 4/4 times, how abnormal is that?\nTo answer the question, we will first simulate a distribution if the dolphins were acting randomly, then print it out on clear paper.\nThis is our null distribution\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nThen we will then graph an area on clear paper that encompasses all xvalues over 4.\nThis is out alternative space\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis\nAfter constructing these two graphs, we will then overlay them to see how much they line up, and apply the same conditional probability logic as before. After testing counting the green squares, 1, vs the amount of total blue squares 16, we can see that this event would have a 1/16 =0.0625 chance of happenng.\nThis is the p value\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nSo if the null is true, such an outcome only has a 6.25% chance of happening in the experiment.\nIs this a weird result?\nLets think about it.\nI will then ask the class to answer, in general, if 5% is a lot. They will probably give nuanced answers about how it depends on the circumstances, but I will request a yes or no answer.\nThis exercise is to point out that significance is subjective and can change.\nWhile this is up to some interpretation, I will then tell them that the significance requirement will be provided on exams unless it is a concept question, and any answer about significance is valid if the calculations are correct and the only difference is the interpretation.\nWe will then jump to the 16 count dolphin example to gather more data. A simulation will be ran, and the counts will be performed automatically.\nOnce performed, the P value will be about 0.003 or 0.03%, I will then ask the class to raise their hand to indicate if this is significant. Regardless of the class answer, I will make a point to note that the experimenters still could have gotten this result by random chance alone, so it is technically valid to claim that there isn’t enough data to reject the null.\nMostly jokes here, but the point is to shift them away from seeing 0.05 as a magic fixed number, and more of a general idea that tends to be used often.\nAssessment:\nFollowing this session, students will be given a similar prompt as the dolphin question, where they are to asked to respond to an identical problem but with a different sample size and proportion. Students will then have to calculate the P value given the information at hand.\nTo avoid different answers that could be falsely marked wrong, students will be introduced to the sim’s “infinite sample mode” (the binomial setting) so that everyone has ideal answers."
  },
  {
    "objectID": "PLANDOCDAY8.html",
    "href": "PLANDOCDAY8.html",
    "title": "PLANDOCDAY8",
    "section": "",
    "text": "T Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to calculate the standardized statistic for a mean with unknown population variance\nStudents know how to obtain t scores and t critical values\nStudents know when to use a T score vs a Z score\nStudents should know the meaning and purpose of a standard error.\n\nConcepts:\n\nT Statistic\nStandard Error\nDegrees of Freedom\nCircumstantial Flow Chart\n\nIntuition Tools:\n\nsimulation limit\ndf as measure of normality\n\nLesson Material:\nIn the last session, we had plotted averages calculated from samples to make a sampling distribution. While we noticed that the distribution became more bell shaped as the sample size increased, it can also be noticed that the variance decreased as the sample size increased.\nThis makes sense, as larger samples are more resistant to random chance, and an outline is less likely to effect an average as the amount of samples gets larger.\nAt this point, a prepared code (Already prepared, will be included) will show that the what happens to the variance (visually) as the sample size goes up, until eventually the sampling distribution only contains about one small class of values.\nIts evident that the variance is decreasing, but we actually have a formula to approximate it: the standard error.\n\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nThis is a fun fact that will become more relevant later, but in regards to todays lesson, if we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nSo the degrees of freedom is directly connected to the sample size.\nIf you look at a T table, you may notice that as the degrees of freedom increases, the T score approaches values that are identical to the Z score table.\nThis effect is related to central limit theorem, and also makes sense because as a sample increases, it becomes more steadily reflective of the population #review this passage, more specifically on central limit theorem.\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work. (to explain this decision making process, a flowchart will be attached in the handout)\nAssessment\nZ VS T: Students will be given a variety of circumstances which would prompt the use of a standardized statistic, and will be asked if they are to use the theory based Z, theory based T, or neither. there will be no evaluation in this portion.\nT SCORE: Students will be given a circumstance prompting the use of a T statistic, and given the sample size, sample variance, null mean, and sample average, they will be asked to find if their value is significant.\nOne example will require them to calculate the exact p value with a table, and the other will ask if there is significant evidence to reject the null given a specific critical value for a specific standard of significance.\nSAMPLING DISTRIBUTION: Students will be given a sample with a sample variance and sample size, and they will be asked to estimate the standard deviation of the sampling distribution with that information.\nThis will test if they are aware that the standard error is an approximation for the sampling distribution.\nthe next sampling distribution will provide the population mean and population variance, ask them to estimate the mean of the sampling distribution. With this question, all they need is the mean as the population mean is approximately equal to the mean of the sampling distribution."
  },
  {
    "objectID": "PLANDOCDAY8.html#day8",
    "href": "PLANDOCDAY8.html#day8",
    "title": "PLANDOCDAY8",
    "section": "",
    "text": "T Standardized Statistic: 50 minutes\nObjectives:\n\nStudents will be able to calculate the standardized statistic for a mean with unknown population variance\nStudents know how to obtain t scores and t critical values\nStudents know when to use a T score vs a Z score\nStudents should know the meaning and purpose of a standard error.\n\nConcepts:\n\nT Statistic\nStandard Error\nDegrees of Freedom\nCircumstantial Flow Chart\n\nIntuition Tools:\n\nsimulation limit\ndf as measure of normality\n\nLesson Material:\nIn the last session, we had plotted averages calculated from samples to make a sampling distribution. While we noticed that the distribution became more bell shaped as the sample size increased, it can also be noticed that the variance decreased as the sample size increased.\nThis makes sense, as larger samples are more resistant to random chance, and an outline is less likely to effect an average as the amount of samples gets larger.\nAt this point, a prepared code (Already prepared, will be included) will show that the what happens to the variance (visually) as the sample size goes up, until eventually the sampling distribution only contains about one small class of values.\nIts evident that the variance is decreasing, but we actually have a formula to approximate it: the standard error.\n\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nThis is a fun fact that will become more relevant later, but in regards to todays lesson, if we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nSo the degrees of freedom is directly connected to the sample size.\nIf you look at a T table, you may notice that as the degrees of freedom increases, the T score approaches values that are identical to the Z score table.\nThis effect is related to central limit theorem, and also makes sense because as a sample increases, it becomes more steadily reflective of the population #review this passage, more specifically on central limit theorem.\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work. (to explain this decision making process, a flowchart will be attached in the handout)\nAssessment\nZ VS T: Students will be given a variety of circumstances which would prompt the use of a standardized statistic, and will be asked if they are to use the theory based Z, theory based T, or neither. there will be no evaluation in this portion.\nT SCORE: Students will be given a circumstance prompting the use of a T statistic, and given the sample size, sample variance, null mean, and sample average, they will be asked to find if their value is significant.\nOne example will require them to calculate the exact p value with a table, and the other will ask if there is significant evidence to reject the null given a specific critical value for a specific standard of significance.\nSAMPLING DISTRIBUTION: Students will be given a sample with a sample variance and sample size, and they will be asked to estimate the standard deviation of the sampling distribution with that information.\nThis will test if they are aware that the standard error is an approximation for the sampling distribution.\nthe next sampling distribution will provide the population mean and population variance, ask them to estimate the mean of the sampling distribution. With this question, all they need is the mean as the population mean is approximately equal to the mean of the sampling distribution."
  },
  {
    "objectID": "PreSemesterToDo.html",
    "href": "PreSemesterToDo.html",
    "title": "PreSemesterToDo",
    "section": "",
    "text": "Prior to the semester, I will expect myself to complete the following tasks:\n\nREFINE upcoming materials + handouts\n\nWhile I feel like I have put effort into these. I will likely find many flaws and possible improvements prior to class. As such, I will likely need to make some changes, so i will go through the first 2+ weeks to prepare.\n\nInitialize canvas and upload introduction activity early\n\nThis intro activity will tell me how students perceive the class and how to address them. This will be a short and easy assignment, but will help me prepare some class expectations and be help with communication later on.\n\nGrab the physical materials needed for demonstrations and activities and place them in a file at least one week ahead.\nFind out if my class is MWF or TR, and make minor adjustments to the time plan.\nPrepare a few outfits for class\nWrite up exact homework questions for the first assessments on canvas, and record some expectations for in class activities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LESSONPLAN",
    "section": "",
    "text": "PreSemesterToDo\nMethodsAndGoals\nSyllabus\nCourseSchedule"
  },
  {
    "objectID": "index.html#main-directory",
    "href": "index.html#main-directory",
    "title": "LESSONPLAN",
    "section": "",
    "text": "PreSemesterToDo\nMethodsAndGoals\nSyllabus\nCourseSchedule"
  },
  {
    "objectID": "HandoutCAR.html",
    "href": "HandoutCAR.html",
    "title": "HandoutCAR",
    "section": "",
    "text": "I will later edit this so that I have a custom image made exactly how I intend to present it, but as a placeholder, I have prepared some useful resources."
  },
  {
    "objectID": "MethodsAndGoals.html",
    "href": "MethodsAndGoals.html",
    "title": "MethodsAndGoals",
    "section": "",
    "text": "The following entry is a combination of my reflection and methods for the class. I think about class design a lot so the methods I would like to employ are deeply related to the ideas on how the class should be structured.\nEvery class has core ideas, so I have decided to plan the class around a few central themes. These ideas will be introduced early on, and then referenced regularly throughout the class:\n\nVisual statistics\n\nStatistics can be much easier to have an intuition for when visuals are provided, so the class will make frequent use of graphs and colors along side formulas and concepts. (This will require a bit of set theory to begin)\n\nlack of words\n\nMath is harder with words, and sometimes words can prevent a deeper understanding of a topic. I would like to frequently give students information and questions that feature very little words or hard data, and instead require them to visually asses the problem\nThis also applies to labeled formulas. Eventually, I would like to prepare annotated formulas that use as few words as practical. Mindlessly using formulas is not good, but sometimes a labeled formula can help build a conceptual understanding if the elements are considered closely by the reader.\n\ndice and spinners\n\nprobability distributions are abstract, but everyone knows how dice and spinners work, I would like to regularly reference these when discussing probability distributions.\n\n“numbers do not speak for themselves”\n\nWithout assumptions in place, statistics can be very subjective (as is math in general). I want to regularly encourage students to interpret the data themselves when no assumptions are provided, and stress how many aspects of statistics are subjective.\n\nvocab is vocab\n\nSometimes theirs no replacement for just reading and understanding vocab. This will be given to them directly rather than hoping the correct definitions are gathered from discussion alone.\n\nSlides not the focus\n\nThere will be some use of slides in class used to paste some visual ideas in front of the class, but they will not be consistently up. Often times words on slides can detract from the topic, so the slides will not have much text information on them. To maintain pace and engagement, I would prefer to use an overhead or board to perform methods in class, and physical methods performed on the spot will be preferred to slides.\nIn my own lectures, I have notices that I tend to disconnect the slideshow frequently to display simulations and write on the board/overhead. So demonstrations will be prioritized to slides.\n\nclear expectation\n\nWhile I would love students to be so interested in the course that they start watching 3blue1brown videos and learn things that I haven’t even mentioned, this does not feel realistic. Students also are likely going through some of the worst years of their lives, and I would like to avoid making that worse by having stochastic expectations. (I will ask some challenge questions, but they will be graded as more of a bonus and gauge of engagement and ability rather than a requirement)\n\n\nWith exams, there are a few methods I am interested to investigate\n\nexam independent of time\n\nWhile I believe that everything in the course will be planned so that, with sufficient hard work, every student will be able to understand and answer correctly, I’ve always felt that time doesn’t effect everyone the same. Some students approach exams with a ton panic, others check their work more than others, some students accidentally get hung up on a question, these kinda traits can make some students do worse on time tests than others even if they don’t have a contrition recognized by the SSD. To ease this aspect and hopefully make the learning less stressful I would like to figure out a way to make tests have such ample time that students don’t think about it (within reason). I don’t want the test to be less challenging, just challenging in the understanding of materials of materials way and not challenging in a resistance to panic way.\nIn a perfect world, I would have a room that I could rent for a few hours and have closed book proctored tests\n\nCompetency and Challenge\n\nTests serve two functions: to measure general competency and to gauge how well the class is absorbing the materials. Both aspects are useful: If I were to only gauge competency, I would not know when to speed up or slow down, and if I were to only gauge ability on a relative scale, this would miss the main point of the class: to produce competent readers of data. An idea I would like to investigate on exams, is to have high weighted competency section and a lesser weighted/bonus challenge section (this will likely not be formally stated in the test). This allows me to grade primarily on competency, and that students who display competency get a good grade, but also lets me see how the class is doing and if students have taken a special interest in statistics. I think of this like a skewed distribution where the competency (hopefully the median) is centered around the higher percentages, but the the distribution quickly drops down and the density at 100% is near 0.\n\n\nI personally believe that final projects are generally more effective than final exams, as many undergrad final projects have given me information and experience that I still find important years later, so the final will be project based:\n\nThe final project will be in three parts: presentation, paper, assessment and reflection\\\nThe presentation will happen first during the week before finals. Students will be given reactions on how to improve anything wrong with the information so far.\nThe paper will be submitted during finals week. This will have more in depth information and be graded to a more exact standard\nThe reflection will be a combination of students attending and reacting to other presentations, along with reflecting on their experience working as a group.\n\nThe class will feature many activities based on participation, with the following main goals. Activities in class will be on the spot and not require to pre reading to understand. Ideally, the handouts and introduction lecture should cover it.\n\nActivities allow students to confront ideas with each other to learn them\nActivities give students an opportunity to question ideas layed out in class (it is near impossible not to miss something when teaching, so if I get successfully (and politely) called out on something during an activity, I will consider it to be a teaching win.\nActivities are not just for completing work, Ideally, they will be structured so that students feel more comfortable expressing ideas openly and to each other. if student are too rigid, they may find engaging in class to be too nerve racking as they feel judged by strangers. In practice, I will not yell at them for discussing topics that aren’t immediately math related (within reason). (If extra time is available, there also are “spells” to spur discussion. USE AT YOUR OWN RISK: but I have had teachers 3 times now mention “water is wet” issue in class and it immediately got a whole class debate going. I once talked about the event and accidentally unleashed the spell on the class i was in. I apologized to my teacher, and thankfully, they were understanding. If students seem disengaged or uncomfortable, and we have a few extra minutes at the end of class, I may consider stating something like that)\n\nThere will also be demonstrations, but these demonstrations will be used as a means to deliver information on methods that are particularly confusing, or display activities that would take a large amount of classroom\n\nFor example, there is a demonstration I do where I calculate the P value using colored squares and clear paper. This could be performed in class but it could be demonstrated as a demonstration in much less time (I have noticed that time can be a huge challenge so I intend to have backups and compensating actions for if things do not go as fast as planned)\n\nHomework will have the following ideas in its design:\n\nHomework will be very short and frequent. This is to avoid all the problems with stressful cramming and encourage learning little by little. Homework will be assigned after the material is began and they will have until the lecture after the lecture after the section is completed.\nHomework should straight forward and avoid situations where these student has no idea how to even look for an answer. All homework questions will related directly to on canvas materials. If the student gives themselves the proper time. the learning process should be calm.\nMost of the homework questions will be nigh-objective in correct answers, but to encourage free thinking and egagement there will always be a creative question they are to answer, which is graded more on submission. This is to encourage them to take a risk and attempt at reorganizing what they learned, without being penalized for not copying definitions exactly. If they make flawed assumptions, they will be notified about it, but it will not harm their grade.\n\nThis plan is already feeling very optimistic, but I have a few even more optimistic stretch goals:\n\nHomework and projects will not just have context, but a consistent narrative. A prime example of narrative in homework is the Murder in SQL City assignment. Aside from being fun, I am willing to be that narrative aspects can increase engagement and understanding.\nStudents will be given intentionally subjective questions in class, which they will discuss with others and hopefully (politely lightheartedly) debate each other.\nSUPER STRETCH GOAL: if by some wild circumstances I end up being am inhumanly efficient teacher and the class goes exceptionally, I would love to briefly mention a little bit of R. Programming basics are extremely useful and I have benefit tremendously from the classes that included it. Realistically, this would be very basic stuff, more realistically, we will not have time, but I like having optimistic stretch goals regardless."
  },
  {
    "objectID": "PLANDOCDAY1.html",
    "href": "PLANDOCDAY1.html",
    "title": "PLANDOCDAY1",
    "section": "",
    "text": "Syllabus Day: 25 minutes\nObjectives:\n\nStudents understand course expectations\nStudents understand the policies in place for the class\nStudents are aware of what resources and accommodations are available to them\nCore rules are stressed enough to be easily remembered\n\nConcepts:\n\nThe syllabus\n\nJust for safety, I will check here to make sure everyone understands mean, median, and mode. You never know where someone is coming from, and I wouldn’t mind going over it.\nActivity: Students will be assigned a starter activity (at home)so I know some basic info about their perception of the class, concerns, and how to address them.\n\n----&gt;First Lecture: 25 minutes\nCourse objectives\n\nStudents know the main datatypes and when to use them\nStudents can draw a connection chart (flowchart labeled how they are related to each other) between classes, histograms, relative frequency histograms, and probability distributions\nStudents can label the shape of distributions fitting the main distribution shapes\n\nConcepts:\n\nvariable\ncategorical vs quantitative\nclasses\nhistograms/relative frequency histograms\nprobability distributions\nshape\n\nIntuition Tools:\n\nProbability distributions as machines that pump out random numbers\nProbability as area, in literal square meters\nProbability distributions as historgrams\nDiscrete as dice\nContinuous as spinner\nRay gun skew\n\nLesson material:\nVARIABLE: this is any measurable quality of an entity\nVariables have two subgroups:\nCATEGORICAL: relating to groups which can be represented by numbers, but do not operate like numbers. Their spacing is either inconsistent or illogical.\nCategorical variables have two groups:\nnominal: groups which have no implied order (“carbs”, “protein”, “fats”)\nordinal: groups which have an implied order, but not necessarily a number (letter grades, outcome of a race)\nQUANTITATIVE: measurements that have numerical values that operate like regular integers/real numbers do. Their spacing is even and predictable.\nQuantitative variable shave two groups:\ndiscrete: If there are finite points between two points in the set, the variable is discrete (people in a room, dice roll points, number of trials succeeded)\n##I need to find out if rationals are discrete or continuous; update, apparently its continuous\n##personal note: some classes discuss the ratio and interval data types, but I had found this setup with continuous and discrete more relevant to this kind of statistics work.\ncontinuous: if there is an infinite number of points between two points, the variable is continuous (time, distance(kinda), mass(big kinda))\nCLASSES: groups of values chunked together, usually to groupn a bunch of values that would be disorderly otherwise\nHISTOGRAM: chart that essentially tallies the number of items that fit in each class in a set, except instead of using tallies, you fill in bars.\nRELATIVE FREQUENCY HISTOGRAM: chart thats exactly like a histogram, but instead of being charted by the number of values in a class, its the number of values in a class divided by the total, forming a fraction between 0 and 1 (equivalently forming a percent between 0% and 100%)\nPROBABILITY DISTRIBUTION: Much like the relative frequency histogram, but instead of thinking of the chart as the outcome of a measurement, it is treated as a machine that pumps out results. The probability of each result is the percentage seen on the side. NOTE: the idea of RVs as the outputs of a machine will be one of the intuition tools used in this class\nImportant note: the probabilities can be added up, as the bars take up a certain amount of area, and area = probability (or at least, this is a way of thinking about it visually, which i intend to use to make statistics more intuitive and visual)\nDISCRETE DISTRIBUTION: Probability distribution, but with discrete values ONLY. Think dice.\nCONTINUOUS DISTRIBUTION: Probability distribution, with an infinite number of values between two points. Think wheel spinner, but with no pegs - just a rainbow of possible hues.\nSHAPE: the literal shape of a distribution, but rather than using traditional shapes, we use words like symmetric, non symmetric ,skewed, unimodal, bimodal, and uniform. (this is where a picture will be provided with these shapes)\nskewed right: mean&gt;median&gt;mode(peak)\nskewed left: mean&lt;median&lt;mode(peak)\nSymmetric unimodal: mean=median=mode\nHere I plan to use the ray gun example on how to remember which skew it is. (the ray gun example is a strategy I came up with to remember skew, if the ray gun is pointing right, its skewed right, if the ray gun is pointing left, its skewed left)\nAssessment:\nDATATYPES: To assess the students knowledge of datatypes, they will be given a list of variables and told to arrange them into certain boxes by their datatype. For example, if the variable “time required to finish a test” is given, it will be put in the variable&gt;quantitative&gt; continuous\nDISTRIBUTIONS AND CHARTS: Students will be given a list of numbers, and a desired class size, and from that they will be asked to count the classes, make a histogram, then a relative frequency histogram, then a discrete probability distribution by hand. Finally, they will be asked to use the chart determine the probability of getting a number in one of two of the classes, which can be added. #verify accuracy of this last part\\\nSHAPE: Students will be given two exercises to asses the knowledge of shape. The first will be to observe a few distributions and declare their shape, and the next part they will be given a mean, median, and mode (then later just two) and asked to determine the skew of a distribution."
  },
  {
    "objectID": "PLANDOCDAY1.html#day-1",
    "href": "PLANDOCDAY1.html#day-1",
    "title": "PLANDOCDAY1",
    "section": "",
    "text": "Syllabus Day: 25 minutes\nObjectives:\n\nStudents understand course expectations\nStudents understand the policies in place for the class\nStudents are aware of what resources and accommodations are available to them\nCore rules are stressed enough to be easily remembered\n\nConcepts:\n\nThe syllabus\n\nJust for safety, I will check here to make sure everyone understands mean, median, and mode. You never know where someone is coming from, and I wouldn’t mind going over it.\nActivity: Students will be assigned a starter activity (at home)so I know some basic info about their perception of the class, concerns, and how to address them.\n\n----&gt;First Lecture: 25 minutes\nCourse objectives\n\nStudents know the main datatypes and when to use them\nStudents can draw a connection chart (flowchart labeled how they are related to each other) between classes, histograms, relative frequency histograms, and probability distributions\nStudents can label the shape of distributions fitting the main distribution shapes\n\nConcepts:\n\nvariable\ncategorical vs quantitative\nclasses\nhistograms/relative frequency histograms\nprobability distributions\nshape\n\nIntuition Tools:\n\nProbability distributions as machines that pump out random numbers\nProbability as area, in literal square meters\nProbability distributions as historgrams\nDiscrete as dice\nContinuous as spinner\nRay gun skew\n\nLesson material:\nVARIABLE: this is any measurable quality of an entity\nVariables have two subgroups:\nCATEGORICAL: relating to groups which can be represented by numbers, but do not operate like numbers. Their spacing is either inconsistent or illogical.\nCategorical variables have two groups:\nnominal: groups which have no implied order (“carbs”, “protein”, “fats”)\nordinal: groups which have an implied order, but not necessarily a number (letter grades, outcome of a race)\nQUANTITATIVE: measurements that have numerical values that operate like regular integers/real numbers do. Their spacing is even and predictable.\nQuantitative variable shave two groups:\ndiscrete: If there are finite points between two points in the set, the variable is discrete (people in a room, dice roll points, number of trials succeeded)\n##I need to find out if rationals are discrete or continuous; update, apparently its continuous\n##personal note: some classes discuss the ratio and interval data types, but I had found this setup with continuous and discrete more relevant to this kind of statistics work.\ncontinuous: if there is an infinite number of points between two points, the variable is continuous (time, distance(kinda), mass(big kinda))\nCLASSES: groups of values chunked together, usually to groupn a bunch of values that would be disorderly otherwise\nHISTOGRAM: chart that essentially tallies the number of items that fit in each class in a set, except instead of using tallies, you fill in bars.\nRELATIVE FREQUENCY HISTOGRAM: chart thats exactly like a histogram, but instead of being charted by the number of values in a class, its the number of values in a class divided by the total, forming a fraction between 0 and 1 (equivalently forming a percent between 0% and 100%)\nPROBABILITY DISTRIBUTION: Much like the relative frequency histogram, but instead of thinking of the chart as the outcome of a measurement, it is treated as a machine that pumps out results. The probability of each result is the percentage seen on the side. NOTE: the idea of RVs as the outputs of a machine will be one of the intuition tools used in this class\nImportant note: the probabilities can be added up, as the bars take up a certain amount of area, and area = probability (or at least, this is a way of thinking about it visually, which i intend to use to make statistics more intuitive and visual)\nDISCRETE DISTRIBUTION: Probability distribution, but with discrete values ONLY. Think dice.\nCONTINUOUS DISTRIBUTION: Probability distribution, with an infinite number of values between two points. Think wheel spinner, but with no pegs - just a rainbow of possible hues.\nSHAPE: the literal shape of a distribution, but rather than using traditional shapes, we use words like symmetric, non symmetric ,skewed, unimodal, bimodal, and uniform. (this is where a picture will be provided with these shapes)\nskewed right: mean&gt;median&gt;mode(peak)\nskewed left: mean&lt;median&lt;mode(peak)\nSymmetric unimodal: mean=median=mode\nHere I plan to use the ray gun example on how to remember which skew it is. (the ray gun example is a strategy I came up with to remember skew, if the ray gun is pointing right, its skewed right, if the ray gun is pointing left, its skewed left)\nAssessment:\nDATATYPES: To assess the students knowledge of datatypes, they will be given a list of variables and told to arrange them into certain boxes by their datatype. For example, if the variable “time required to finish a test” is given, it will be put in the variable&gt;quantitative&gt; continuous\nDISTRIBUTIONS AND CHARTS: Students will be given a list of numbers, and a desired class size, and from that they will be asked to count the classes, make a histogram, then a relative frequency histogram, then a discrete probability distribution by hand. Finally, they will be asked to use the chart determine the probability of getting a number in one of two of the classes, which can be added. #verify accuracy of this last part\\\nSHAPE: Students will be given two exercises to asses the knowledge of shape. The first will be to observe a few distributions and declare their shape, and the next part they will be given a mean, median, and mode (then later just two) and asked to determine the skew of a distribution."
  },
  {
    "objectID": "handoutPREIIIA.html",
    "href": "handoutPREIIIA.html",
    "title": "HANDOUT PRE A",
    "section": "",
    "text": "DATA TYPES\n\nlibrary(DiagrammeR)\n\nWarning: package 'DiagrammeR' was built under R version 4.2.3\n\nDiagrammeR::grViz(\"digraph {\n\ngraph [layout = dot, rankdir = LR]\n\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = rectangle, style = filled, fillcolor = Linen]\n\nvar  [label = 'variable']\ncat [label = 'categorical']\nquant [label =  'quantitative']\nord [label = 'ordinal']\nnomi [label= 'nominal']\ndisc [label= 'discrete']\ncont [label= 'continuous']\n\n# edge definitions with the node IDs\nvar -&gt;{cat, quant} \ncat -&gt; {nomi, ord}\nquant -&gt; {disc, cont}\n}\")\n\n\n\n\n\nCLASS: groups of values\n\n\n\n\n\n\n\nCount\n(these are the classes)\nFrequency\n(number of times a value within these classes showed up in the data)\n\n\n\n\n1-10\n2\n\n\n11-20\n5\n\n\n21-30\n3\n\n\n\nHISTOGRAM: A chart displaying the amount of elements in each class\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar()+\n  labs(title=\"Histogram\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Frequency\")\n\n\n\n\nRELATIVE FREQUENCY HISTOGRAM: like a normal histogram, but it shows the amount of said class relative to the total, rather than just the amount of said class. NOTE: the scale ranges from 0-100%, AKA, 0-1\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Relative Frequency Histogram\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Frequency/Total\")\n\n\n\n\nPROBABILITY DISTRIBUTION: Very similar to the relative frequency histogram, except the percentages on the y-axis can be thought of as probabilities. You can add these, so in the below example, if you were to put all of the numbers in a bag and select one at random, there is a 20% chance of getting a number between 1 and 10, and a 70% chance of getting a number between 1 and 20.\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2, 12,13,14,15,16,25,26,27)\n)\n\ncutoff &lt;-c(0,11,21,31)\nclassttl &lt;-c(\"1-10\",\"11-20\",\"21-30\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Probability distribution\")+\n  xlab(\"bins/classes\")+\n  ylab(\"Probability\")\n\n\n\n\nDISCRETE DISTRIBUTION: A probability distribution with a finite number of outcomes between two different points. In other words, you can only get certain exact numbers.\nThe distribution of a six sided die can be seen below:\n\nlibrary(ggplot2)\ndf = data.frame(\n  amt = c(1,2,3,4,5,6)\n)\n\ncutoff &lt;-c(0,1,2,3,4,5,6)\nclassttl &lt;-c(\"1\",\"2\",\"3\",\"4\", \"5\",\"6\")\ndf$binz &lt;- cut(df$amt, breaks=cutoff, labels=classttl, include.lowest = TRUE)\n\n\nggplot(df, aes(x=binz))+\n  geom_bar(aes(y=after_stat(count) / sum(after_stat(count))))+\n  labs(title=\"Distribution of 6 sided die\")+\n  xlab(\"Number Rolled\")+\n  ylab(\"Probability\")\n\n\n\n\nIMPORTANT CONCEPT: the probability of rolling a value is the area that that the bars take up, so for the chance of rolling a 1 or 2, the width of the bars is 1+1=2 and the height of the bars is \\(\\approx\\) 0.16666, so the probability of rolling one or two is 0.16666*2 \\(\\approx\\) 0.3333 or 33.3%.\nCONTINUOUS DISTRIBUTION: A probability distribution with an infinite number of outcomes between two different points. (NOTE: there are an infinite number of real numbers between 0 and 1.)\n\nlibrary(ggplot2)\n\nlistval &lt;- data.frame(vals= seq(0,10,length.out=1000000))\n\n\nggplot(listval, aes(x=vals))+\n  geom_density(fill=\"grey\", color=\"black\")+\n  labs(title = \"continuous probability distribution\")+\n  xlab(\"x\")+\n  ylab(\"density\")\n\n\n\n\nSHAPE: literally the shape of the distribution, visually.\nNOTE: the distributions are below are a bit blocky, but continuous distributions can also have the same shapes."
  },
  {
    "objectID": "handoutA.html",
    "href": "handoutA.html",
    "title": "HandoutA",
    "section": "",
    "text": "DATA: The measured values or categories recorded on an individual of interest.\nPOPULATION: The complete collection of ALL items of interest for a given problem\nexample: “all humans”, “all UNL students”, “all Toyata Previas”\nSAMPLE: Sub collection of items from the population\nexamples: “1000 humans”, “25 UNL students”, “5 Toyota Previas”\n\nVARIABLE: Any measurable characteristic\nexamples: “height”, “vertical jump” , “mass”, “color”, “shape”, “whether or not a person said they liked Toyota Previas”\nEXPLANATORY VARIABLE: expected cause of an event. Similar to the “independent variable”\nRESPONSE VARIABLE: the observed outcome for a explanatory variable. Similar to the “dependent variable”\nexample:\nexplanatory=“amount of milk given to participants in an experiment”\nresponse=“digestive issues reported following the treatment”\nLURKING VARIABLE: variable that impacts the explanatory and response variable, causing a false association\n\nObservational Unit: The unit at which the data is collected. NOT THE UNIT USED TO MEASURE THE VARIABLE\nexamples: “a single person asked their opinion on Toyota Previas”, “A single piece of ice measured in mass”, “a single bird that had its speed measured”\nOBSERVATIONAL STUDY: Study where the variables are not manipulated, and are instead passively observed.\nexample:“asking several people if they smoke and then seeing how fast they can pedal a bicycle”\nEXPERIMENT: Study where the independent variable is manipulated by scientists and the response variable is measured\nexample:“making several people smoke for a few months and then seeing how fast they can pedal a bicycle”"
  },
  {
    "objectID": "handoutB.html",
    "href": "handoutB.html",
    "title": "handoutB",
    "section": "",
    "text": "Variance: measure of variability present in a set of data.\nThis is calculated by taking average difference between each value and the mean of the data set\nNOTE: xbar=mean\n\\(Variance(population)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n}=\\sigma^{2}\\)\n\\(Variance(sample)= \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})}{n-1}=s^{2}\\)\nStandard deviation: literally just the square root of the variance\n\\(StandardDeviation=\\sqrt{Variance}\\)\nDistribution with lower variance\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=0.5)\n\nplot(x,y, main=\"distribution with lower variance\")\n\n\n\n\nDistribution with higher variance\n\nx&lt;- seq(-3,3, length=10000)\ny=dnorm(x, sd=1.5)\n\nplot(x,y,\n     ylim=c(0,.8),\n     main=\"distribution with higher variance\")\n\n\n\n\nOUTLIER: a data point that differs significantly from the rest of the data. What counts as an outlier is subjective and outliers aren’t usually removed from data outside of specific circumstances."
  },
  {
    "objectID": "HandoutC.html",
    "href": "HandoutC.html",
    "title": "HandoutC",
    "section": "",
    "text": "RANDOM VARIABLE: A quantity that can be assigned a variable based on probability. Usually denoted by capital letters rather than lower case ones.\nSIMULATION: Drawing samples using a computer to mimic a real process.\nShort Day!"
  },
  {
    "objectID": "HandoutDE.html",
    "href": "HandoutDE.html",
    "title": "HandoutDE",
    "section": "",
    "text": "Col1\nSample\nStatistics\n(modified latin letters)\nPopulation Parameters\n(greek letters)\n\n\n\n\nProportion/\nCategorical\n(Start with “p”)\n\\(\\hat{p}\\)\n\\(\\pi\\)\n\n\nMean/\nQuantitative\n\\(\\bar{x}\\)\n\\(\\mu\\)\n\n\nVariance\n\\(s^{2}\\)\n\\(\\sigma^{2}\\)\n\n\n\nNUL DISTRIBUTION: probability distribution assuming the null hypothesis is true.\nALTERNATIVE SPACE: this is a term I use to describe the area you are questioning for the alternative hypothesis.\nPVALUE: Proportion of successive experiments resulting in an outcome assuming the null is true\nNOTE: the P Value and P Hat Value are completely different variables\nHypothesis symbols:\n\\(null= H_{0}\\)\n\\(alternative= H_{a}\\)\nLet “k” be the null value (the mean of the null distribution)\nTesting for difference of means\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu=k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu \\neq k\n\\]\nTesting if mean specificially greater than\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu \\leq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu &gt; k\n\\]\nTesting if mean specifically less than\n\\[\nnull.hypotheis -&gt; H_{0}: \\mu \\geq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\mu &lt; k\n\\]\nLet “k” be the null value (the proportion of the null distribution)\nTesting for difference of proportion\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} =k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} \\neq k\n\\]\nTesting if specificially greater than\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} \\leq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} &gt; k\n\\]\nTesting if specifically less than\n\\[\nnull.hypotheis -&gt; H_{0}: \\hat{p} \\geq k\n\\]\n\\[\nalternative.hypotheis -&gt; H_{a}: \\hat{p} &lt; k\n\\]\n#the other lesson used an equal sign, but I think this makes sense and the i have seen it on the internet. I am unsure which is correct\nP value as conditional probability\n\\[\nP.Value=\\frac{AlternativeSpace \\cap H_{0}}{H_{0}}\n\\]"
  },
  {
    "objectID": "HandoutF.html",
    "href": "HandoutF.html",
    "title": "HandoutF",
    "section": "",
    "text": "SAMPLING DISTRIBUTION: a distribution of a statistic obtained from a large number of samples pulled from a population. In other words, the distribution of the means. (in this context)\nCENTRAL LIMIT THEOREM: in many situations, for independent and identically distributed random samples, the distribution of the sample means approximates the normal distribution. In other words, the plot of the means often becomes normal.\nSTANDARDIZED STATISTIC=A value which represents how many standard deviations an input value is from the mean of the null distribution.\nWith this statistic, we have two options:\n1) we can use our standardized statistic on a Z table to calculate the P value\nThis is direct, just using a table\n2) we can find the critical value for our significance level and see if our standardized statistic is beyond that.\nOur critical value depends on two things, the standard of significance we desire, and if we are interested in a “greater”, “less than” ,or “not equal to” hypothesis.\nCRITICAL VALUE: Point on distribution, in standard deviations, where standardized statistics beyond that value are considered significant.\nThis process shouldn’t be applied to every situation, as there are some validity conditions:\n\nLarge enough sample size (subjective, but in our course 10 failures and 10 successes or 20 quantitative samples is enough)\nDistribution must not be skewed or in general non-bell shaped\nIf the variable is not a proportion and is instead a mean, and population variance is not known, a t statistic has to be used instead of a z statistic (in next lesson)\n\nSimulation ALWAYS works, however. #review this\n\\[\nZ=\\frac{x-\\mu}{\\sigma}\n\\]\nRecipe for low p Values\n\nAs variation decreases, p value decreases\nAs the difference between the null mean and the alternative mean increase, the p value decreases ."
  },
  {
    "objectID": "HandoutG.html",
    "href": "HandoutG.html",
    "title": "HandoutG",
    "section": "",
    "text": "\\[\nStandard Error =\\sqrt{\\frac{variance}{n}}\n\\]\nSTANDARD ERROR: The approximate standard deviation of the sampling distribution.\nIf we replace the population variacne in the Z statistic formula with the standard error, we can calculate the T statistic:\n\\[\nZ= \\frac{x-\\mu}{\\sigma}\n\\]\n\\[\nT= \\frac{\\bar{x}-\\mu}{\\sqrt{\\frac{s}{n}}}\n\\]\nNow the system for the critical value, standardized statistic, and p value is the same as before: we use a table, but there is an extra aspect to look out for: the degrees of freedom.\nThe degrees of freedom in this context is just the sample size minus 1.\n\\[\ndf = n-1\n\\]\nDEGREES OF FREEDOM: the maximum number of logically independent values, which are values that have the freedom ton vary. You can also think of this as a measure of how close the statistic is approaching the Z.\nWith this information, you can calculate T statistics with a method almost identical to the way Z statistics were calculated. But what is the relevance of the T statistic?\nIf the sample variance is unknown (as it most often tends to be), and we are calculating this score using a sample mean, then the Z statistic will not work.\n###(to explain this decision making process, a flowchart will be attached in the handout)"
  },
  {
    "objectID": "HandoutH.html",
    "href": "HandoutH.html",
    "title": "HandoutH",
    "section": "",
    "text": "GENERALIZABILITY: how well the information in the sample works for the larger population rather than just being representative of the sample\nREPRESENTATIVE SAMPLE: a sample which is representative of the whole population for which it has been drawn.\nGENERALIZATIONS: making conclusions from the sample, which can be used to make conclusions outside of the sample to the larger population\nBIAS: when a statistic consistently overestimates or underestimates.\nCONVINIENT SAMPLING: when samples are selected based on convenience and availability rather than more evenly dispersed randomness.\nexample: asking everyone outside of your workplace to take a poll rather than driving around your city to sample from all areas.\nSIMPLE RANDOM SAMPLING: Every individual in the population has an equal probability of being selected an involved in the sample.\nNOTE: it may seem like simple random sampling is “good” sampling and convenient sampling is “bad” sampling, but sometimes convenient sampling is selected for ethical reasons (e.g. only selecting willing participants) as using a true random sample would be unethical.\nOTHER FORMS OF SAMPLING: These will not be mentioned in this session, but just know that they exist.\nSAMPLING FRAME: list of all individuals in a population."
  },
  {
    "objectID": "HandoutI.html",
    "href": "HandoutI.html",
    "title": "HandoutI",
    "section": "",
    "text": "The Null Hypothesis is ________\nTrue\nFalse\n\n\n\n\nRejected\nType 1 Error\n“False Positive”\nCorrect\n\n\nNot rejected\nCorrect\nType 2 error\n“False Negative”\n\n\n\nT1ER: Type 1 Error (is when you) Reject\nT2EFTR: Type 2 Error (is when you) Fail To Reject\nTYPE 1 ERROR: The null is true, you reject the null hypothesis\nTYPE 2 ERROR: The alt is true, you fail to reject the null hypothesis\nALPHA: Probability of rejecting the null hypothesis when the null is true. For a p value standard of 0.05, alpha is 0.05] ; AKA probability of type 1 error.\nBETA: Probability of failing to reject the null hypothesis when the alt is true ; AKA, the probability of a type 2 error.\nPOWER: Likelihood of detecting an effect (rejecting the null hypothesis) if the alt is true. Power = 1-beta. AKA, probability of NOT making a type 2 error.\nCOMMON VARIANCE: The variance shared by two distributions when they have the same variance\nThe formula for power of two independent samples with equal variance requires a few variables:\n\nsignificance level alpha\nsample size\nthe sample type (e.g. two sample)\nthe effect size\n\nWe haven’t seen the effect size, but we can calculate it with the following formula.\n\\[\neffect size=\\frac{|\\mu_{1}-\\mu_{2}|}{\\sigma}\n\\]\n\\(\\mu_{1}=\\) mean of your alternative distribution\n\\(\\mu_{2}=\\) mean of your null distribution\n\\(\\sigma=\\) common variance of the two distributions\nTo see the impact of the variables on the power level, the following charts were made by adjusting one of the variables and holding all other variables constant:\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,1, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = effect, sig.level = numdf$inp, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"alpha\", main=\"power as significance level alpha increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,200, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=numdf$inp, d = effect, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"n\", main=\"power as sample size increases\", )\n\n\n\n\n\nnumdf &lt;- data.frame(\n  inp &lt;- seq(0,5, length=1000)\n)\n\ndiff &lt;- 10\nsd &lt;- sqrt(100)\neffect &lt;- diff/sd\n\nnumdf$yy &lt;- (power.t.test(n=20, d = numdf$inp, sig.level = 0.05, power = NULL, type = \"two.sample\"))$power\n\nplot(numdf$inp, numdf$yy,ylab=\"power\",xlab=\"effect size\", main=\"power as effect size increases\", )"
  }
]